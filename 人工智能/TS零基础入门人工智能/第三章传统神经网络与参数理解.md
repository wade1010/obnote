### 感知机

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148792.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148068.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148233.jpg)

### 激活函数

激活函数的原理、类别与实现

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148293.jpg)

激活函数最大的作用就是将线性可分变为非线性可分。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148484.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148743.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148904.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148357.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148556.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148844.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148991.jpg)

上图w1和w2写反了

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148256.jpg)

### 激活函数

种类

·Sigmoid

。Tanh

·ReLU

·Leaky ReLU

。Maxout

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148364.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148488.jpg)

上图最后一行，Sigmoid(x)是一个小于1的小数，1减它也是个小数，这两个小数相乘，越来越小。

当这个神经网络越来越深的时候，通过不断地反向传播，Sigmoid(x)的值也会越来越小，就会出现梯度消失的情况。所以这个时候就没办法完成深层网络训练。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148535.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148676.jpg)

Tanh梯度消失的情况比Sigmoid要好一些，

Sigmoid函数和Tanh函数都属于Sigmoid系列的激活函数。因为Tanh也可以用Sigmoid来表达。

Sigmoid系的激活函数对中央区的增益比较大，两侧的增益比较小。所以训练过程中可以将较好的训练数据推向中央区。也就是导数变化比较剧烈的区域。

缺点：计算量比较大，每个数据都设计指数运算。另外梯度消失的现象比较严重 。

Sigmoid函数最大值才0.25，经过几层传播，可能就消失了。所以深层的网络没办法使用sigmoid函数来训练的。要使用的话，可以优先考虑Tanh函数。

修正线性单元 ReLU

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148781.jpg)

优点：计算简单，梯度不会消失。如果神经元<0的时候，输出都是0，减少参数相关性，减少过拟合

·ReLU

·优点：使用ReLU得到的SGD的收敛速度会比sigmoid,/tanh快很多

。

缺点：训练的时候很”脆弱”，很容易就”die”了

举个例子：比如一个非常大的梯度，经过一个ReLU的神经元，更新过参数之后，那么这个神经元再也不会对任何数据有激活的现象，所以这个神经元的梯度永远都是0，而且当这个学习率非常大的时候，很有可能这个网络中40%的神经元会失活。

换句话说大的梯度或者说负的梯度，有的时候并不是认为不好，但是经过ReLU之后，传播的梯度就是0，那么就相当于让很多负值的神经元都变成了0，抑制了很多神经元的表达能力，所以在这个过程中我们可能会损失很多必要的信息。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172148938.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149692.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149879.jpg)

可以把softmax看成sigmoid的一种扩展，因为当类别K=2的时候，也就是一个二维向量，softmax回归会退化为logits回归。

所以在很多时候，在最后一层的时候，比如猫狗分类，车牌识别等等，通常会加入softmax这一层，来进行最后的分类输出。

在CNN中通常采用ReLU这个激活函数。

### 损失函数

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149299.jpg)

常见的损失函数

0-1损失函数

平方损失函数

绝对值损失函数

对数损失函数

全局损失函数

交叉熵损失函数

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149770.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149917.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149089.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149217.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149133.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149328.jpg)

交叉熵损失函数，经常用于分类问题当中。特别是神经网络做分类问题的时候，也会经常使用交叉熵做损失函数。

熵的概念是从信息论中出来的，熵是衡量数据的混乱程度的。

举个例子来解释怎么衡量，

现在是周一上班高峰期，你站在地铁看，看着大家都往地铁口走，现在告诉你有一个任务，需要告诉我你看到的每个人选择的交通工具，是坐地铁还是打车，还是骑自行车，而我们通讯工具是二进制的通信管道，里面只能传输0或者1，这个管道收费是1元每bit。显然你需要设计一组二进制串，每个串对应一个选择的交通工具，比如101选择的是地铁，那出租车会设计跟坐地铁同样长度的串吗？很明显根据经验来看，是不会这样设计的，因为你知道在早高峰的时候，大家一般都是坐地铁，那么做出租车时使用同样长度的bit来传输，肯定是不经济的。肯定会为坐地铁设计一个比较短的串，坐出租车设计一个比较长的串。

这个时候其实就是利用你对分布的知识来减少所需要的比特数量。

信息论里面，香农证明了，如果你知道一个变量的真实分布，那么为了使得你使用的平均bit最少，你应该给这个变量的第i个取值分配1/log(yi)个bit，其中yi是变量第i个取值的概率。

在我们刚刚举的例子当中，我们是利用了对数据分布y的了解来设计的传输方案，在这个方案中，数据的真实分布y充当了一个工具的角色，这个工具可以让我们的平均bit长度达到最小，但是在真实场景中，我们往往是不知道真实y的分布的，但是我们可以通过统计的方法得到一个y的估计。

交叉熵永远大于等于熵，因为交叉熵是在用存在错误的信息来编码数据，所以一定会比使用正确的信息要使用更多的bit

只有当yi=ai的时候，交叉熵等于熵

回到分类问题上来，假如我们通过训练得到一个模型，我们希望评估这个模型的好坏，从信道传输的角度来看，这个模型其实是提供了对真实分布y的一个估计ai，要评价这个模型的好坏，实际上是想知道，我们预测的ai，和真实的分布yi相差有多大，那么就可以使用交叉熵来度量这个差异。简单说来，交叉熵就是衡量两个分布之间的差异。所以说分类问题的损失函数，从信息论的角度来看，等价与训练出来的模型分布和真实的模型分布之间的交叉熵。而这个交叉熵的大小，衡量了训练模型与真实模型之间的差距，交叉熵越小，两者越接近，从而说明模型越准确。

所以说模型优化的本质就是在于减少数据中的信息，也就是不确定性。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149615.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149745.jpg)

上图可以看出是一个凸函数。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149234.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149573.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172149011.jpg)

上图

a是神经元的实际输出，

y是我们期望的输出

C代表cost

在实际应用当中，我们知道，参数的修正是和权重的偏导以及偏置的偏导成正比， 

![](https://gitee.com/hxc8/images1/raw/master/img/202407172149087.jpg)

这个是代表激活函数

如果采用sigmoid函数的话，那么它的导数在去绝大部分值的时候都会造成饱和现象，也就是趋近于0，从而使得参数更新的速度非常慢，甚至会造成离期望值越来越远，更新越来越慢的现象。也就是我们之前提到的梯度消失，那么怎么样克服这个问题呢？

想到了交叉熵函数，上面已经知道了熵的计算公式，在实际操作过程中，我们并不知道y的分布，只能对y的分布做一个估计，也就是算出它的预测值，那么我们通过预测值就可以表示它的真实值的交叉熵，

从上图可以看出交叉熵的导数有一个很好的性质，

求导之后是下面的式子：

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149821.jpg)

并没有激活函数的导数这一项，这样一来，就不会受到饱和性的影响，当误差大的时候，权重更新就快，当误差小的时候，权重更新的就慢。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149966.jpg)

其实对于输出层来说，它的输出函数和损失函数是由任务类型决定的，如上表，他们与隐藏层的激活函数的选择是相互独立的，

回归任务：最后一层输出层选择的是线性激活函数，损失函数选择的是平方误差。

   二分类：最后一层输出层选择的是 sigmoid 函数，损失函数选择的是二分类的cross-entropy。

  多分类：最后一层输出层选择的是 softmax  函数，损失函数选择的是多分类的cross-entropy。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149227.jpg)

上图

为什么选MSE时，要用Sigmoid以外的激活函数？

MSE(均方误差)是一个万金油，没有太大问题，但是基本不能很好的解决问题，因为sigmoid系的函数在图像两端都比较平缓，这样一个性质导会出现致梯度消失，MSE没办法处理梯度消失。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149317.jpg)

### 梯度下降

如果是一元函数，

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149309.jpg)

可以理解为该点的切线或者导数

如果是二元函数。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149237.jpg)

因为是二元函数，所以分别对x,y求偏导，就可以求得两个方向的斜率。那么这个点的斜率就可以有x和y两个方向上的斜率做一个线性组合，来表达这个方向上的梯度。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149386.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149572.jpg)

常用的梯度下降法

批量梯度下降法

随机梯度下降法

·小批量梯度下降法

样本数量不一样，导致每次的准确率和收敛速度不一样。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149900.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149187.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149262.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149607.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149706.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149848.jpg)

#### Momentum:

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149971.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149278.jpg)

实际训练过程中一般选择这个梯度下降法

#### Nesterov Momentum

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149686.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149963.jpg)

蓝色线是之前的动力方法，

由棕色和红色得到最终的绿色是Nesterov Momentum方法

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149114.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149159.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149298.jpg)

### 正则化

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149330.jpg)

上图有错别字，应该是泛化误差

#### 参数添加约束

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149301.jpg)

##### L1正则

![](https://gitee.com/hxc8/images2/raw/master/img/202407172149243.jpg)

##### L2正则

![](images/WEBRESOURCE0475da861f122c462682b064c784741d截图.png)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150920.jpg)

#### Dropout

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150000.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150979.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150077.jpg)

#### 数据增强

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150108.jpg)

#### 提前停止

![](images/WEBRESOURCEf9269ea63113a3e0e41271cf8464b9d1截图.png)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150334.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150275.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150286.jpg)

#### 集成化方法

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150560.jpg)

#### 对抗训练

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150694.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150812.jpg)

dropout:只需要在这个神经元后面再加一层dropout层，我们可以设置下让神经元失活的概率等于多少。后面不变，再加上相应的层

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150862.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150955.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150700.jpg)
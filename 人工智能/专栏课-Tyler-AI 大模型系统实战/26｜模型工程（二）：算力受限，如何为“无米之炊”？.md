 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172042612.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172042889.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172042697.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172043860.jpg)

```
input_dim = 768  # 例如，预训练模型的隐藏层大小
output_dim = 768  # 例如，层的输出大小
rank = 8  # 低秩适应的秩 'r'

W = ...  # 从预训练网络中获取，形状为 input_dim x output_dim

W_A = nn.Parameter(torch.empty(input_dim, rank))  # LoRA 权重 A
W_B = nn.Parameter(torch empty(rank, output_dim))  # LoRA 权重 B

# 初始化LoRA权重
nn.init.kaiming_uniform_(W_A, a=math.sqrt(5))
nn.init.zeros_(W_B)

def regular_forward_matmul(x, W):
    h = x @ W
    return h

def lora_forward_matmul(x, W, W_A, W_B):
    h = x @ W  # 常规矩阵相乘
    h += x @ (W_A @ W_B) * alpha  # 使用缩放的LoRA权重
    return h

```

![](https://gitee.com/hxc8/images0/raw/master/img/202407172043852.jpg)
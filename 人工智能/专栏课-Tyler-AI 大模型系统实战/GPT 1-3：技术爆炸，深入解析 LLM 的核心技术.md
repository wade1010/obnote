你好，我是Tyler。 

在今天的课程中，我们将深入探讨GPT 1-3的发展历程。GPT的主要内容其实已经体现在它的名字中，它的全称是Generative Pre-trained Transformer，其中集合了我们大模型关注的各种要素，包括预训练大模型（Pre-trained Transformer）和 生成式AI（Generative AI）。

通过上节课的学习，你已经理解了Transformer的工作原理，有了这个基础，我们再来学习GPT系列就相对轻松了。接下来，我们就从GPT-1开始说起。

## GPT-1：学会微调（Finetune）

GPT-1 符合我们之前对预训练模型的美好幻想，就像 CV 领域的预训练模型一样，首先在大规模的数据上进行学习，之后在具体的任务上继续微调。

不过，你可能会问，之前不是说过因为缺乏合适的数据集，所以一直无法制作出适合用在自然语言处理的预训练模型吗。那么，GPT-1 的训练数据是从哪里获取的呢？

这是一个非常好的问题！在这里，我们所说的不是ImageNet那样有标签的数据集，而是 Common Crawl 这类大规模的无标签数据集。

GPT-1 是基于海量的无标签数据，通过对比学习来进行训练的。这个思路来源于从 Word2Vec 到 ELMo 的发展过程中积累的经验。它们都通过在大规模文本数据上进行无监督预训练，学习到了丰富的语言知识。

GPT-1 的微调方法是，使用预训练好的模型来初始化权重，同时在 GPT-1 的预训练模型后面增加一个线性层，就像我们在 [第13节课](https://time.geekbang.org/column/article/698540) 的时候预训练 CV 领域 PTM 那样。最后只要根据下游具体 NLP 任务的训练数据来微调更新模型的参数就可以了，是不是感觉特别地熟悉？

这里和之前你学习的 CV 模型微调唯一不同的是，每个任务的适配过程里，需要在训练数据的输入上也进行相应的改造。

下图展示了四个经典 NLP 场景的改造方法，分别是文本分类、蕴含关系检测、相似性检测和多项选择题的改造方式。通过这样的微调策略，GPT-1能够迅速适应各种NLP任务，而无需重新训练整个模型。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172041301.jpg)

在理解了 GPT-1 的训练框架之后，我们再来学习一下它的具体工作流程。

就像图片里展示的那样，GPT-1 的模型结构其实就是 Transformer 的架构。与 Transformer 不同的是，GPT-1 使用了一个只包含解码器部分的架构。它是一个单向的语言模型，通过自回归地生成文本，来模拟序列生成的过程。自回归指的是使用前面的观测值，来预测未来的值，GPT 模型一个词一个词往外“蹦”，就是源于它的特性。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172041212.jpg)

在自回归生成的过程中，GPT-1 会为序列开始后的每个时刻生成一个词，并把它当作下一个时刻生成的上下文。这个过程可以用后面的伪代码来表示。当然这里有一些生成加速的实现方法，你可以先思考一下该如何实现，我们在下一章中将会具体学习。

```python
def generate_text(prompt, model, max_length=100):
    # 输入一个初始的提示语，然后逐步生成文本
    output_text = prompt
    while len(output_text) < max_length:
        # 从模型中预测下一个词
        next_word = model.predict_next_word(output_text)
        # 将下一个词添加到输出文本中
        output_text += next_word
    return output_text


```

GPT-1 在训练过程中则会根据海量数据，统筹考量不同上下文语境下生成的文本在语法和语义上的合理性，来动态调整不同词的生成概率，这个能力主要由它所使用的 12 个 Transformer 共同提供。

这便是 GPT-1 所提供的一整套完备的预训练加微调的 NLP PTM 方法论，它也是第一个提出这种方法的。从这个角度说啊， GPT-1 的创新度是在 BERT 之上的。

而且在 GPT-1 论文的最后，GPT 的作者还提出了影响深远的“零样本学习”的构想，它直接造就了我们基于 Prompt 的模型使用方式，这里先按下不表，接下来马上将会提到。

## GPT-2：放下微调（Zero-shot）

我们先趁热打铁，继续学习 GPT-2。

上节课我们曾讲到，BERT 是在 GPT-1 诞生后不久就发布的一篇工作，BERT 使用了与 GPT-1类似的 idea 和更大数据集，达到了比它更好的效果，所以 OpenAI 团队一直在憋着想要打赢这场翻身仗，他们的反击，就是 GPT-2。

GPT-2 第一次推出了 WebText 这个百万级别的数据集。BERT 你的数据多是吧？我比你还多。而且，GPT-2 还使用了比 BERT 更大的 15 亿参数的 Transformer，BERT 你模型大吧？我比你还大。有了更大的数据集和参数，GPT-2 就能更好地捕捉复杂的语言特征和更长距离的上下文语义关系。

事实证明，GPT-2在各种标准任务中表现出色。然而，OpenAI团队并没有满足于在标准任务上取得的进步，而是想到既然已经投入了大量资源来构建这个“小怪物”，为什么还需要下游任务进行微调呢？

正如我们前面按下不表的 “零样本学习”的概念。那什么是零样本学习呢，就是创造一个通用的、 **不需要任何额外样本，就能解决所有下游任务的预训练大模型**。

于是，这次 OpenAI 索性就把所有可能用到的下游任务数据汇集成了一个多任务学习（MTL）的数据集，并将这些数据都加入到了 GPT-2 模型的训练数据当中，想要看看到底能合成出一个什么样的“新物种”。

这个想法很有吸引力，但是如果不进行下游任务的微调，模型要怎么知道自己该做什么任务呢。这时，OpenAI 提出了一种影响了后续所有语言模型工作的方法，那就是通过提示词（prompt）的方式，来告知模型它需要解决什么问题。

那既然方法都已经想清楚了，就直接开干吧。OpenAI 在预训练过程中，将各类 NLP 任务的数据都放到 GPT-2 的训练数据中，帮助大模型更好地理解和生成文本。在经过这些步骤以后呢，GPT-2 的预训练模型在未经过任何微调的情况下，就能战胜许多下游任务的最佳结果。

它不仅在很多 NLP 任务上超越了 BERT，还成功地提出并完成了 “零样本学习” 这个更为困难的任务。

## GPT-3：超越微调（in-Context Learning）

不过，“零样本学习” 的方式仍然存在一定的局限性，因为下游的使用者，很难把新的下游数据注入到模型中，因为 GPT-3 预训练模型的规模已经变得非常庞大了，它是当时规模最大的模型之一，具有惊人的 1.75 万亿个参数，很少有机构有能力承担微调所需的巨大算力成本。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172041072.jpg)

于是，OpenAI 提出了一个更新的理念，也就是全新的“少样本学习”（Few-Shot Learning）的概念。这和传统意义上模型微调的“少样本学习”是不一样的。GPT-3 所提出的方式是，允许下游使用者通过提示词（prompt）直接把下游任务样本输入到模型中，让模型在提示语中学习新样本的模式和规律，这种方法的学名叫做 in-context learning。

我来举个例子帮你加深理解，我们可以用后面这种方式，使用 GPT-3 来执行翻译任务，而无需专门地进行翻译任务的微调。

```python
def translate(text, model):
    instruction = "Translate the following English text to French:\n"
    example = "sea otter => loutre de mer\n"
    task = text + " => "
    # Construct the prompt
    prompt = f"{instruction}{example}{task}"
    translation = model.generate_text(prompt)
    return translation


```

当然了，这种方法也存在缺点，其中最明显的问题是，注入样本的数量完全受限于模型能接收的最大提示词长度。这就导致 GPT 向着参数规模越来越大、训练数据越来越多，还有提示词输入长度越来越长这样的趋势发展。你在 GPT-4 的各项参数中，一定也发现了这个规律。

正是 GPT-3 这种基于提示词的开放输入方式，让用户可以直接与大语言模型（LLM）进行互动，逐渐开始感受到了大模型的"涌现"和"思维链"等能力的魅力和价值。

当然，GPT-3 只是让大型语言模型逐渐进入公众视野的第一步，虽然在当时也引发了一定程度的社会焦虑。不过啊，当时的 GPT-3 还远远没有达到 ChatGPT 的效果。

这里我想引用 OpenAI CEO 在 Twitter 上的一段话，这一定程度上反映了当时 OpenAI 对 GPT-3 的判断，这里我翻译一下：“GPT-3 的炒作过于夸张了。它的表现令人印象深刻，但它仍然存在严重缺陷，有时会犯一些非常愚蠢的错误。AI 将改变世界，但 GPT-3 只是一个非常早期的雏形，我们还有很多问题要解决。”

![](https://gitee.com/hxc8/images0/raw/master/img/202407172041949.jpg)

另外，GPT-3 的问世也引发了中小企业的担忧。这么高昂的训练成本可能会导致大公司在技术方面形成垄断，这让全球各公司逐渐认识到当中蕴含的价值，纷纷开始加入这场技术军备竞赛。这直接导致了 NVIDIA 这个不时出现在我们课程中的公司股价持续攀升，让老黄笑得合不拢嘴。

## 小结

这里我们做个总结吧。

从 Transformer 到 GPT 系列的发展，让我们见证了自然语言处理领域的巨大飞跃。Transformer 的引入改变了 NLP 任务学习方式，而 GPT 系列模型，则是通过大规模数据的预训练，在各类NLP下游任务中表现出色，在生成式预训练大模型的道路上不断前行。

GPT 系列属于憋着一口气，一条道走到黑的典范。BERT 他强任他强，我 GPT 明月照大江。GPT 从一开始就选择了更难的生成问题，功夫不负有心人，OpenAI 终于坚持到了 GPT-3 横空出世，大杀四方。

当然我们还是要客观地看待当时的大语言模型技术，虽然 GPT-3 的规模和性能不断提升，但仍然面临一些挑战和局限，比如在一些小语种和特定领域的应用上还存在很多问题，甚至还会出现“幻觉”，需要额外的处理，来确保模型生成内容的准确性和可靠性。

在下一节课中，我们将见证ChatGPT如何让OpenAI走向人工智能领域舞台的正中央，敬请期待。

## 思考题

你觉得 GPT-3 和你目前所使用的 ChatGPT 之间最大的区别是什么？

恭喜完成我们第 15 次打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。
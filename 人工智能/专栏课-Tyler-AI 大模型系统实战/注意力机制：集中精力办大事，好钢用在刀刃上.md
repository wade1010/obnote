你会发现，我们刚刚学习的LSTM在编码器阶段，将所有输入信息都被“压缩”到了一个固定长度的向量中——也就是最后一个RNN单元的隐层表示。

你也许会疑惑，这么狭小的空间真的能够容纳如此丰富的信息吗？

这是个很好的问题，因为这种“压缩”确实可能会导致关键信息的流失。正因如此，为了更加有效地捕捉输入序列中的关键信息，我们就需要引入注意力机制。 

这里我举个例子来帮你理解注意力机制。综艺节目里有个非常经典的节目——“传声筒游戏”。在这个游戏中，通常会有N个参与者，每个人有一个编号。每个人只能听取编号比自己小的人传来的话，同时只能将这段话传递给编号比自己大的下一个人。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172041808.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172041580.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172041959.jpg)

Attention机制的出色表现让AI领域的研究人员都感到兴奋不已，纷纷直呼“真香”。于是，Google的研究人员产生了一个有趣的想法：如果只用Attention这一招，会不会更香？

他们并没有停留在猜想环节，Google AI的论文 “Attention Is All You Need”应运而生。这就是大名鼎鼎的Transformer，灵感正是来自于“注意力机制”。即便 NLP 模型发展如此迅猛，还是没有走出自己的预训练模型之路，不过 Transformer 的出现为这一切带来了转机，后面的故事我下节课再为你揭秘。
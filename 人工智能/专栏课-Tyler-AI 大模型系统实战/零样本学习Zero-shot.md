我们先趁热打铁，继续学习 GPT-2。 

上节课我们曾讲到，BERT 是在 GPT-1 诞生后不久就发布的一篇工作，BERT 使用了与 GPT-1类似的 idea 和更大数据集，达到了比它更好的效果，所以 OpenAI 团队一直在憋着想要打赢这场翻身仗，他们的反击，就是 GPT-2。

GPT-2 第一次推出了 WebText 这个百万级别的数据集。BERT 你的数据多是吧？我比你还多。而且，GPT-2 还使用了比 BERT 更大的 15 亿参数的 Transformer，BERT 你模型大吧？我比你还大。有了更大的数据集和参数，GPT-2 就能更好地捕捉复杂的语言特征和更长距离的上下文语义关系。

事实证明，GPT-2在各种标准任务中表现出色。然而，OpenAI团队并没有满足于在标准任务上取得的进步，而是想到既然已经投入了大量资源来构建这个“小怪物”，为什么还需要下游任务进行微调呢？

正如我们前面按下不表的 “零样本学习”的概念。那什么是零样本学习呢，就是创造一个通用的、 **不需要任何额外样本，就能解决所有下游任务的预训练大模型**。

于是，这次 OpenAI 索性就把所有可能用到的下游任务数据汇集成了一个多任务学习（MTL）的数据集，并将这些数据都加入到了 GPT-2 模型的训练数据当中，想要看看到底能合成出一个什么样的“新物种”。

这个想法很有吸引力，但是如果不进行下游任务的微调，模型要怎么知道自己该做什么任务呢。这时，OpenAI 提出了一种影响了后续所有语言模型工作的方法，那就是通过提示词（prompt）的方式，来告知模型它需要解决什么问题。

那既然方法都已经想清楚了，就直接开干吧。OpenAI 在预训练过程中，将各类 NLP 任务的数据都放到 GPT-2 的训练数据中，帮助大模型更好地理解和生成文本。在经过这些步骤以后呢，GPT-2 的预训练模型在未经过任何微调的情况下，就能战胜许多下游任务的最佳结果。

它不仅在很多 NLP 任务上超越了 BERT，还成功地提出并完成了 “零样本学习” 这个更为困难的任务。
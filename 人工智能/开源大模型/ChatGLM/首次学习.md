ChatGLM-6B 简价

ChatGLM-6B是一个开源的、支持中英双语的对话语言模型，基于General LanguageModel(GLM)架构，具有 62 亿参数。

更新 v1.1 版本 checkpoint，训练数据增加英文指令微调数据以平衡中英文数据比例，解决英文回答中夹杂中文词语的现象

![](https://gitee.com/hxc8/images2/raw/master/img/202407172158023.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172158426.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159352.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159359.jpg)

大概需要3个小时

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159995.jpg)

去掉上面 --quantization_bit就会使用fp16(这个是浮点数精度计算的，  bit4精确度高，但是计算就慢了，有些计算其实不需要太精确，所以可以用精确度较低的fp16)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159054.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159342.jpg)

RTX 3090 有24G显存

用P-tuning+4-bit量化的话其实7G就够了，

但是如果是P-tuning不做量化的话需要十几个G

如果做全量微调的话，8张RTX 3090好像是跑不起来，但是4张A100的80G版本就可以跑起来，8张的话40G版本也行

如果是Linux可以起一个torch container，可以方便的帮助你配置环境

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159077.jpg)

Step 2: Download largeiles from Tsinghua Cloud

download one by ones painful ...

git clone git@github.com:chenyifanthu/THU-Cloud-Downloader.git

cd THU-Cloud-Downloader

pip install argparse requests tqdm

python main .py --link [https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/](https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/) --save ../chatglm-6b/

```
(venv) ~/ChatGLM-6B> nvdisa-smi
bash: nvdisa-smi: command not found
(venv) ~/ChatGLM-6B> nvidia-smi
Sun Jun 18 05:22:30 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:07.0 Off |                    0 |
| N/A   34C    P0    54W / 300W |  13361MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
```

fine-tuning

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159706.jpg)

### p-tuning 微调

参考 [https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md](https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md)

- **安装依赖**

```
pip install rouge_chinese nltk jieba datasets
```

- **下载数据集**

在源码的ptuning目录下进行下载

wget [https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1)      

然后用tar命令解压

- **修改脚本**

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159341.jpg)

修改为你的模型下载到服务器上的目录

- **训练**

```
bash train.sh
```

```
train.sh 中的 PRE_SEQ_LEN 和 LR 分别是 soft prompt 长度和训练的学习率，可以进行调节以取得最佳的效果。P-Tuning-v2 方法会冻结全部的模型参数，可通过调整 quantization_bit 来被原始模型的量化等级，不加此选项则为 FP16 精度加载。
在默认配置 quantization_bit=4、per_device_train_batch_size=1、gradient_accumulation_steps=16 下，INT4 的模型参数被冻结，一次训练迭代会以 1 的批处理大小进行 16 次累加的前后向传播，等效为 16 的总批处理大小，此时最低只需 6.7G 显存。若想在同等批处理大小下提升训练效率，可在二者乘积不变的情况下，加大 per_device_train_batch_size 的值，但也会带来更多的显存消耗，请根据实际情况酌情调整。
如果你想要从本地加载模型，可以将 train.sh 中的 THUDM/chatglm-6b 改为你本地的模型路径。
```

Finetune

如果需要进行全参数的 Finetune，需要安装 [Deepspeed](https://github.com/microsoft/DeepSpeed)，然后运行以下指令：

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159387.jpg)

```
bash ds_train_finetune.sh
```

#### 推理

在 P-tuning v2 训练时模型只保存 PrefixEncoder 部分的参数，所以在推理时需要同时加载原 ChatGLM-6B 模型以及 PrefixEncoder 的权重，因此需要指定 evaluate.sh 中的参数：

```
--model_name_or_path THUDM/chatglm-6b
--ptuning_checkpoint $CHECKPOINT_PATH
```

仍然兼容旧版全参保存的 Checkpoint，只需要跟之前一样设定 model_name_or_path：

```
--model_name_or_path $CHECKPOINT_PATH
```

评测指标为中文 Rouge score 和 BLEU-4。生成的结果保存在 ./output/adgen-chatglm-6b-pt-8-1e-2/generated_predictions.txt。

evaluate.sh

```
PRE_SEQ_LEN=128
CHECKPOINT=adgen-chatglm-6b-pt-128-2e-2
STEP=3000

CUDA_VISIBLE_DEVICES=0 python3 main.py \
    --do_predict \
    --validation_file AdvertiseGen/dev.json \
    --test_file AdvertiseGen/dev.json \
    --overwrite_cache \
    --prompt_column content \
    --response_column summary \
    --model_name_or_path ../../chatglm-6b \
    --ptuning_checkpoint ./output/$CHECKPOINT/checkpoint-$STEP \
    --output_dir ./output/$CHECKPOINT \
    --overwrite_output_dir \
    --max_source_length 64 \
    --max_target_length 64 \
    --per_device_eval_batch_size 1 \
    --predict_with_generate \
    --pre_seq_len $PRE_SEQ_LEN \
    --quantization_bit 4

```

 

time bash evaluate.sh

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159510.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159447.jpg)

### 例子

- Input: 类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞

- Label: 简约而不简单的牛仔外套,白色的衣身十分百搭。衣身多处有做旧破洞设计,打破单调乏味,增加一丝造型看点。衣身后背处有趣味刺绣装饰,丰富层次感,彰显别样时尚。

- Output[微调前]: 这件上衣的材质是牛仔布,颜色是白色,风格是简约,图案是刺绣,衣样式是外套,衣款式是破洞。

- Output[微调后]: 这是一款简约的牛仔外套,破洞设计,将牛仔布破洞,带来一种随意与个性。破洞的牛仔外套,展现出时尚气息,带来一种休闲感。同时,刺绣图案,让整件外套更加立体。

label应该是模型原来要求的输入

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159022.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159460.jpg)
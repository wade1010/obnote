2.6 api 介绍

1 梯度下降法

- sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)

	- SGDRegressor类实现了随机梯度下降学习，它支持不同的**loss函数和正则化惩罚项**来拟合线性回归模型。

	- 参数：

		- loss:损失类型

			- loss=”squared_loss”: 普通最小二乘法

		- fit_intercept：是否计算偏置

		- learning_rate : string, optional

			- 学习率填充

			- 'constant': eta = eta0

			- 'optimal': eta = 1.0 / (alpha * (t + t0))

			- 'invscaling': eta = eta0 / pow(t, power_t)[default]

				- power_t=0.25:存在父类当中

			- 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。

	- 属性：

		- SGDRegressor.coef_：回归系数

		- SGDRegressor.intercept_：偏置

2.8 欠拟合和过拟合

欠拟合：

在训练集熵表现不好，在测试集熵表现不好

解决办法：

继续学习

1 添加其他特征项

2 添加多项式特征

过拟合：

在训练集熵表现好，在测试集熵表现不好

解决办法：

1 重新清晰数据集

2 增大数据的训练量

3 正则化

4 减少特征维度

正则化

通过限制高次项的系数进行防止过拟合

L1正则化

理解：直接把告辞项前面的系数变为0

Lasso回归

L2正则化

理解：把高次项系数前面的系数变成特别小的值

岭回归

2.9 正则化线性模型

 1 Ridge Regression 岭回归

就是把系数前面添加平方项

然后限制系数值的大小

α值越小，系数值越大，α越大，系数值越小

2 Lasso 回归

对系数值进行绝对值处理

由于绝对值在顶点处不可导，所以进行计算的过程中产生很多0，最后得到结果为：稀疏矩阵

3 Elastic Net 弹性网络

是前面两个内容的综合

设置一个r，如果r=0，是岭回归，r=1，是Lasso回归

4 Early stopping

通过限制错误率的阈值，进行停止

2.10 线性回归的改进-岭回归 

1 api

sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)

具有l2正则化的线性回归

alpha:正则化力度，也叫 λ

solver:会根据数据自动选择优化方法

sag:如果数据集、特征都比较大，选择该随机梯度下降优化（SAG）

normalize:数据是否进行标准化

normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据

Ridge.coef_:回归权重

Ridge.intercept_:回归偏置

3 逻辑回归

3.1 逻辑回归介绍

1 逻辑回归概念

解决的是一个二分类问题

逻辑回归的输入是线性回归的输出

2 原理

1 输入：

线性回归的输出

2 激活函数

sigmoid函数

把整体的值映射到[0,1]

再设置一个阈值，进行分类判断

3 损失

对数似然损失

借助log思想，进行完成

真实值等于0，等于1两种情况进行划分

4 优化

提升原本属于1类别的概率，降低原本是0类别的概率

3.2 逻辑回归api介绍

- sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)

	- solver可选参数:{'liblinear', 'sag', 'saga','newton-cg', 'lbfgs'}，

		- 默认: 'liblinear'；用于优化问题的算法。

		- 对于小数据集来说，“liblinear”是个不错的选择，而“sag”和'saga'对于大型数据集会更快。

		- 对于多类问题，只有'newton-cg'， 'sag'， 'saga'和'lbfgs'可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。

	- penalty：正则化的种类

	- C：正则化力度

> **默认将类别数量少的当做正例**


注意：回归、分类API有时候是可以混合使用的

3.3 案例：癌症分类预测-良 /恶性乳腺癌肿瘤预测

1 获取数据

2 基本数据处理

2.1 缺失值处理

2.2 确定特征值、目标值

2.3 分割数据

3 特征工程（这里是标准化）

4机器学习（逻辑回归）

5模型评估

3.4 分类评估方法

1 混淆矩阵

真正例（TP）

伪反例（FN）

伪正例（FP）

真反例（TN）

2 精确率（precision）召回率（recall）

准确率：

（TP+TN）/(TP+TN+FP+FN)

精确度——查的准不准

TP/(TP+FP)

召回率——查的全不全

TP/(TP+FN)

3 api

- sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )

	- y_true：真实目标值

	- y_pred：估计器预测目标值

	- labels:指定类别对应的数字

	- target_names：目标类别名称

	- return：每个类别精确率与召回率

4 roc曲线和auc指标

roc曲线

通过tpr和fpr来进行图形绘制，然后绘制之后，形成一个指标auc

auc

越接近1，效果越好

越接近0，效果越差

注意：

这个指标主要用于评价不平衡的二分类问题

5 api

- sklearn.metrics.roc_auc_score(y_true, y_score)

	- 计算ROC曲线面积，即AUC值

	- y_true：每个样本的真实类别，必须为0(反例),1(正例)标记

	- y_score：预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值

注意：y_true——要把正例转换为1，反例转换为0

3.5 ROC曲线的绘制

1 构建模型，把模型的概率值从大到小进行排序

2 从概率最大的点开始取值，一直进行tpr和fpr的计算，然后构建整体模型，得到结果

3 其实就是在求解积分（面积）

4 决策树算法

4.1 决策树算法简介

简介

定义：

是一种树形构造，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶子结点代表一种分类结果，本质是一颗多个判断节点组成的树

4.2 决策树分类原理

1 熵

用于衡量一个对象的有序程度

系统越有序，熵值月底；系统越混乱或者分散，熵值越高。

2 信息熵

1 从信息的完整性上进行的描述：

当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。

2 从信息的有序性上进行的描述

当数据量一致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高。

3 把信息转换成熵值

-P*log2P
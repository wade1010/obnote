## 1 损失函数

总损失定义为：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144620.jpg)

- yi为第i个训练样本的真实值

- h(xi)为第i个训练样本特征值组合预测函数

- 又称最小二乘法

如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！

## 2 优化算法

**如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）**

- 线性回归经常使用的两种优化算法

	- 正规方程

	- 梯度下降法

### 2.1 正规方程

#### 2.1.1 什么是正规方程

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144626.jpg)

理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果

缺点：当特征过多过复杂时，求解速度太慢并且得不到结果

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144441.jpg)

#### 2.1.2 正规方程求解举例

以下表示数据为例：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144529.jpg)

即：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144424.jpg)

运用正规方程方法求解参数：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144278.jpg)

#### 2.1.3 正规方程的推导

- 推导方式1：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144145.jpg)

- 推导方式2：

把该损失函数转换成矩阵写法：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144238.jpg)

其中y是真实值矩阵，X是特征值矩阵，w是权重矩阵

把损失函数分开书写：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144973.jpg)

对展开上式进行求导：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144662.jpg)

需要求得求导函数的极小值，即上式求导结果为0，经过化解，得结果为：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144566.jpg)

补充：需要用到的矩阵求导公式：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144473.jpg)

### 2.2 梯度下降

#### 2.2.1 什么是梯度下降

梯度下降法(Gradient Descent)的基本思想可以类比为一个下山的过程。

假设这样一个场景：

一个人**被困在山上，需要从山上下来**(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。

因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。

具体来说就是，以他当前的所处的位置为基准，**寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走**，（同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走）。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144427.jpg)

梯度下降的基本过程就和下山的场景很类似。

首先，我们有一个**可微分的函数**。这个函数就代表着一座山。

我们的目标就是找到**这个函数的最小值**，也就是山底。

根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是**找到给定点的梯度** ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数值变化最快的方向。 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。

#### 2.2.2 梯度的概念

梯度是微积分中一个很重要的概念

- 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；

- 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；

	- 在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。

> 


这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的反方向一直走，就能走到局部的最低点！

#### 2.2.3 梯度下降举例

- 1. 单变量函数的梯度下降

我们假设有一个单变量的函数 :J(θ) = θ2

函数的微分:J、(θ) = 2θ

初始化，起点为： θ0 = 1

学习率：α = 0.4

我们开始进行梯度下降的迭代计算过程:

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144251.jpg)

如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144581.jpg)

- 2.多变量函数的梯度下降

我们假设有一个目标函数 ：:J(θ) = θ12 + θ22

现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下 来，我们会从梯度下降算法开始一步步计算到这个最小值! 我们假设初始的起点为: θ0 = (1, 3)

初始的学习率为:α = 0.1

函数的梯度为:▽:J(θ) =< 2θ1 ,2θ2>

进行多次迭代:

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144509.jpg)

我们发现，已经基本靠近函数的最小值点

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144590.jpg)

#### 2.2.4 梯度下降公式

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144553.jpg)

- 1) α是什么含义？

α在梯度下降算法中被称作为**学习率**或者**步长**，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144310.jpg)

- 2) 为什么梯度要乘以一个负号？

梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号

我们通过两个图更好理解梯度下降的过程

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144443.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144593.jpg)

**所以有了梯度下降这样一个优化算法，回归就有了"自动学习"的能力**

## 3 梯度下降和正规方程的对比

### 3.1 两种方法对比

| 梯度下降 | 正规方程 | 
| -- | -- |
| 需要选择学习率 | 不需要 | 
| 需要迭代求解 | 一次运算得出 | 
| 特征数量较大可以使用 | 需要计算方程，时间复杂度高O(n3) | 


经过前面的介绍，我们发现最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。

- 首先，最小二乘法需要计算X

TX的逆矩阵，**有可能它的逆矩阵不存在**，这样就没有办法直接用最小二乘法了。

	- 此时就需要使用梯度下降法。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让X

TX的行列式不为0，然后继续使用最小二乘法。

- 第二，当样本特征n非常的大的时候，计算X

TX的逆矩阵是一个非常耗时的工作（n x n的矩阵求逆），甚至不可行。

	- 此时以梯度下降为代表的迭代法仍然可以使用。

	- 那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。

- 第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。

- 第四，以下特殊情况，。

	- 当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。

	- 当样本量m等于特征数n的时候，用方程组求解就可以了。

	- 当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。

### 3.2 算法选择依据

- 小规模数据：

	- 正规方程：**LinearRegression(不能解决拟合问题)**

	- 岭回归

- 大规模数据：

	- 梯度下降法：**SGDRegressor**

经过前面介绍，我们发现在真正的开发中，我们使用梯度下降法偏多（深度学习中更加明显），下一节中我们会进一步介绍梯度下降法的一些原理。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144778.jpg)

## 4 小结

- 损失函数【知道】

	- 最小二乘法

- 线性回归优化方法【知道】

	- 正规方程

	- 梯度下降法

- 正规方程 -- 一蹴而就【知道】

	- 利用矩阵的逆,转置进行一步求解

	- 只是适合样本和特征比较少的情况

- 梯度下降法 — 循序渐进【知道】

	- 梯度的概念

		- 单变量 -- 切线

		- 多变量 -- 向量

	- 梯度下降法中关注的两个参数

		- α -- 就是步长

			- 步长太小 -- 下山太慢

			- 步长太大 -- 容易跳过极小值点(*****)

		- 为什么梯度要加一个负号

			- 梯度方向是上升最快方向,负号就是下降最快方向

- 梯度下降法和正规方程选择依据【知道】

	- 小规模数据：

		- 正规方程：**LinearRegression(不能解决拟合问题)**

		- 岭回归

	- 大规模数据：

		- 梯度下降法：**SGDRegressor**
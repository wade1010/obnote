## 梯度下降法大家族

首先，我们来看一下，常见的梯度下降算法有：

- 全梯度下降算法(Full gradient descent),

- 随机梯度下降算法(Stochastic gradient descent),

- 小批量梯度下降算法(Mini-batch gradient descent),

- 随机平均梯度下降算法(Stochastic average gradient descent)

它们都是为了正确地调节权重向量，通过为每个权重计算一个梯度，从而更新权值，使目标函数尽可能最小化。其差别在于样本的使用方式不同。

### 2.1 全梯度下降算法(FG)

批量梯度下降法，是梯度下降法最常用的形式，**具体做法也就是在更新参数时使用所有的样本来进行更新。**

**计算训练集所有样本误差**，**对其求和再取平均值作为目标函数**。

权重向量沿其梯度相反的方向移动，从而使当前目标函数减少得最多。

其是在整个训练数据集上计算损失函数关于参数θ 的梯度：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144796.jpg)

> 由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。


注意：

- 因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。

- 批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本

### 2.2 随机梯度下降算法(SG)

由于FG每迭代更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训练样本，故效率偏低，且容易陷入局部最优解，因此提出了随机梯度下降算法。

其每轮计算的目标函数不再是全体样本误差，而仅是单个样本误差，即**每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。**

此过程简单，高效，通常可以较好地避免更新迭代收敛到局部最优解。其迭代形式为

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144331.jpg)

但是由于，SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。

### 2.3 小批量梯度下降算法

小批量梯度下降(mini-batch)算法是FG和SG的折中方案,在一定程度上兼顾了以上两种方法的优点。

**每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。**

被抽出的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更有利于GPU加速处理。

特别的，若batch_size=1，则变成了SG；若batch_size=n，则变成了FG.其迭代形式为

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144644.jpg)

上式中，也就是我们从m个样本中，选择x个样本进行迭代(1<x<m),

### 2.4 随机平均梯度下降算法(SAG)

在SG方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG效果常不尽如人意，因为每一轮梯度更新都完全与上一轮的数据和梯度无关。

**随机平均梯度算法克服了这个问题，在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。**

如此，每一轮更新仅需计算一个样本的梯度，计算成本等同于SG，但收敛速度快得多。

其迭代形式为：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172144640.jpg)

- 我们知道sgd是当前权重减去步长乘以梯度，得到新的权重。sag中的a，就是平均的意思，具体说，就是在第k步迭代的时候，我考虑的这一步和前面n-1个梯度的平均值，当前权重减去步长乘以最近n个梯度的平均值。

- n是自己设置的，当n=1的时候，就是普通的sgd。

- 这个想法非常的简单，在随机中又增加了确定性，类似于mini-batch sgd的作用，但不同的是，sag又没有去计算更多的样本，只是利用了之前计算出来的梯度，所以每次迭代的计算成本远小于mini-batch sgd，和sgd相当。效果而言，sag相对于sgd，收敛速度快了很多。这一点下面的论文中有具体的描述和证明。

- SAG论文链接：

## 3 小结

- 全梯度下降算法(FG)【知道】

	- 在进行计算的时候,计算所有样本的误差平均值,作为我的目标函数

- 随机梯度下降算法(SG)【知道】

	- 每次只选择一个样本进行考核

- 小批量梯度下降算法(mini-batch)【知道】

	- 选择一部分样本进行考核

- 随机平均梯度下降算法(SAG)【知道】

	- 会给每个样本都维持一个平均值,后期计算的时候,参考这个平均值
K近邻既能处理分类，又能处理回归

前面已经讲到，关于数据类型，我们主要可以把其分为两类，**连续型数据和离散型数据。**在面对不同数据时，决策树也可以分为两大类型：

- 分类决策树和回归决策树。

- 前者主要用于处理离散型数据，后者主要用于处理连续型数据。

## 1.原理概述

不管是回归决策树还是分类决策树，都会存在两个核心问题：

- 如何选择划分点？

- 如何决定叶节点的输出值？

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值。分类树中，我们采用信息论中的方法，通过计算选择最佳划分点。

而在回归树中，采用的是启发式的方法。**假如我们有n个特征，每个特征有**

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141953.jpg)

**个取值，那我们遍历所有特征，尝试该特征所有取值，对空间进行划分，直到取到特征 j 的取值 s，使得损失函数最小，这样就得到了一个划分点。**描述该过程的公式如下：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141139.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141218.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141338.jpg)

举例：

如下图，假如我们想要对楼内居民的年龄进行回归，将楼划分为3个区域R1,R2,R3（红线），

那么R1的输出就是第一列四个居民年龄的平均值，

R2的输出就是第二列四个居民年龄的平均值，

R3的输出就是第三、四列八个居民年龄的平均值。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141405.jpg)

## 2.算法描述

- 输入：训练数据集D:

- 输出：回归树f(x)​.

- 在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：

（1）选择最优切分特征j与切分点s，求解

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141246.jpg)

遍历特征j,对固定的切分特征j扫描切分点s,选择使得上式达到最小值的对 (j,s).

（2）用选定的对(j,s)划分区域并决定相应的输出值：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141115.jpg)

- （3）继续对两个子区域调用步骤（1）和（2），直至满足停止条件。

- （4）将输入空间划分为M个区域R1,R2,……,Rm, 生成决策树：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141989.jpg)

## 3.简单实例

为了易于理解，接下来通过一个简单实例加深对回归决策树的理解。

训练数据见下表，目标是得到一棵最小二乘回归树。

| x | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| y | 5.56 | 5.7 | 5.91 | 6.4 | 6.8 | 7.05 | 8.9 | 8.7 | 9 | 9.05 | 


### 3.1 实例计算过程

**（1）选择最优的切分特征j与最优切分点s：**

- 确定第一个问题：选择最优切分特征：

	- 在本数据集中，只有一个特征，因此最优切分特征自然是x。

- 确定第二个问题：我们考虑9个切分点 [1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5] 。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141887.jpg)

**a、计算子区域输出值：**

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141734.jpg)

同理，得到其他各切分点的子区域输出值，如下表：

| s | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 6.5 | 7.5 | 8.5 | 9.5 | 
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| c1 | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 | 6.24 | 6.62 | 6.88 | 7.11 | 
| c2 | 7.5 | 7.73 | 7.99 | 8.25 | 8.54 | 8.91 | 8.92 | 9.03 | 9.05 | 


**b、计算损失函数值，找到最优切分点：**

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141497.jpg)

同理，计算得到其他各切分点的损失函数值，可获得下表：

| s | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 6.5 | 7.5 | 8.5 | 9.5 | 
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |
| m(s) | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 | 


显然取 s=6.5时，m(s)最小。因此，第一个划分变量【j=x,s=6.5】

**（2）用选定的(j,s)划分区域，并决定输出值;**

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141369.jpg)

**（3）调用步骤 (1)、(2)，继续划分：**

对R1继续进行划分:

| x | 1 | 2 | 3 | 4 | 5 | 6 | 
| -- | -- | -- | -- | -- | -- | -- |
| y | 5.56 | 5.7 | 5.91 | 6.4 | 6.8 | 7.05 | 


取切分点[1.5,2.5,3.5,4.5,5.5]，则各区域的输出值c如下表：

| s | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 
| -- | -- | -- | -- | -- | -- |
| c1 | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 | 
| c2 | 6.37 | 6.54 | 6.75 | 6.93 | 7.05 | 


计算损失函数值m(s)：

| s | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 
| -- | -- | -- | -- | -- | -- |
| m(s) | 1.3087 | 0.754 | 0.2771 | 0.4368 | 1.0644 | 


s=3.5时，m(s)最小。

**（4）生成回归树**

**假设**在生成**3个区域**之后停止划分，那么最终生成的回归树形式如下：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141220.jpg)

```
import numpy as np
from matplotlib import pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression

# 生成数据
x = np.array(list(range(1,11)))
x

x = x.reshape(-1,1)
x

y = [5.56,5.70,5.91,6.40,6.80,7.05,8.90,8.70,9.00,9.05]

# 模型训练
m1 = DecisionTreeRegressor(max_depth=1)
m2 = DecisionTreeRegressor(max_depth=3)
m3 = LinearRegression()

m1.fit(x,y)
m2.fit(x,y)
m3.fit(x,y)

# 模型预测
x_test = np.arange(0,10,0.01).reshape(-1,1)


y_predict1 = m1.predict(x_test)
y_predict2 = m2.predict(x_test)
y_predict3 = m3.predict(x_test)

# 结果可视化
plt.figure(figsize=(20,8),dpi=100)
plt.scatter(x,y,label="data")
plt.plot(x_test,y_predict1,label='max_depth=1')
plt.plot(x_test,y_predict2,label='max_depth=3')
plt.plot(x_test,y_predict3,label='linear regression')
plt.xlabel('data')
plt.ylabel('predict')
plt.legend()
plt.show()
```

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141009.jpg)

## 4 小结

- 回归决策树算法总结【指导】

	- 输入：训练数据集D:

	- 输出：回归树f(x).

	- 流程：在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：

- （1）选择最优切分**特征j与切分点s**，求解

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141914.jpg)

遍历特征j,对固定的切分特征j扫描切分点s,选择使得上式达到最小值的对(j,s).

- （2）用选定的对(j,s)划分区域并决定相应的输出值：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141820.jpg)

- （3）继续对两个子区域调用步骤（1）和（2），直至满足停止条件。

- （4）将输入空间划分为M个区域R1,R2,……,Rm, 生成决策树：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172141968.jpg)
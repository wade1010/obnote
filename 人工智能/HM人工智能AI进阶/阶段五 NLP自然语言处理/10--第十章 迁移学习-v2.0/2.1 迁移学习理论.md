### 学习目标

- 了解迁移学习中的有关概念.

- 掌握迁移学习的两种迁移方式.

- 迁移学习中的有关概念:

	- 预训练模型

	- 微调

	- 微调脚本

- 预训练模型(Pretrained model):

	- 一般情况下预训练模型都是大型模型，具备复杂的网络结构，众多的参数量，以及在足够大的数据集下进行训练而产生的模型. 在NLP领域，预训练模型往往是语言模型，因为语言模型的训练是无监督的，可以获得大规模语料，同时语言模型又是许多典型NLP任务的基础，如机器翻译，文本生成，阅读理解等，常见的预训练模型有BERT, GPT, roBERTa, transformer-XL等.

- **微调(Fine-tuning):**

	- 根据给定的预训练模型，改变它的部分参数或者为其新增部分输出结构后，通过在小部分数据集上训练，来使整个模型更好的适应特定任务.

做下游任务的时候，我们为了让预训练模型提升，等到一个更好的效果，怎么办？就可以进行finetune。举个例子，神经网络有100层，改变它的部分参数，假如只把最后两层或者最后三层，把输出结构给变了，假如之前是一个512的全连接层，把它换成256的全连接层，然后加上一些dropout的工作。

或者在后面新添加两层。

就是把最后部分的参数和少部分的结构做一个改变。然后通过小数据集上的训练，为什么是小数据集？比如说我们这个财经领域，我们有一些小数据，在医疗领域，有一些小数据，在这个小数据集上重新拿过来，让它通过完整的前面这个大型网络以及通过我们最后微调的这个小网络，那么模型是不是因为我们给它改变了部分参数，或者改变了最后两层，或者新增了两层，那么是不是相当于最后两层的参数跟之前不一样了？那我们把小部分数据集扔上去训练之后，是不是就把最后的两层又给练出来了？这个时候，绝大多数层是没有动的，只有最后两层简单动了，这个就是所谓的finetune，微调完成之后，整个模型，就可以更好的适应我们当前的这个任务。

- 微调脚本(Fine-tuning script):

	- 实现微调过程的代码文件。这些脚本文件中，应包括对预训练模型的调用，对微调参数的选定以及对微调结构的更改等，同时，因为微调是一个训练过程，它同样需要一些超参数的设定，以及损失函数和优化器的选取等, 因此微调脚本往往也包含了整个迁移学习的过程.

- 关于微调脚本的说明:

	- 一般情况下，微调脚本应该由不同的任务类型开发者自己编写，但是由于目前研究的NLP任务类型（分类，提取，生成）以及对应的微调输出结构都是有限的，有些微调方式已经在很多数据集上被验证是有效的，因此微调脚本也可以使用已经完成的规范脚本.

- 两种迁移方式:

	- 直接使用预训练模型，进行相同任务的处理，不需要调整参数或模型结构，这些模型开箱即用。但是这种情况一般只适用于普适任务, 如：fasttest工具包中预训练的词向量模型。另外，很多预训练模型开发者为了达到开箱即用的效果，将模型结构分各个部分保存为不同的预训练模型，提供对应的加载方法来完成特定目标.

	- 更加主流的迁移学习方式是发挥预训练模型特征抽象的能力，然后再通过微调的方式，通过训练更新小部分参数以此来适应不同的任务。这种迁移方式需要提供小部分的标注数据来进行监督学习.

- 关于迁移方式的说明:

	- 直接使用预训练模型的方式, 已经在fasttext的词向量迁移中学习. 接下来的迁移学习实践将主要讲解通过微调的方式进行迁移学习.

微调（Fine-tuning）

- 微调的对象：预训练模型

- 微调的部分：改变模型的部分参数或是新增加的输出结构

- 微调的动作：较小的语料集上进行训练

- 微调的目标：使得预训练模型可以完成特定的任务需求
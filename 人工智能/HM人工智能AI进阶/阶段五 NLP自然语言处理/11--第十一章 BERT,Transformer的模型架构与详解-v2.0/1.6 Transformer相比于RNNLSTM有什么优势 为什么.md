## 1.6 Transformer相比于RNN/LSTM有什么优势? 为什么?

### 学习目标

- 掌握Transformer相比于RNN/LSTM的优势和背后的原因.

### Transformer的并行计算

- 对于Transformer比传统序列模型RNN/LSTM具备优势的第一大原因就是强大的并行计算能力.

	- 对于RNN来说, 任意时刻t的输入是时刻t的输入x(t)和上一时刻的隐藏层输出h(t-1), 经过运算后得到当前时刻隐藏层的输出h(t), 这个h(t)也即将作为下一时刻t+1的输入的一部分. 这个计算过程是RNN的本质特征, RNN的历史信息是需要通过这个时间步一步一步向后传递的. 而这就意味着RNN序列后面的信息只能等到前面的计算结束后, 将历史信息通过hidden state传递给后面才能开始计算, 形成链式的序列依赖关系, 无法实现并行.

	- 对于Transformer结构来说, 在self-attention层, 无论序列的长度是多少, 都可以一次性计算所有单词之间的注意力关系, 这个attention的计算是同步的, 可以实现并行.

### Transformer的特征抽取能力

- 对于Transformer比传统序列模型RNN/LSTM具备优势的第二大原因就是强大的特征抽取能力.

	- Transformer因为采用了Multi-head Attention结构和计算机制, 拥有比RNN/LSTM更强大的特征抽取能力, 这里并不仅仅由理论分析得来, 而是大量的试验数据和对比结果, 清楚的展示了Transformer的特征抽取能力远远胜于RNN/LSTM.

	- 注意: 不是越先进的模型就越无敌, 在很多具体的应用中RNN/LSTM依然大有用武之地（比如人名识别，这种短文本序列依然是要采用RNN LSTM的，因为模型简单，我们训练起来容易。）, 要具体问题具体分析.

### 小节总结

- 学习了Transformer相比于RNN/LSTM的优势和原因.

	- 1: 第一大优势是并行计算的优势.

	- 2: 第二大优势是特征提取能力强.
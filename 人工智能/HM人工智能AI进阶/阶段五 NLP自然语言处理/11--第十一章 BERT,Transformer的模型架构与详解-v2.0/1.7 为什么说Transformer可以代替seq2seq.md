## 1.7 为什么说Transformer可以代替seq2seq?

### 学习目标

- 掌握Transformer可以替代seq2seq的核心原因.

### seq2seq的两大缺陷

- 1: seq2seq架构的第一大缺陷是将Encoder端的所有信息压缩成一个固定长度的语义向量中, 用这个固定的向量来代表编码器端的全部信息. 这样既会造成信息的损耗, 也无法让Decoder端在解码的时候去用注意力聚焦哪些是更重要的信息.

- 2: seq2seq架构的第二大缺陷是无法并行, 本质上和RNN/LSTM无法并行的原因一样.

缺陷一补充：比如在英译法任务中，在整个英文的编码输入进来之后，我们是不是把它压缩成一个语义张量C，接下来这个语义张量C会作用到解码器的每一步。

不管这个英文语句长度是10还是50，把它压缩成一个长度固定的语义张量中，这里面一定会造成信息的损耗，压缩的损耗，这个是不可避免的，而且当你把这段话压缩成一个固定的张量之后，我们没有办法让decoder在解码的时候采用注意力机制，因为你已经压缩成一个固定长度的张量了，原始的英文文本到底到底哪个词？我们应该把张量往哪里聚焦，这个没法做了。

缺陷二补充：在英译法案例中，在encoder端，是一个英文单词一个英文单词的往里输的，经过一个for循环，把它压缩成一个中间的张量，也就是我们代码里面的encoder outputs，最后到了解码器端，我们也是解码器端一个字符一个字符的扔进去，一个字符一个字符的解，比如解码20个字，for循环就要20次。

### Transformer的改进

- Transformer架构同时解决了seq2seq的两大缺陷, 既可以并行计算, 又应用Multi-head Attention机制来解决Encoder固定编码的问题, 让Decoder在解码的每一步可以通过注意力去关注编码器输出中最重要的那些部分.

### 小节总结

- 学习了seq2seq架构的两大缺陷.

	- 第一个缺陷是Encoder端的所有信息被压缩成一个固定的输出张量, 当序列长度较长时会造成比较严重的信息损耗.

	- 第二个缺陷是无法并行计算.

- 学习了Transformer架构对seq2seq两大缺陷的改进.

	- Transformer应用Multi-head Attention机制让编码器信息可以更好的展示给解码器.

	- Transformer可以实现Encoder端的并行计算.
## 1.4 Transformer中一直强调的self-attention是什么? 为什么能发挥如此大的作用? 计算的时候如果不使用三元组(Q, K, V), 而仅仅使用(Q, V)或者(K, V)或者(V)行不行?

### 学习目标

- 掌握self-attention的机制和原理.

- 掌握为什么要使用三元组(Q, K, V)来计算self-attention.

### self-attention的机制和原理

- self-attention是一种通过自身和自身进行关联的attention机制, 从而得到更好的representation来表达自身.

- self-attention是attention机制的一种特殊情况:

	- 在self-attention中, Q=K=V, 序列中的每个单词(token)都和该序列中的其他所有单词(token)进行attention规则的计算.

- attention机制计算的特点在于, 可以直接跨越一句话中不同距离的token, 可以远距离的学习到序列的知识依赖和语序结构.

![](https://gitee.com/hxc8/images1/raw/master/img/202407172130884.jpg)

> 从上图中可以看到, self-attention可以远距离的捕捉到语义层面的特征(its的指代对象是Law).
> 应用传统的RNN, LSTM, 在获取长距离语义特征和结构特征的时候, 需要按照序列顺序依次计算, 距离越远的联系信息的损耗越大, 有效提取和捕获的可能性越小.
> 但是应用self-attention时, 计算过程中会直接将句子中任意两个token的联系通过一个计算步骤直接联系起来,


- 关于self-attention为什么要使用(Q, K, V)三元组而不是其他形式:

	- 首先一条就是从分析的角度看, 查询Query是一条独立的序列信息, 通过关键词Key的提示作用, 得到最终语义的真实值Value表达, 数学意义更充分, 完备.

	- 这里不使用(K, V)或者(V)没有什么必须的理由, 也没有相关的论文来严格阐述比较试验的结果差异, 所以可以作为开放性问题未来去探索, 只要明确在经典self-attention实现中用的是三元组就好.

### 小节总结

- self-attention机制的重点是使用三元组(Q, K, V)参与规则运算, 这里面Q=K=V.

- self-attention最大的优势是可以方便有效的提取远距离依赖的特征和结构信息, 不必向RNN那样依次计算产生传递损耗.

- 关于self-attention采用三元组的原因, 经典实现的方式数学意义明确, 理由充分, 至于其他方式的可行性暂时没有论文做充分的对比试验研究.
## 1.12 长文本预测任务如果想用BERT来实现, 要如何构造训练样本?

### 学习目标

- 掌握利用BERT处理长文本的任务如何构造训练样本.

### BERT处理长文本的方法

- 首选要明确一点, BERT预训练模型所接收的最大sequence长度是512.

- 那么对于长文本(文本长度超过512的句子), 就需要特殊的方式来构造训练样本. 核心就是如何进行截断.

	- 1: head-only方式: 这是只保留长文本头部信息的截断方式, 具体为保存前510个token (要留两个位置给[CLS]和[SEP]).

	- 2: tail-only方式: 这是只保留长文本尾部信息的截断方式, 具体为保存最后510个token (要留两个位置给[CLS]和[SEP]).

	- 3: head+only方式: 选择前128个token和最后382个token (文本总长度在800以内), 或者前256个token和最后254个token (文本总长度大于800).

### 小节总结

- 学习了长文本处理如果要利用BERT的话, 需要进行截断处理.

	- 第一种方式就是只保留前面510个token.

	- 第二种方式就是只保留后面510个token.

	- 第三种方式就是前后分别保留一部分token, 总数是510.
## 1.9 Transformer架构的并行化是如何进行的? 具体体现在哪里?

### 学习目标

- 掌握Transformer架构的并行化是如何进行的.

- 理解为什么采用这样的方式可以实现Transformer的并行化.

### Transformer架构中Encoder的并行化

- 首先Transformer的并行化主要体现在Encoder模块上.

![](https://gitee.com/hxc8/images1/raw/master/img/202407172129426.jpg)

> 1: 上图最底层绿色的部分, 整个序列所有的token可以并行的进行Embedding操作, 这一层的处理是没有依赖关系的.2: 上图第二层土黄色的部分, 也就是Transformer中最重要的self-attention部分, 这里对于任意一个单词比如x1, 要计算x1对于其他所有token的注意力分布, 得到z1. 这个过程是具有依赖性的, 必须等到序列中所有的单词完成Embedding才可以进行. 因此这一步是不能并行处理的. 但是从另一个角度看, 我们真实计算注意力分布的时候, 采用的都是矩阵运算, 也就是可以一次性的计算出所有token的注意力张量, 从这个角度看也算是实现了并行, 只是矩阵运算的"并行"和词嵌入的"并行"概念上不同而已.3: 上图第三层蓝色的部分, 也就是前馈全连接层, 对于不同的向量z之间也是没有依赖关系的, 所以这一层是可以实现并行化处理的. 也就是所有的向量z输入Feed Forward网络的计算可以同步进行, 互不干扰.


### Transformer架构中Decoder的并行化

- 其次Transformer的并行化也部分的体现在Decoder模块上.

![](https://gitee.com/hxc8/images1/raw/master/img/202407172129621.jpg)

> 1: Decoder模块在训练阶段采用了并行化处理. 其中Self-Attention和Encoder-Decoder Attention两个子层的并行化也是在进行矩阵乘法, 和Encoder的理解是一致的. 在进行Embedding和Feed Forward的处理时, 因为各个token之间没有依赖关系, 所以也是可以完全并行化处理的, 这里和Encoder的理解也是一致的.2: Decoder模块在预测阶段基本上不认为采用了并行化处理. 因为第一个time step的输入只是一个"SOS", 后续每一个time step的输入也只是依次添加之前所有的预测token.3: 注意: 最重要的区别是训练阶段目标文本如果有20个token, 在训练过程中是一次性的输入给Decoder端, 可以做到一些子层的并行化处理. 但是在预测阶段, 如果预测的结果语句总共有20个token, 则需要重复处理20次循环的过程, 每次的输入添加进去一个token, 每次的输入序列比上一次多一个token, 所以不认为是并行处理.


### 小节总结

- 学习了Transformer架构中Encoder模块的并行化机制.

	- Encoder模块在训练阶段和测试阶段都可以实现完全相同的并行化.

	- Encoder模块在Embedding层, Feed Forward层, Add & Norm层都是可以并行化的.

	- Encoder模块在self-attention层, 因为各个token之间存在依赖关系, 无法独立计算, 不是真正意义上的并行化.

	- Encoder模块在self-attention层, 因为采用了矩阵运算的实现方式, 可以一次性的完成所有注意力张量的计算, 也是另一种"并行化"的体现.

- 学习了Transformer架构中Decoder模块的并行化机制.

	- Decoder模块在训练阶段可以实现并行化.

	- Decoder模块在训练阶段的Embedding层, Feed Forward层, Add & Norm层都是可以并行化的.

	- Decoder模块在self-attention层, 以及Encoder-Decoder Attention层, 因为各个token之间存在依赖关系, 无法独立计算, 不是真正意义上的并行化.

	- Decoder模块在self-attention层, 以及Encoder-Decoder Attention层, 因为采用了矩阵运算的实现方式, 可以一次性的完成所有注意力张量的计算, 也是另一种"并行化"的体现.

	- Decoder模块在预测计算不能并行化处理.

transformer的并行化理解

1、Encoder

1.1训练和测试，并行化是完全一样的

1.2除了self-attention之外的可以进行并行化（比如embedding层、Feed Forward层、Add&Normalize层都可以进行并行化，为什么除了self-attention,因为要等待embedding层全部完成之后，才能进行矩阵运算。总不能，比如说一共10个词，只进行9个embedding，那第十个还没embedding完，是不能进行矩阵运算的。）

1.3 但是self-attention层可以进行矩阵运算（数学意义上的并行化）

2、Decoder

2.1 训练：并行化是一样的，embedding层、前馈全连接层 add+normalize层

self-attention是数学意义上的并行（矩阵运算）

2.2 预测：预测的过程只能是一个时间步一个时间步的预测，也就是说不能并行化。
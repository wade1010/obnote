## 1.10 BERT模型的优点和缺点?

### 学习目标

- 理解BERT模型的优点和原因.

- 理解BERT模型的缺点和原因.

### BERT的优点

- 1: 通过预训练, 加上Fine-tunning, 在11项NLP任务上取得最优结果（也就是所谓的SOTA，state of the art）.

- 2: BERT的根基源于Transformer, 相比传统RNN更加高效, 可以并行化处理同时能捕捉长距离的语义和结构依赖.

- 3: BERT采用了Transformer架构中的Encoder模块, 不仅仅获得了真正意义上的bidirectional context, 而且为后续微调任务留出了足够的调整空间.

### BERT的缺点

- 1: BERT模型过于庞大, 参数太多, 不利于资源紧张的应用场景, 也不利于上线的实时处理.

- 2: BERT目前给出的中文模型中, 是以字为基本token单位的, 很多需要词向量的应用无法直接使用. 同时该模型无法识别很多生僻词, 只能以UNK代替.

- 3: BERT中第一个预训练任务MLM中, [MASK]标记只在训练阶段出现, 而在预测阶段不会出现, 这就造成了一定的信息偏差, 因此训练时不能过多的使用[MASK], 否则会影响模型的表现.

- 4: 按照BERT的MLM任务中的约定, 每个batch数据中只有15%的token参与了训练, 被模型学习和预测, 所以BERT收敛的速度比left-to-right模型要慢很多(left-to-right模型中每一个token都会参与训练).

### 小节总结

- 学习了BERT模型的3个优点:

	- 在11个NLP任务上取得SOAT成绩.

	- 利用了Transformer的并行化能力以及长语句捕捉语义依赖和结构依赖.

	- BERT实现了双向Transformer并为后续的微调任务留出足够的空间.

- 学习了BERT模型的4个缺点:

	- BERT模型太大, 太慢.

	- BERT模型中的中文模型是以字为基本token单位的, 无法利用词向量, 无法识别生僻词.

	- BERT模型中的MLM任务, [MASK]标记在训练阶段出现, 预测阶段不出现, 这种偏差会对模型有一定影响.

	- BERT模型的MLM任务, 每个batch只有15%的token参与了训练, 造成大量文本数据的"无用", 收敛速度慢, 需要的算力和算时都大大提高.
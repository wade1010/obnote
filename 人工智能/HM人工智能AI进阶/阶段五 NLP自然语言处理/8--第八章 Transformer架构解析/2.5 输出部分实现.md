### 学习目标

- 了解线性层和softmax的作用.

- 掌握线性层和softmax的实现过程.

- 输出部分包含:

	- 线性层

	- softmax层

![](https://gitee.com/hxc8/images1/raw/master/img/202407172131157.jpg)

### 线性层的作用

- 通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.

### softmax层的作用

- 使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.

- 线性层和softmax层的代码分析:

```
# nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层
import torch.nn.functional as F

# 将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构
# 因此把类的名字叫做Generator, 生成器类
class Generator(nn.Module):
    def __init__(self, d_model, vocab_size):
        """初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小."""
        super(Generator, self).__init__()
        # 首先就是使用nn中的预定义线性层进行实例化, 得到一个对象self.project等待使用, 
        # 这个线性层的参数有两个, 就是初始化函数传进来的两个参数: d_model, vocab_size
        self.project = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        """前向逻辑函数中输入是上一层的输出张量x"""
        # 在函数中, 首先使用上一步得到的self.project对x进行线性变化, 
        # 然后使用F中已经实现的log_softmax进行的softmax处理.
        # 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.
        # log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, 
        # 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.
        return F.log_softmax(self.project(x), dim=-1)

```

- nn.Linear演示:

```
>>> m = nn.Linear(20, 30)
>>> input = torch.randn(128, 20)
>>> output = m(input)
>>> print(output.size())
torch.Size([128, 30])

```

> 实例化参数:


```
# 词嵌入维度是512维
d_model = 512

# 词表大小是1000
vocab_size = 1000

```

> 输入参数:


```
# 输入x是上一层网络的输出, 我们使用来自解码器层的输出
x = de_result

```

> 调用:


```
gen = Generator(d_model, vocab_size)
gen_result = gen(x)
print(gen_result)
print(gen_result.shape)

```

> 输出效果:


```
tensor([[[-7.8098, -7.5260, -6.9244,  ..., -7.6340, -6.9026, -7.5232],
         [-6.9093, -7.3295, -7.2972,  ..., -6.6221, -7.2268, -7.0772],
         [-7.0263, -7.2229, -7.8533,  ..., -6.7307, -6.9294, -7.3042],
         [-6.5045, -6.0504, -6.6241,  ..., -5.9063, -6.5361, -7.1484]],

        [[-7.1651, -6.0224, -7.4931,  ..., -7.9565, -8.0460, -6.6490],
         [-6.3779, -7.6133, -8.3572,  ..., -6.6565, -7.1867, -6.5112],
         [-6.4914, -6.9289, -6.2634,  ..., -6.2471, -7.5348, -6.8541],
         [-6.8651, -7.0460, -7.6239,  ..., -7.1411, -6.5496, -7.3749]]],
       grad_fn=<LogSoftmaxBackward>)
torch.Size([2, 4, 1000])

```

### 小节总结

- 学习了输出部分包含:

	- 线性层

	- softmax层

- 线性层的作用:

	- 通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.

- softmax层的作用:

	- 使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.

- 学习并实现了线性层和softmax层的类: Generator

	- 初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.

	- forward函数接受上一层的输出.

	- 最终获得经过线性层和softmax层处理的结果.
分不同学派，有的人叫loss函数，有的人叫cost函数，有的叫j函数

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150722.jpg)

上面的

![](https://gitee.com/hxc8/images2/raw/master/img/202407172150654.jpg)

就是梯度

经过softmax之后，会变成一个向量，也可以看成一个概率分布，里面的值全部加起来等于1，每个值都在0到1之间。

有了softmax之后cross-entropy能做什么呢？

对于person1 = [85,34,34,54,54]  （数据不一定对，只是做示例）

y_about_person = [0,1,0.0,0] 表示这个人心情是难过  （心情 =['开心','难过','平静','惊喜','愤怒']）

通过这组weights

```
array([[ 0.22934813, -0.05505528, -0.70674387,  0.54556204, -2.08174579],
       [-0.20854507, -2.44317708, -0.13377832,  0.58086144,  1.55963271],
       [-1.56708584, -0.34708511, -1.6268867 , -0.01694483,  0.71351449],
       [ 0.12612368,  0.76328824, -0.0278995 , -1.64338888,  1.86011174]])
```

来判断他的好坏，就要通过梯度下来来改变这个值，通过改变weights里面的值，最终会改变softmax的参数的值（也就是向量如softmax([xxx,xxx,xx,xxx])，这里面的[xxx,xxx,xx,xxx]）.

也就是改变weights，然后和person1做点乘，他会改变它求解出来的向量，而这个向量通过softmax会求解出概率分布

这个向量是wx+b求得的。而wx+b求得的结果是weights和person1做点乘得到的结果，所以改变weights，也就会改变得到的向量，从而改变softmax的结果，达到改变概率分布的目的。

假设现在有一个weights，让softmax得到的结果无限接近我们想要得到的结果y_about_person = [0,1,0.0,0]

需要衡量这一组weights和bias的好坏程度，就用cross-entropy

其实就是衡量  [0,1,0.0,0]和softmax的参数向量[x,xx,xxx,xxxx]之间的loss，越小越好

这个loss是一个值，这个值
[https://blog.csdn.net/qq_42642142/article/details/120627895](https://blog.csdn.net/qq_42642142/article/details/120627895)

2 数据预处理

2.1 为向量化表示进行前处理

进行自然语言处理时，必须将单词转换为机器学习算法可以利用的向量。如果目标是对文本数据进行机器学习建模，例如电影评论或推文或其他任何内容，则需要将文本数据转换为数字。此过程称为“嵌入”或“向量化”。

进行向量化时，请务必记住，它不仅仅是将单个单词变成单个数字。单词可以转换为数字，整个文档就可以转换为向量。向量的维度往往不止一个，而且对于文本数据，向量通常是高维的。这是因为特征数据的每个维度将对应一个单词，而我们所处理的文档通常包含数千个单词。

2.2 TF-IDF

在信息检索中，tf–idf 或 TFIDF（term frequency–inverse document frequency）是一种数值统计，旨在反映单词对语料库中文档的重要性。在信息检索，文本挖掘和用户建模的搜索中，它通常用作加权因子。 tf-idf 值与单词在文档中出现的次数成正比，同时被单词在语料库中的出现频率所抵消，这有助于调整某些单词通常会更频繁出现的事实。 如今，tf-idf是最流行的术语加权方案之一。在数字图书馆领域，有83％的基于文本的推荐系统使用tf-idf。

搜索引擎经常使用tf–idf加权方案的变体作为在给定用户查询时对文档相关性进行评分和排名的主要工具。tf–idf可成功用于各种领域的停用词过滤，包括文本摘要和分类。

排名函数中最简单的是通过将每个查询词的tf–idf相加得出，许多更复杂的排名函数是此简单模型的变体。

当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词

tf-idf算法步骤：

计算词频:

某个词在文章中出现的次数 = p，文章总词数 = n

标准化词频(tf) = p / n p / np/n

计算逆文档频率

此时需要一个语料库来模拟语言的使用环境

逆文档频率(idf) = log ⁡ ( 语 料 库 文 档 总 数 / 包 含 该 词 的 文 档 树 + 1 ) \log (语料库文档总数/{包含该词的文档树+1})log(语料库文档总数/包含该词的文档树+1)

可见一个词越常见，分母就越大，逆文档频率就越小越接近于0，分母+1是为了防止所有文档都不包含该词(防止分母为0）

计算tf-idf

t f − i d f = 词 频 ( t f ) ∗ 逆 文 档 频 率 ( i d f ) tf-idf = 词频(tf) * 逆文档频率(idf)tf−idf=词频(tf)∗逆文档频率(idf)

TF-IDF优缺点：

TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。

2.3 Stemming

stemming 是将单词还原为词干（即词根形式）的过程。 词根形式不一定是单词本身，而是可以通过连接正确的后缀来生成单词。 例如，“fish”，“fishes”和“fishing”这几个词的词干都是“fish”，这是一个正确的单词。 另一方面，“study”，“studies”和“studying”一词源于“studi”，这不是一个正确的英语单词。

2.4 Tokenizing

Tokenization 将句子分解为单词和标点符号
### 注意力

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039582.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039871.jpg)

为什么连自己都要问一问呢？比如前面的例子，哪个it，这个代词通常通常翻译为它，如果不把自己在这个句子当中的重要性页表项出来，不把自己的这个信息重构一下的话，它可能就没办法决定要不要对自己原本的意思进行一个改变。

通过相乘后，得到了这个衡量关系紧密不紧密的这样一个结果出来，那么怎么去重构这个V呢？

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040789.jpg)

接下来需要对它进行一个softmax操作，比如上图，1-1的位置是0.1，我就知道重构的时候，需要把自己10%的信息保留下来；1-2的位置为0.3，就是说，我要把第二个词的30%的信息拿过来。后面同理。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040087.jpg)

下面就开始重构我们的词向量，同样也是进行一个矩阵乘法，就是用我们重构后的权重矩阵乘以这个V矩阵得到一个新的Z矩阵。这个Z就是我们重构后的向量。

那为什么能衡量这个词跟别的词之间的关系紧密不紧密，衡量之后又把多少信息拿过来。它是通过这样一种计算方式得到的，

比如上图z的第一行，它是怎么计算得来的呢？那是不是权重矩阵的第一行，然后对这个v这个矩阵进行了一个矩阵乘法呀。

那么，如果我们换一种表现形式，跟它同样等价的一种形式，如下图

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040044.jpg)

第一行（如下图）

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040006.jpg)

分别乘以V矩阵的每一行（如下图）然后相加

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040761.jpg)

后面的同理

从表现形式上看，是不是从第一个词（紫色矩阵第一行）我自身的信息当中拿出30%，第二个词（紫色矩阵第2行）拿出20%，第三个词（紫色矩阵第3行）也拿出20%，第四个词（紫色矩阵第4行）拿出30%，通过这种方式，我们重构好了第一个词的一个向量（就是Z向量的第一行）。

就是说原本词向量是V这样的形式，我重构成了Z的第一行。那么这样，一个词是不是就包含了我与其他所有词之间这样一个关系，一些信息。

### 多头注意力机制

多头注意力机制主要影响在权重矩阵这个地方（如下图，只截取了WQ）

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040699.jpg)

如果我们现在有一组WQ WK  WV，这样的权重矩阵，是否生成了一组Q  K  V?

如果我还有第二组WQ WK WV ，它是不是就生成另外一组 Q K V，对不对？这一组跟第一组是有一种不同的询问方式、回答方式、重构方式。

多头是从不同的方面去提取语句的信息。比如第一张图的实例，一组头提取的是it指代的是什么，另外一组提取的是it当前的状态

### GPT1

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040621.jpg)

GPT1提出前，面临的问题：

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040772.jpg)

GPT1模型训练分为两步：

1）无监督的训练

第一阶段是在大型文本预料库上学习大容量语言信息

2）有监督的模型微调

在经过第一阶段的无监督，训练后，进入微调阶段，利用有标签数据使模型适应特定的任务

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040181.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040223.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040331.jpg)

U（原本的词汇）进来要进行一个单词的编码，在进行一个位置编码形成h0这个数据矩阵。

数据矩阵要经过transformerblock，对于每一个词任意的i,从1到n，每一个词都要进行这样的预测，然后生成出来一个重构的项链。

然后重构的向量经过线性层，经过softmax，然后映射储一个概率。

经过这么一个无监督的训练之后，模型具备了一个说话的能力，但是说的不一定是对我们有用的，可能是很合理但是没有用的。

GPT1值得关注点

1）随着模型增大，模型精度和泛化能力还有提升空间。 （开创了先河）

2）zero-shot 零样本提示

### GPT2

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040190.jpg)

更大的数据集，更大的模型

模型结构变化：

(1) 后置层归一化（post-norm）改为前置层归一化（pre-norm）;

(2) 在模型最优一个自注意力之后，额外增加一个层归一化；

(3) 调整参数的初始化方式，按残差层个数进行缩放，缩放比例为1：根号n

(4) 输入序列的最大长度从512扩充到1024；

zero-shot:

当时研究的现状：针对某一个任务，需要该任务专门的数据集，训练出一个专门的模型来应对该任务，主要原因是当前系统缺乏泛化性。

一个解决的办法——多任务学习：多任务学习通过训练一个模型来同时执行多个相关任务。相比传统的单一任务学习，多任务存在的问题：需要一个模型出来多任务的数据集；可能需要设置多个损失函数；在NLP领域应用并不广泛。

GPT-1所使用的无监督预训练+有监督的微调也还是没能避免这个问题。所以GPT2在训练之初就提出了zero-shot的概念：当训练完成一个模型后，无需对模型的参数或者架构进行修改，就能执行各项任务兵获得好的成绩。（GPT1是预训练完，还要进行一个就是插入不同的，开始符、分隔符以及最后的抽取符，然后最后还要调整一下这个线性层参数）现在GPT2都不需要干了。

如何做到zero-shot？

zero-shot想要达到的一个效果是：在构建下游任务的时候，不再使用gpt1时那样的开始符、分隔符及抽取符的形式告诉模型要执行任务了。而是通过自然语言的方式来指定任务，比如要执行一个翻译任务，就可以给模型这样输入：将英语翻译为中文，英文内容。通过这样的方式来提示（promot）模型应该做什么。

### GPT3

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040095.jpg)

gpt3的特点:

模型很大，有1750亿个参数

gpt3在执行所有任务时，都不会再更新参数或者微调，所有的任务提示都通过文本和模型进行交互完成

gpt3在各种NLP任务数据集上都表现出了最好的成绩(翻译、问答、完形填空等等)

能生成人类都难以区分的新闻文章

三个问题:

1、仍然是数据问题，每个不同的任务都需要大量的已标记的数据，限制了语言模型的适用性。

2、微调后的模型泛化性不一定就能更好

3、人类在学习的时候仅需要少量的示例，而不是需要大量的已标注数据。

针对上面问题GPT-3做了什么呢？

为了使模型能够解决上述的问题，论文中重点方法如下：

在上述情景提示中，通常分为zero-shot、one-shot以及few-shot。

GPT3中使用的数据包含情景数据（GPT1是书的数据，GPT2是Reddit论坛的数据），给模型输入情景的描述，模型会依据用户提出的情景要求来生成答案，这部分被称为in-context learning

这个情景类对应的就是few-shot，给一小段示例，模型从这一小段示例里面，如果模型学过这样的内容，模型就把这样相关的内容找出来，然后完成任务。如果没学过，就把其他的能力能够用上，然后通过抽取这里面的这个模式，然后完成任务。

至于为什么完成的很好，这个神经网络大家都知道，黑盒模型，你要说去解释它里面到底是怎么思考的，谁也不知道。

### ChatGPT

![](https://gitee.com/hxc8/images0/raw/master/img/202407172040972.jpg)

ChatGPT的训练，分为以下三个阶段：

第一阶段：训练监督策略模型

GPT3本身很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果，为了让GPT3初步具备理解指令的意图，首先会在数据集中随机抽取问题，由人类标注人员，给出高质量答案，然后用这些人工标注好的数据来微调GPT3模型（获得SFT模型，Supervised Fine-Tuning）。

此时的SFT模型在遵循指令/对话方面已经由于GPT3，但不一定符合人类偏好。

第二阶段：训练奖励模型（Reward Mode,RM）

 	这个阶段的主要任务是通过人工标注训练数据（约33K个数据），来训练回报模型。在数据集中随机抽取问题，使用第一阶段生成的模型，对于每个问题，生成多个不同回答。人类标注者对这些结果总和考虑给出排名顺序。这一过程类似于教练和老师辅导。 

接下来，使用这个排序结果数据来训练奖励模型。对多个排序结果，两两组合，形成多个训练数据对。RM模型接受一个输入，给出评价回答质量的分数。这样，对于一对训练数据，调解参数使得高质量回答的打分比低质量的打分要高。

第三阶段：采用PPO（Proximal Policy Optimization,近端策略优化） 01:39:41
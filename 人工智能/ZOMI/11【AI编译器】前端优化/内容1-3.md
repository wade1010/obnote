### 1、内容介绍：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172119548.jpg)

上图左边是真正的面向图层的一些IR或者图层的一些Pass的优化

右边的面向与一些更传统的一些DAG图的一些优化

布局转换：更多的指内存的布局内存的排布（指的是数据）

内存分配，是指内存的地址和内存的空间进行一个分配

常量折叠，就是神经网络里面可能出现一些常量，这个时候我们把它预先地计算了

后面几个概念（公共子表达式消除，死代码消除，代数简化），传统编译器里面也会有

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120621.jpg)

AI框架最终的事就是产生一个计算图，拿到计算图就把它变成一个graph IR ,传给我们编译器前端，（编译器有前端，中间优化，后端）

这里主要聚焦前端优化，有非常多的Pass,不同的pass执行不同的优化逻辑

下面红色部分就是我们刚刚上面讲到的一些优化

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120577.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120652.jpg)

上图显示了前端优化所做的事，前端优化，输进去的是一个类似于Graph的IR，就是图层的IR，然后经过常量折叠之后输出一个图的IR，接着这个图的IR重新丢给Pass,然后经过常量传播或者算子融合得到另外一个新的IR，接着又重新返回来，每一次传，它都是图的表达、图的形式，所以叫做图层IR

### 2、什么是图层 IR

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120748.jpg)

计算图的基本构成，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120728.jpg)

用来表示深度学习网络模型在训练与推理过程中计算逻辑与状态。主要是表达整个网络模型的。

AI框架在后端，会将python构建的网络模型的前向与反向梯度计算，以计算图的形式表示出来

由基本数据结构张量（Tensor）和基本运算单元算子（Operator）构成

既然是图，肯定会有节点和边，那这个节点就是算子，节点和节点（也就是算子和算子）之间，会有一个连接，有一条边，那这个边流动的数据就是我们的张量。 

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120990.jpg)

AI框架式产生计算图的，所以我们经常谈到的算子，还是AI框架的一个概念，真正在硬件底层或者使用CUDA写一些算子表达的时候，我们叫做CUDA的Kernel

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120265.jpg)

反向传播的时候也是一个有向无环图，

有两点需要注意的是，会遇到特殊的操作和特殊的边，而这个所谓的特殊，主要是指控制流， 控制流式在计算图里面比较难表示的

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120710.jpg)

AI框架式怎么样去生成计算图的？计算图跟自动微分有什么关系。

在平时写代码的时候，一般只写一个正向的图，或者做了一个神经网络对正向的表达，实际上AI框架会帮我们自动建好反向图。然后整体变成一个计算图，下发给我们AI编译器。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120960.jpg)

AI 框架生成一个计算图有以下方式：

一、静态计算图

静态计算图比较好表示，上图代码是用MindSpore写的一些伪代码，写完这个伪代码之后，会把这些代码通过AI框架的前端定义，就是上图里面的nn.SequentialCell、nn.ReLU等，然后基于python的代码做一个源码的转换，对这些源码呢进行一个分析和重构，变成我们静态的计算图，这个静态计算图，实际上它只是一个特殊的数据结构，所以说AI框架生成静态图，主要是AI框架所提供的前端的API，对这些API进行分析重构，然后变成特殊的静态图的数据结构。

二、生成动态计算图

主要是利用python语言自身的解释器对代码进行解析，利用AI框架本身的算子分发能力，然后去执行，算子会立即执行并输出结果。 那很简单，pytorch最典型的就是动态的计算图利用python去写完代码，通过autograd这个函数建立方向图，方向图之后呢就变成一个整计算图了或者变成一个算子序列，然后不断的分发到硬件去执行并且返回结果。

它使用的是命令式变成的范式，使用前端语言构造神经网络模型更加简洁。

优点：动态图模式灵活易用，动态生成可以使用前端语言的原生控制流，充分发挥前端语言的编程友好特性。

缺点：不过动态生成中完整的网络结构在执行前是未知的，不能使用静态图中的图形优化技术来提供计算执行能力（缺点就是没有编译器，所以没办法去提升这些性能。）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120203.jpg)

AI框架的动态图和静态图是两种不同的计算图表示方法，用于定义和执行机器学习模型。下面是它们之间的比较：

**动态图（Dynamic Graph）**：

- 动态图是一种基于命令式编程的计算图表示方法。在动态图中，模型的计算图是通过在运行时逐个操作节点进行定义和执行的。

- 动态图具有灵活性，允许用户以编写常规程序的方式定义模型。您可以使用条件语句、循环和控制流来动态构建计算图。

- 动态图适合于快速原型设计和调试，因为您可以方便地进行调试和打印中间结果。

- PyTorch和TensorFlow 2.0中的Eager Execution都是动态图的示例。

**静态图（Static Graph）**：

- 静态图是一种基于声明式编程的计算图表示方法。在静态图中，模型的计算图是在编译时创建的，并且在执行前已经完全定义好了。

- 静态图需要用户首先定义模型的计算图结构，然后将数据传递给图中的节点进行计算。这种方式可以进行优化和图级别的并行计算。

- 静态图适合于在训练过程中进行高效的计算，特别是在大规模模型和大规模数据集上。

- TensorFlow 1.x中的Graph

 Execution就是静态图的一个示例。

动态图和静态图都有各自的优缺点，选择哪种方式取决于具体的需求和使用场景。动态图适合于快速原型设计和探索性工作，而静态图适合于高效的计算和生产环境中的部署。

近年来，越来越多的框架开始支持动态图和静态图的混合模式，以兼顾两种方式的优点。例如，TensorFlow 2.0通过Eager Execution提供了动态图的灵活性，同时仍然支持静态图的优化和部署。

在部署能力方面，静态图和动态图有一些区别。以下是它们在部署方面的比较：

**静态图的部署能力**：

- 静态图在编译时已经完全定义好了计算图结构，这使得它可以进行一些优化，例如图级别的并行计算和设备的优化。

- 静态图可以通过将模型转换为可移植的格式（如TensorFlow SavedModel或ONNX）来实现跨平台部署。这样，您可以在不同的硬件和软件环境中使用预先编译的模型。

- 静态图还可以使用专门的深度学习推理引擎（如TensorRT）进行高效的推理加速，以提高模型的性能。

**动态图的部署能力**：

- 动态图的部署通常涉及将整个框架和模型打包，以便在部署环境中进行执行。这可能需要更多的依赖项和资源。

- 由于动态图的灵活性，您可以通过直接在部署环境中加载和执行模型，进行实时的调整和修改。这对于需要快速迭代和适应变化的场景非常有用。

- 动态图适合于在部署中进行实时推理和交互式应用程序，因为您可以根据输入动态调整模型的计算图。

总的来说，静态图在将模型部署到不同环境和硬件时具有更好的优化和性能，而动态图则更适合于需要快速迭代和实时调整的部署场景。选择哪种方式取决于您的具体需求和部署环境。

动态图和静态图在部署能力上的差异主要是由它们的计算图表示和执行方式所决定的。

**动态图**：

- 动态图是在运行时逐个操作节点进行定义和执行的，计算图是在每次运行时动态生成的。

- 这种动态生成计算图的方式使得动态图更加灵活和易于调试，但也导致了一些挑战，特别是在部署方面。

- 动态图的部署需要将整个框架和模型打包，并在部署环境中进行加载和执行。这通常需要更多的依赖项和资源，以及对框架本身的支持。

- 动态图的部署方式也不够优化和高效，因为每次运行都需要动态生成计算图，无法进行静态优化和图级别的并行计算。

**静态图**：

- 静态图是在编译时创建的，计算图在执行前已经完全定义好了。

- 这种静态定义计算图的方式使得静态图可以进行一些优化，例如图级别的并行计算和设备的优化。

- 静态图可以通过将模型转换为可移植的格式（如TensorFlow SavedModel或ONNX）来实现跨平台部署。这样，您可以在不同的硬件和软件环境中使用预先编译的模型。

- 静态图还可以使用专门的深度学习推理引擎（如TensorRT）进行高效的推理加速，以提高模型的性能。

总的来说，动态图和静态图在部署能力上的差异主要是由它们的计算图表示和执行方式所决定的。动态图的灵活性和易用性使得它在快速原型设计和调试方面表现出色，但在部署方面需要更多的依赖和资源。静态图在部署时具有更好的优化和性能，可以通过预编译和专门的推理引擎进行高效的部署。

希望对您有所帮助！如果还有其他问题，请随时提问。

动态图和静态图在部署能力上的差异主要是由它们的计算图表示和执行方式所决定的。

**动态图**：

- 动态图是在运行时逐个操作节点进行定义和执行的，计算图是在每次运行时动态生成的。

- 这种动态生成计算图的方式使得动态图更加灵活和易于调试，但也导致了一些挑战，特别是在部署方面。

- 动态图的部署需要将整个框架和模型打包，并在部署环境中进行加载和执行。这通常需要更多的依赖项和资源，以及对框架本身的支持。

- 动态图的部署方式也不够优化和高效，因为每次运行都需要动态生成计算图，无法进行静态优化和图级别的并行计算。

**静态图**：

- 静态图是在编译时创建的，计算图在执行前已经完全定义好了。

- 这种静态定义计算图的方式使得静态图可以进行一些优化，例如图级别的并行计算和设备的优化。

- 静态图可以通过将模型转换为可移植的格式（如TensorFlow SavedModel或ONNX）来实现跨平台部署。这样，您可以在不同的硬件和软件环境中使用预先编译的模型。

- 静态图还可以使用专门的深度学习推理引擎（如TensorRT）进行高效的推理加速，以提高模型的性能。

总的来说，动态图和静态图在部署能力上的差异主要是由它们的计算图表示和执行方式所决定的。动态图的灵活性和易用性使得它在快速原型设计和调试方面表现出色，但在部署方面需要更多的依赖和资源。静态图在部署时具有更好的优化和性能，可以通过预编译和专门的推理引擎进行高效的部署。

希望对您有所帮助！如果还有其他问题，请随时提问。

动态图和静态图在部署能力上的差异主要是由它们的计算图表示和执行方式所决定的。

**动态图**：

- 动态图是在运行时逐个操作节点进行定义和执行的，计算图是在每次运行时动态生成的。

- 这种动态生成计算图的方式使得动态图更加灵活和易于调试，但也导致了一些挑战，特别是在部署方面。

- 动态图的部署需要将整个框架和模型打包，并在部署环境中进行加载和执行。这通常需要更多的依赖项和资源，以及对框架本身的支持。

- 动态图的部署方式也不够优化和高效，因为每次运行都需要动态生成计算图，无法进行静态优化和图级别的并行计算。

**静态图**：

- 静态图是在编译时创建的，计算图在执行前已经完全定义好了。

- 这种静态定义计算图的方式使得静态图可以进行一些优化，例如图级别的并行计算和设备的优化。

- 静态图可以通过将模型转换为可移植的格式（如TensorFlow SavedModel或ONNX）来实现跨平台部署。这样，您可以在不同的硬件和软件环境中使用预先编译的模型。

- 静态图还可以使用专门的深度学习推理引擎（如TensorRT）进行高效的推理加速，以提高模型的性能。

总的来说，动态图和静态图在部署能力上的差异主要是由它们的计算图表示和执行方式所决定的。动态图的灵活性和易用性使得它在快速原型设计和调试方面表现出色，但在部署方面需要更多的依赖和资源。静态图在部署时具有更好的优化和性能，可以通过预编译和专门的推理引擎进行高效的部署。

希望对您有所帮助！如果还有其他问题，请随时提问。

动态图转化为静态图：

基于追踪转换：以动态图模式执行并记录调度的算子，构建和保存为静态图模型。（如pytorch的FX）

基于源码转换：分析前端代码将动态图代码自动转为静态图代码，底层使用静态图执行运行。（pytorch的JIT）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120220.jpg)

在前端获得我们的计算图，最主要的工作就是方便底层进行编译优化，而这个图的概念，可以用它来保存或者表示整个神经网络的全过程。也可以序列化保存，不需要再次编译前端源代码，就可以进行推理或者训练的加速。

静态图对编译器的实际作用：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120349.jpg)

1、图优化。拿到这个图之后，可以做很多图层的优化。计算图的出现允许AI框架在执行前看到深度学习模型定义的全局信息，拿到这个信息，我们就知道未来要执行什么，之前我执行过什么。既然我知道全局信息，我肯定可以做一些系统级的优化。

2、计算图作为AI框架中的高层中间表示，可以通过图优化Pass去化简计算图或提高执行效率

就像之前提到的LLVM一样，由非常多的Pass组成，然后中间通过一个IR进行一个优化的。

上图的自动微分在mindspore里面也是通过一个编译来实现的，因此对于AI编译器来说，可以将神经网络中间表达或者IR变成不同的硬件代码，就直接怼到硬件上面了，然后可以直接快速的部署起来，提供高效的服务。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120727.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120162.jpg)

实际上对到每一层，我们都是通过不同的优化Pass去执行的

计算图优化

运行时优化调度

算子/内核执行优化

所以后面把他们解耦出来，可以对不同的层做不同的Pass优化。这对整个AI系统全栈来说是非常重要的。	

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120742.jpg)

### 3、算子融合原理

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120120.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120252.jpg)

上图，进行纵向融合，把C和D融合，得到右边图，就已经减少了一次Kernel的开销，而且也减少了一次中间数据的缓存。 

![](https://gitee.com/hxc8/images1/raw/master/img/202407172120281.jpg)

上图左，就是算子A计算完之后，分别给B算子和C算子进行执行，而B算子和C算子是同时执行的，那这里面就会有两次的访存。

上图右边，把A算子和B算子一起去执行，再把A和C一起去执行。AB和AC是同时执行的，这个时候我只执行两次Kernel的开销。 而且并发只有一个来回，就是我只需要一次的访存。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121579.jpg)

上右图，横向融合，减少了一次kernel的调度，而且两次的计算结果都放在同一个内存块里面，加快了内存访问的效率

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121647.jpg)

上右图，把A和B进行融合，然后合并完之后，把所有的结果丢在内存里面，这个时候再给C进行计算，另外一个结果给下一个进行计算。这个时候可以提高内存的使用效率。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121611.jpg)

把上图第一部分，卷积算子，把它进行一个扩充，enlarge卷积，然后变成3x3x256,

上图第二部分，对两个3x3x256的进行一个合并，合并成一个算子，变成卷积3x3x512。有了这个很像融合后，想做一个纵向融合，

纵向融合，现在有一个split,有个卷积，有个add,现在把split卷积和add合成一个算子，变成上图第四部分的conv2d 3x3x256，但是我们觉得还不够，像激活一半都可以合并在前一个计算里面，所以上图第五部分，我们又把ReLU合并起来。

上图整体看来，我们一开始计算图四比较复杂的，比较多算子的，最后融合起来了，算子就减少了很多，虽然有部分的计算可能会变大，但是总体来说减少了kernel的开销。减少了对内存的不断的访问。

从上图也可以很清楚的看到，算子融合确实对整个计算图，或者整个运算的时候产生很大的收益

从上面例子可以看出，不同的算子融合策略会产生不同的算子开销。也可以带来不同的内存访问的性能的提升。主要是解决下面两个问题。

内存墙：

并行墙：

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121227.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121285.jpg)

下面是一个非常经典的卷积 BN-ReLu三个算子，三个非常经典的算子进行融合，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121329.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121569.jpg)

上图，方向过程，包括求参数误差还有求输入误差两个部分，BN这个算子在方向计算的时候，关键的访存特征就是我们需要对内存里面的这些参数

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121670.jpg)

进行访问，第二个就是大量的访问

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121449.jpg)

把我们之前方向计算出来的一个输出误差给到我们

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121599.jpg)

来计算，还给到我们的

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121318.jpg)

来计算，这种方式就引起我们对内存进行大量的访问。

在实际的AI框架计算里面，这个

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121069.jpg)

不单单是一个数，而是一个向量一个矩阵一个张量，就可能是高维的张量，数据量非常大，所以它对访存的要求非常高的，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121932.jpg)

加速器或者NPU或者芯片上面的访存容量是非常有限的，没办法无限制的去保存数据，所以这个时候就需要把中间结果可能会offload到CPU的内存里面，并且在反向的时候再读取出来。那这种方式是非常低效的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121134.jpg)

上图是一个更加明确的例子，还是卷积 BN  ReLU三个算子。横框就是内存。

正向的时候需要Z1、W1进行完卷积计算，输出X，在BN计算的时候，需要把X输进去，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121347.jpg)

去求得我们的平均值和方差，还有Y，再把Y输进ReLU,输出Z2

反向的时候，同样需要把刚才计算的Y，刚才的X还有

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121138.jpg)

也是需要计算和存储的。

上图可以看出有大量的箭头，而每种箭头都不一样，实线红色就是正向对内存的访问，虚线红色就是求参数误差对内存的访问，而绿色虚线就是求输入的误差堆内存的访问。

可以看到对内存的访问次数非常多。每次都要大量的交互，所以希望把这些零散的算子合成一个大的kernel，把这些零散的数据一次性的读取，一次性的读出。

重构如下图

![](D:/download/youdaonote-pull-master/data/Technology/人工智能/ZOMI/11【AI编译器】前端优化/images/WEBRESOURCEf5ba3cf87f4d19ab2c077cf0147104e0image.png)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172121466.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122713.jpg)
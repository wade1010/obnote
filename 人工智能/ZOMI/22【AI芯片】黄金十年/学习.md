![](https://gitee.com/hxc8/images0/raw/master/img/202407172056014.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172056920.jpg)

 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057073.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057157.jpg)

为什么AI编程，关注SIMT和SIMD？

大家发现在AI编程里面GPU真香，但是你指的GPU是有个牌子的，牌子就是英伟达的GPU，而不是AMD的GPU，这里面香的是CUDA，CUDA是它的生态香吗？还是CUDA的架构设计香呢？所以这是我们值得深入探讨的问题。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057354.jpg)

右边是单个神经元展开，里面最核心的一个部件，就是矩阵乘（X乘以W）再做一个激活，无论是FFN CNN，包括现在的大模型非常火的tr ansformer，还是以前传统的LSTM，都离不开矩阵乘。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057517.jpg)

 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057562.jpg)

1、算法或者开发人员，需要定义神经网络具体长什么样；

2、去用我们的AI框架去编写对应的程序（去写这个网络）；

3、有了这个网络之后，AI框架会把这个程序构成一个正向的计算图；

4、AI框架根据自动微分的原理，去构建这个反向的计算图，反向的计算图，最终在runtime（运行时），把所有的图变成一个算子的执行序列，而这个算子在真正的硬件执行，是我们的kernel。

大家有没有发现，这整个AI系统运行的过程当中，我们看到的全都是刚才描述的一个流程，然后引发下图的几个思考。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057532.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057515.jpg)

程序员关心的是我们的编程的模型，怎么去编写算子，怎么去编写这些执行算子的kernel，

而硬件关心的是它的执行的模型，关心我们的指令

因此在这里面有两个gap，一个gap就是程序员关心的编程模型，第二个就是硬件关心的执行模型，

根据我们之前讲到的计算机的系统设计里面，其实定义基本上就是那四个（SISD、SIMD、MISD、MIMD），都是对数据进行并行，但是这里面有个很重要的概念，就是编程却以生态非常强大的CUDA自己定义的SIMT为主。所以这里面到底是一个什么关系？接下来就是澄清和思考这个问题。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057530.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057592.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057633.jpg)

### SIMD & SIMT 区别与联系

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057498.jpg)

SIMD主要是对多个相同的元素数据进行相同的计算操作，这里面很重要的一点就是利用数据级的并行，而不是并发，里面有多个计算，但是只有一个进程在运行。这就意味着我们一个进程是由单一指令或者单一命令对多个数据进行操作，这种方式属于提升我们现代CPU的计算能力，特别是提升我们数据并行的方式，从而提升系统的吞吐。对硬件的要求就是需要更宽位数的计算单元。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057628.jpg)

我们对向量A和B进行相乘，这里面相乘操作都是OP1，相同的计算单元进行相乘，最后得到C，这就是SIMD的计算本质。

了解完本质之后，看看硬件到底怎么组成。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057708.jpg)

看下SIMD实现一次乘法可以完成多个元素的计算， 这个时候就要求硬件上面增加我们的ALU的数量（上图processing unit，可以看出堆叠了好几个），同时也需要增加一个功能，就是数据的通路的数量（绿色的箭头把我们数据传给不同的processing unit，这样才能够实现在相同的时钟周期内，才能够提升整体计算的吞吐量），

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057037.jpg)

缺点：使用独立线程，这个线程同时进行多个元素的操作，因为是一个线程去计算，而且，ALU的宽度比较宽，我们以前计算机可能有16到32再到64，到现在的128，整体对计算数据的要求，就要求数据的类型、格式，大小能必须严格的对齐，不让ALU没办法给你计算。

优点：提升计算性能，利用计算机系统的内存的数据的总线的宽度，可以使得我们多个数据同时从内存里面read/write。

往上图右边看看，实际上为什么叫SIMD，是因为控制的是指令，而不是编写的代码，右边就是计算机具体执行的一些指令，指令里面可以看到有4个ST，分别对4个元素进行逐元素进行相加相乘。而是用SIMD之后，里面只有一个ST（最右边），v1 v2 v3后面就需要同时对4个元素进行操作。这个就是通过我们平时编写代码，然后编译器会帮我们编写成我们硬件能够识别的SIMD的指令。这个就是SIMD具体的执行的方式和逻辑。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057419.jpg)

 ARM架构。里面的汇编是NEON指令集，就引入了SIMD，整体的单元到128bit，比较宽，包含16个128bit寄存器，相当于能做32个64bit的寄存器。他们之间可以进行一个配置，而这些寄存器都是用来存放，相同数据类型的vector，这就意味着数据是对齐的，数据的元素都是相同的时候就可以使用一个进程对多个数据进行计算。

同样的，对我们刚才的四个元素进行乘法计算，同样是A[3:0]，B[3:0]相乘，得到C[3:0]

 一个寄存器是128位宽，可以存放4个X32bit，也就是刚好可以存放四个元素，每个元素32bit，就是一个简单的浮点数。

向量B放在S15寄存器里面，向量C放在S14寄存器里面，对两个数进行相乘，把计算结果保存在S15的寄存器，把数据重新覆盖过来。这整个就是SIMD的原理。

可以看出来SIMD最重要的就是改变饿了我们硬件计算单元的数量，还有我们数据读取的通路的数量。从而对上提供更多的指令集，让我们去计算。

很少的程序员去真正碰到SIMD里面的I（指令）

SIMT，单指令多线程

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057658.jpg)

二者都是通过将同样的指令广播给多个执行单元来实现我们的数据的并行和计算的。

不同点：SIMD要求所有的向量的元素都是在同一个线程里面同步的执行，SIMT允许多个线程在一个warp中独立执行。

独立执行这个概念就很特别了，因为它是允许多个线程，每个线程自己独立的进行操作。

SIMT本质上还是一个SIMD的计算方式。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057810.jpg)

每个元素上面都有一个线程（弯弯的线），每一个线程都负责一个具体的元素的计算，例如thread a,tid 0 ,就负责vector A里面的一个元素跟vector B里面第一个元素相乘，得到vector C里面的这个元素。

就所有的线程从thread0到threadN-1，都是并发的执行所有的计算。这个就是SIMT计算的本质。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057888.jpg)

右边是host侧的CPU，最里面，可以假设它为一个GPU或者NPU，或者一个AI处理器，SIMT就提供了一个多核的系统，所以叫做SIMT Core Cluster，里面有非常多的SIMT core，也就是非常多个多核的系统，每个核心又有非常多的SIMT，具体的core就有很多个具体的执行单元。每个执行单元里面都有自己独立的寄存器文件。也就是register file，还有ALU计算单元，还有data cache。但是有一点值得注意的是，每一个SIMT core cluster里面只有一个指令寄存器和指令的译码器，指令同时广播给所有的SIMT，就是图片里面堆叠的SIMT Core，去执行具体的计算。

整个GPU是由多个SIMT Core Cluster组成。也就是GPU里面SM的概念。而SM里面每个CUDA core，就类似我们这里面的SIMT core具体的核心。所以右边整个图就是对GPU进行一个硬件的概念的抽象架构图。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172057960.jpg)

粗略图如上，详细图如下

![](https://gitee.com/hxc8/images0/raw/master/img/202407172058203.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172058320.jpg)

允许一条指令对数据分开寻址，所以它可以做到每个线程的独立的去寻址，每个线程独立的去运作。而SIMD必须数据是连续的取值，所以要求数据必须是对齐的、数据的格式也是对齐的，数据的data type也是相同的。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172058428.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172058606.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172058597.jpg)

在一个block里面，所有的线程都是执行同一段代码的，那这段代码在英伟达GPU里面叫做kernel。之前说到AI框架，看到很多算子，算子是一个高度抽象的概念，每个算子里面可能针对一个kernel来实现，也可能针对多个kernel去实现。而每一个线程都是自己的index，用于计算内存地址和执行具体的控制逻辑。

看下上图，就是一个thread box里面具体的执行内容。这个thread box就支持同时去执行同一个指令，示例指令比较简单，就是A+B等于C，其中i就是索引，i等于threadidx.x,就是每个线程会去索引自己的一个独立地址的数据，然后惊醒执行。

 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172058562.jpg)

线程块就变成了SM的调度单元。SM是它硬件的一个真正的形态，而Thread block就是编程里面抽象出来的概念。还是之前A+B=C的例子。刚才只有threadidx.x，但是现在多个thread block，组合起来之后，我们的thread id之上，要进行多一层的索引，多一层的索引叫做blockidx.x跟blockdim.x，因为它有两个维度，一个横向的维度，一个竖向的维度，取决于我们具体硬件计算单元的排布。

所以执行这么一个相同的操作，再加一层索引，就可以完成大规模的数据的并行的操作了。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172058745.jpg)

线程以线程块为单位被分配到SM上面，SM负责维护线程块和线程ID，对它进行执行和调度。而每个线程块，又以N个线程组成一个warp被执行，warp是SM里面的调度单位。真正warp里面执行的是SIMD。
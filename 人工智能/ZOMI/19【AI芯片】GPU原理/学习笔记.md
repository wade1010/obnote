  

![](https://gitee.com/hxc8/images0/raw/master/img/202407172109976.jpg)

并行主要是指能够同时处理多个任务，并发是指能够处理多个任务的功能，但不一定是同时，

![](https://gitee.com/hxc8/images0/raw/master/img/202407172109316.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172109651.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172109634.jpg)

  

![](https://gitee.com/hxc8/images0/raw/master/img/202407172109706.jpg)

把计算通过并发进行循环展开，整个总线处于一个忙碌的状态当中，从计算来看11659/16=729，也就是循环展开之后执行729次请求都是没有问题的，这个时候叫做通过并发使得总线处于忙碌的状态当中。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172109717.jpg)

上图第二个，每一次执行的指令数量是有限的，他不可能执行非常非常多并发的数量

有了上面三个问题，于是就引入了硬件架构的一个问题，虽然并发操作能让一次能够执行更多的指令，能够执行流水的操作，但是同样架构，也会受到所限制所约束。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110568.jpg)

上面同样的demo，并行就是通过并行处理器，或者多个线程，去执行Ax+Y这个操作。同样使得总线处于忙碌的状态当中，每一次可以执行729个迭代，

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110486.jpg)

有一点和并发展开不同，就是现在每个线程独立的去负责相关的运算，也就是每个线程去计算一次Ax+Y，这么一个操作，假设现在要执行729次计算，一共就需要729个线程，也就是parallel一共有729次，这个时候遇到的瓶颈或者约束就会受到线程数量，还有内存请求的一个约束了。

在实际的硬件工作过程中，更多的倾向于利用多线程，对循环展开，来提高整体硬件的利用率，这个就是GPU最主要的原理。

来看下硬件限制的情况下，一般能够执行多少个线程，下图以三款芯片作为例子，

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110116.jpg)

从上表可以看到几个关键数据，GPU的时延会比CPU要高出好几个倍数的等级，第二个比较重要的就是GPU的线程数，是CPU的接近二、三十倍，另外一个比较重要的就是线程的数量，GPU的可用线程数是CPU的一百多倍。这就是GPU最重要的一个设计点，它哟拥有非常多的线程，为大量大规模任务并行而去设计的，因此所GPU，是一个大型吞吐机，有一部分线程在等待着数据，有一部分线程等待被激活去计算，有一部分线程已经正在计算的过程当中，GPU的硬件设计师将所有的硬件资源都投入到增加更多的线程当中，而不是想办法去减少数据搬运的延迟、指令执行的延迟。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110227.jpg)

相对应的，可以把CPU比喻成一台延迟机，希望一个线程里面完成所有工作，为什么线程比只有一点多，是因为希望能够用足够的线程去解决延迟的问题，所以这个时候CPU的硬件设计者，就会把所有的资源和重心都投入到减少延迟上面，这也是SIMD跟SIMT的架构的最大区别。

CPU不是通过增加线程来解决问题，而是使用相反的方式，去优化线程的执行的速率和效率。这就是CPU跟GPU之间最大的区别，也是他们的本质区别。

### GPU Cache 缓存机制

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110429.jpg)

我们尽可能的希望去减少内存的时延、内存的搬运，还有内存的带宽等一系列关于内存的问题。缓存对于内存来说，就变得非常重要了，显存就是GPU里面独立的内存。在这里面我们把一些寄存器文件也当做缓存，寄存器离我们的SM（真正的执行单元）是非常近的，因为我们的SM希望尽可能快速的去获取数据，于是就会从寄存器里面去读取cache里面的一些数据，

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110810.jpg)

在真正计算的时候，我们希望缓存能尽快用完，然后换下一批数据上来，这个时候就会遇到时延的问题，可以看出HBM的延迟是L1的15倍，这个时候就会很清楚的理解到为什么GPU里面有单独的显存。

假设把CPU里面的DRAM的数据传过来给GPU进行计算，那时延会非常非常的高，完全跟不上计算的速度，还有跟不上带宽的传输的速度，这个时候就要求GPU里面，有自己的一个高带宽的内存HBM，最后就通过PCIe来对数据进行传输，把CPU的DRAM里面的数据传输到GPU的HBM里面，

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110300.jpg)

上图左右边，假设HBM计算强度为100，那L2的缓存的计算强度就为39，好很多。意味着每个数据只要执行39个操作，而L1的缓存更少，它的计算强度更少，只需要8个操作，这个时候对硬件来说是非常容易实现的。这样就解释了L1  L2和寄存器对GPU来说这么重要了，可以把数据放在L1缓存里面，然后对数据，进行8个操作，使得计算达到一个饱和的状态，使得GPU里面SM具体的算力利用率更高。

往下看,PCIe的带宽就变得非常的糟糕，他的整体的时延也非常的高，整体的算力利用率也会很低，算力的强度、计算的强度太高，在带宽增加的同时，线程的数量和请求数也需要相对应的增加，这个时候才能够处理刚才所说的并行操作。每个线程执行一个对应的数据，才能够把算力利用率提升上去，只有线程数足够多，才能够让整个系统的内存处于忙碌的状态，让计算也处于忙碌的状态。

### GPU线程原理

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110432.jpg)

一个时钟周期内可以自行多个warp。

每个warp可以并发的执行

GPU的工程师主要是增加线程增加warp来解决或者掩盖延迟的问题。而不是去减少延迟的时间。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110645.jpg)

有时候看到GPU的算力利用率并不是非常的高，但是完全不觉得它慢，是因为线程是超配的。

### AI训练使用GPU

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110816.jpg)

#### 卷积计算

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110070.jpg)

上图左边的这一块内容，就是一些卷积核，我们的模板或者kernel，右边就是需要处理要带卷积的图片最后模板里面的内容，就是卷积核每一个元素跟图片里面的每一个元素逐元素相乘再相加就得到最终的一个输出feature map （最后边黄色）

在GPU里面真正去执行卷积运算的时候，不会通过刚才那种滑窗的方式对图片对数据进行卷积，而是把卷积变成image to column，把图片变成一个矩阵的向量，去模拟去恢复卷积的运算，原来的图片，我们会把它进行一个重排，把图片里面的一个窗口（如下图）123789，逐元素排成一排，然后再按chanel数往后排。通过这种方式对图像，进行一个矩阵的重排。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110242.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110228.jpg)

接着把kernel 把filter 把卷积核进行重排，卷积核的重排跟刚刚一样，把每一个卷积核123456789，然后重新展开成为一行（上图右边）。不断地去堆叠chanel的数量，然后每一列就是代表有多少个N，代表有多少个卷积核

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110211.jpg)

通过这种方式就把图片、把feature map、把kernel编程了两个大矩阵（上图左边），通过矩阵相乘的方式，得到最终的feature map。这种方式去模拟卷积的运算。

![](D:/download/youdaonote-pull-master/data/Technology/人工智能/ZOMI/19【AI芯片】GPU原理/images/WEBRESOURCEe7652c780f0b115d8407c9c4e398d4a0image.png)

简单总结：就是卷积的运算转换成为两个矩阵相乘的一个求解，最终得到卷积的输出。

 

#### GPU线程分级

 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110389.jpg)

Element-wise就是逐元素相加或者逐元素相乘的这种操作。

但是在真正卷积或者AI运算过程当中，会有大量的卷积大量的卷积就涉及到元素之间，其实是有交互的，数据之间是有交互的，我要算一个元素，可能就需要周边其它元素、周边的数据进行配合

第三种就是类似快速傅里叶变换，一个元素的求解得到另外一个元素，数据与数据之间并不能做到完全的线程独立。

下面以第二个Local Convolution 卷积作为例子，看看local memory在GPU里面跟thread是怎么去配合工作的。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172110747.jpg)

首先会用grid进行覆盖，切分成一个个块，其中就拿出中间的一个块进行处理，每个grid实际上还会分开不同的block，这里面的block就有可能重叠的。block会去独立的执行的

第三步里面 block里面会有大量的线程，通过本地的数据共享，或者叫做local data memory，来去进行计算。每个元素或者每个像素点都会给一个线程进行计算，这个时候就变成了**整个线程是分层分级的**。

 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111734.jpg)

网格式最高层的概念，网格之下，就会分成非常多的block，每个block里面，又有非常多的这种线程。

块中，block之间的线程是互相独立的，也就是block A和block B之间 里面的线程是独立执行的，而block里面的线程他们是共享本地内存数据的，就是共享local memory。这个时候就在每一个block里面 每一个块里面 就可以执行相同相关的操作

网络里面block其实在GPU里面是超额分配的。通过超额分配大量的这些block,而block里面又有大量的线程，每个线程就可以对像素进行大量的操作，而block的超额分配，就可以通过计算，去掩盖延时的问题。不管是网格也是，不管是线程也好，都会在block里面真正的执行。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111765.jpg)

之前强调过并行的能力是最重要的，而并行其实是为了解决，带宽慢，带宽时延长的问题。而到底需要多少线程是由计算强度来决定的。

 对于element-wise这个操作来说，每增加一个线程就以为着需要对数据进行一次新的加载，因为在GPU里面是并行的去加载的或者并行的去提供线程的，于是增加线程的数量，并不会对实际的运算，或者对实际的时延产生任何的影响，就变成数据的规模在合理范围内增大，并不会影响，实际算法的效率，并不会在计算的过程当中觉得它会变慢。同样的原因我们线程是分级的，对于卷积的这一类运算的时候，每增加一个线程，对于数据的读取，因为是并行的，它的影响也不会太大，这个时候GPU的执行效率跟AI计算计算模式之间就非常的匹配了。

强度规模都是O(1)，非常高效。因此在GPU里面通过线程的分层分级，能够很好的匹配到算法的一些强度。

#### AI计算模式和线程关系

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111004.jpg)

之前说过实际上卷积的计算，可以用矩阵乘来代替，脱离卷积的这个概念，真正的在我们的kernel，在GPU执行的阶段下，看看矩阵乘跟AI计算模式或者矩阵乘跟GPU之间是怎么进行更好的去融合交互的。红色矩阵的每一行跟蓝色矩阵的每一列，进行一个乘加的操作之后，就得到输出的橙色这块的一个元素，

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111948.jpg)

首先要提取红色矩阵里面的第一行的元素，还有蓝色矩阵第一列的元素，红色和蓝色每一个元素进行相乘，相乘完之后再逐个元素进行相加，从而得到最终的橙色的值。

在GPU里面因为会提供一个FFM或者MAC的操作（就是把乘加变成一个具体的指令）所以每一次乘法和加法，是一起去执行的。但是在矩阵乘里面，不仅是要算一行一列，可能以这一行（红色第一行为例），算蓝色第一列第二列.....第五列，然后得出橙色元素第一行的第一个元素，第二个元素...第五个元素。

在整个矩阵乘里面，红色的这一行，加载了一次，但是蓝色加载了5次，每一次都有5个元素，因此它执行了25次计算。随着相乘的矩阵的大小的提高，算力的需求就不断地提高了。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111952.jpg)

这个时候我们就知道了，其实还是有一个算术的强度的，需要进行的矩阵越大，算术的强度就越大，需要搬运数据的量就会增大，这个时候算术的强度的比也会相对应的增大  

#### 矩阵乘的算力利用率

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111442.jpg)

横坐标就是矩阵大小的增加，纵坐标就是算术的强度，或者计算的强度。从横坐标可以看出矩阵的大小从1-64，大小不断地增大， 计算强度成线性的增加

橙色的横线代表GPU浮点运算的计算的强度，从橙色和蓝色这条线，中间有一个交叉点，交叉点的位置就是GPU的浮点运算跟矩阵的计算强度的一个交点。为了满足整个GPU的计算强度，也就是让SM或者让计算单元，不断地去执行的时候，要求整个矩阵的大小大概是在50左右，这个时候就会满足整个计算强度，硬件就会忙碌起来。

理想情况下，为了让机器或者GPU在运算跟搬运数据的同时，保持平衡，就是百分百的速度去整体的运算，这个就是所说的GPU它是个吞吐机的意义。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111798.jpg)

现在急需把蓝色的这条线不断地扩大

就是矩阵大小为50的时候，能够充分发挥GPU在FP32的计算的强度，当矩阵的大小不断地增加的时候，GPU里面的内存就会空闲下来了。所谓的空闲不是指它的内存容量降低了，或者它不需要那么多的内存容量了，而是指内存的搬运越来越慢了，内存数据的刷新变得越来越慢了。因为在GPU里面的计算单元里面，需要花费更多的时间去对矩阵进行执行和运算。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111890.jpg)

AI的计算我们需要找到一个更好的平衡点，去匹配更大的矩阵运算，去找到更好的计算的强度，新的这条线（红色）

就是英伟达GPU里面，后来提出了tensor core ，专门针对矩阵进行运算，去提高计算的强度。使得我们内存的搬运跟得上我们数据的运算的速度。

不管是用于AI加速的GPU，还是用于AI加速的NPU、TPU也好，都在寻找中间最好的平衡点。在满足计算强度的提高的同时，还能让矩阵越大越好。因为在神经网络里面，处理的是非常大的一个矩阵的运算，而不是小矩阵的运算。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111860.jpg)

回到带宽和延迟的表里面，最右边一列，如果使用L1的缓存，整个计算强度是32，L2是156....，所以需要根据多级的缓存，缓存到底在哪里去搭配我们张量的核心，是的整个张量在小矩阵或者大矩阵里面，都能够更加高效的去执行运算。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111444.jpg)

上图就表明在L1缓存，L2的缓存，HBM的缓存里面，针对矩阵的运算的强度，有一个交叉点，这个时候就明白为什么说数据在哪里非常的重要，比算力的强度或者FLOP数更加重要。

假设数据已经搬运到L1的缓存里面，那这个时候可以执行一些，更小规模的矩阵运算，例如小的卷积核。那对于NLP这种大的transformer这种结构，可能会把数据搬运到L2，然后通过L2的cache去读取然后执行。因为数据跟读取之间是有一个关系的比例的，数据如果大量都在搬运，大量都在等待计算的时候，就会导致计算跟通讯之间的不平衡。因此找到计算的强度的点，对于系统优化变得非常重要。
### ISA-Instruction Set Architecture

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111693.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111812.jpg)

ALU：算术和逻辑单元

AI芯片很重要的一部分就是写一部分运算指令，包括谷歌TPU还有华为昇腾NPU，都会有自己独特的运算指令

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111742.jpg)

整体有6个生命周期，但不是所有的指令都会循环这个周期，但是这个周期基本上原则来说是不会变化的。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111912.jpg)

上图是更宏观的去看看指令集架构

指令架构有计算指令、数据寻址的指令，和控制指令，通过这些指令，我们连接好硬件，那硬件就可能有一些硬架构，还有一些IC的电路，还有逻辑门，对上面就对到了机器码，指令集架构对应的是机器码，对应到机器码之上，才是我们的编译器，还有我们的一些汇编，还有编程语言，再上去就是操作系统，还有算法还有应用。

 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111209.jpg)

1、计算机架构中的寄存器数量可以因架构而异。不同的计算机架构和处理器架构可能具有不同数量和类型的寄存器。

一般来说，计算机架构中的寄存器可以被分为多个类别，包括通用寄存器、特殊目的寄存器、浮点寄存器等。通用寄存器用于存储临时数据和计算结果，特殊目的寄存器用于执行特定的功能，如程序计数器、堆栈指针等，而浮点寄存器用于浮点数运算。

具体到某个架构或处理器，例如x86架构的处理器，常见的通用寄存器数量为16个，特殊目的寄存器和浮点寄存器则有各自的数量。而ARM架构的处理器则具有不同的寄存器架构，包括通用寄存器、程序状态寄存器等。

因此，寄存器的数量会因具体的计算机架构和处理器而有所不同。

2、ISA（指令集架构）定义了计算机体系结构中的指令集和操作。不同的ISA可以支持不同的运算操作，具体取决于其设计和功能。

一般而言，ISA可以支持以下基本的运算操作：

1. 算术操作：包括加法、减法、乘法、除法等基本的数学运算。

1. 逻辑操作：包括与、或、非、异或等逻辑运算，用于处理布尔逻辑和位操作。

1. 移位操作：包括左移、右移等位移操作，用于在二进制数中移动位。

1. 比较操作：用于比较两个值的大小或相等性，通常用于条件判断和分支操作。

1. 条件操作：用于根据条件执行不同的操作，例如条件分支和条件跳转。

1. 存储操作：用于将数据存储到内存或寄存器中，或从中读取数据。

1. 控制操作：用于控制程序的执行流程，例如函数调用、返回和异常处理。

此外，不同的ISA还可以支持特定的扩展运算操作，如浮点数运算、向量运算、加密指令等，这些操作可以提供更高级的功能和性能。

需要注意的是，具体支持的运算操作取决于所使用的计算机体系结构和处理器。不同的ISA可能具有不同的指令集和操作功能。因此，在使用特定的计算机体系结构和处理器时，应仔细查阅相关的文档和规范以了解其支持的运算操作。

3、当计算机遇到异常或中断时，通常会采取以下步骤来处理：

1. 保存当前状态：计算机会将当前的执行状态保存到特定的存储区域，包括程序计数器（PC）和其他寄存器的值。这样可以确保在处理完异常或中断后能够恢复到正确的执行点。

1. 转移控制流：计算机会根据异常或中断的类型，跳转到相应的处理程序或中断服务例程。这些处理程序通常是预先定义好的，用于处理特定类型的异常或中断。

1. 执行处理程序：一旦控制流转移到处理程序或中断服务例程，计算机会执行相应的指令序列来处理异常或中断。这可能涉及到错误处理、状态恢复、数据保存等操作，以确保系统能够正确地处理异常情况。

1. 恢复执行：处理程序执行完毕后，计算机会从之前保存的状态中恢复执行。这包括恢复程序计数器和其他寄存器的值，以及恢复被中断的程序或任务的执行。

需要注意的是，具体的异常处理和中断处理过程会因计算机体系结构和操作系统而有所不同。不同的体系结构和操作系统可能有各自的异常处理机制和中断处理机制。因此，在具体的计算机系统中，需要参考相关的文档和规范以了解异常和中断的处理方式和步骤。

4、

数据可以有多种类型，取决于编程语言和数据表示的需求。以下是一些常见的数据类型：

1. 整数类型（Integer）：表示整数值，可以是有符号（包括正数、负数和零）或无符号（只包括非负数和零）。常见的整数类型有：

int、short、long等。

1. 浮点数类型（Floating-point）：表示带有小数部分的数值，用于表示实数。常见的浮点数类型有：

float、double等。

1. 字符类型（Character）：表示单个字符，如字母、数字或特殊符号。通常使用

char类型来表示。

1. 布尔类型（Boolean）：表示逻辑值，只有两个可能的取值，即

true和false。

1. 字符串类型（String）：表示一串字符组成的文本。在不同的编程语言中，字符串类型的表示方式可能会有所不同。

此外，还有其他一些复杂的数据类型，如数组、结构体、枚举等。这些类型可以用来组织和存储更复杂的数据结构。

关于数据类型的字节大小，它取决于计算机体系结构和编程语言的规范。常见的字节大小包括：

- 1

- 2

4

8

- 字符串类型的字节大小取决于字符串的长度和编码方式。

需要注意的是，不同的编程语言和计算机体系结构可能对数据类型和字节大小有所不同。因此，在具体的编程环境中，应查阅相关的文档和规范以了解具体的数据类型和字节大小。

在计算机中，int加法和float加法通常是使用不同的指令来执行的。这是因为整数和浮点数具有不同的表示方式和运算规则。

对于整数加法，计算机通常会使用专门的指令来执行。这些指令可以直接对整数进行加法运算，使用适当的进位和溢出处理规则。

对于浮点数加法，计算机也会使用专门的指令来执行。浮点数加法涉及到更复杂的运算规则，例如对阶、尾数对齐和舍入等。因此，计算机需要使用特定的指令来处理浮点数的加法运算。

虽然整数加法和浮点数加法使用不同的指令，但它们通常都属于计算机的算术指令集（Arithmetic Instruction Set），用于执行数值计算操作。

需要注意的是，具体的指令实现和指令集架构可能因计算机体系结构和处理器设计而有所不同。不同的处理器可能对整数和浮点数加法有不同的指令实现和优化策略。因此，在具体的计算机体系结构中，需要参考相关的文档和规范以了解整数和浮点数加法的指令细节。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111275.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111432.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111426.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172111509.jpg)

AI芯片的方寸方式是跟我们的CPU跟GPU不一样的，所以AI芯片用的是一个CISC的架构，这里要澄清一点，虽然是从CPU衍生过来，但是反推过去，它不一定代表CISC就是针对CPU才能够设计的，可能AI芯片里面也可以用到SISC的架构，AI芯片里面或许也会用到RISC的架构

![](https://gitee.com/hxc8/images0/raw/master/img/202407172112462.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112550.jpg)

### CPU计算本质

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112687.jpg)

横轴坐标是代表算力的敏感度，每次操作能执行多少数据，

纵坐标是性能，每一秒能执行多少的操作，

可以看到随着PE的增加，PE就是计算单元，整体的计算的峰值肯定是越来越高的，当然了计算单元不会无限制的增加，肯定会有一个数量的限制，所以当PE数量增加到一定程度的时候，就会遇到一个理论的峰值。

左边灰色的这个峰块，就是带宽bandwidth的一个约束，右边就是计算bounded，计算的约束。

中间可以看到有一条非常之明确的线段（或者说是转折点）那这个就是带宽跟计算最好的一个中位数。

随着计算单元的增加，如果带宽跟不上，那这个时候的增加是没有效果的，因此需要寻求一个从带宽到PE之间一个最好的平衡点。也就是中间的这个转折点（如下图）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112601.jpg)

从AMD最新一个演讲视频截图，如下图，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112593.jpg)

可以看到服务器的整体性能的趋势，基本上没2.4年就会翻一番，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112875.jpg)

单个GPU的性能，基本上2.2年翻一番

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112886.jpg)

超算中心不仅仅指的是GPU以及CPU，更多的是指超异构的架构，除了单单CPU和GPU的算力增长，还有很多异构芯片的增长，于是可以看到性能基本上1.2年翻一番

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112761.jpg)

预测未来突破硅的极限，接近2纳米

一般大家都是强调自己的峰值算力比英伟达高。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112703.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112781.jpg)

不管怎么看，整体的峰值算力，也是越来越高的，大家都在强调峰值算力，但是客户真正在乎的是算力吗？客户真正在乎的是Flops嘛，是没买哦能够运行多少算力吗？

其实最重要的是物理定律，还有硬件本身很大程度决定了对机器的编程的方式，而机器的编程方式不一样，当我们深入到整个计算的本质的时候，我们就会有一个更深的体会，我的数据到底在哪里，真正关心的可能是数据搬运。我算的非常的快，但是我的数据来得及提供吗？如果我的数据来不及提供，那我们可能用于处于下图黄色部分的位置，而达不到峰值算力。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112871.jpg)

IPU（Intelligence Processing Unit）是一种新兴的处理器类型，专门用于进行人工智能（AI）相关的计算任务。IPU的设计目标是提供高效、高性能的计算能力，以加速深度学习、机器学习和其他AI工作负载的处理。

与传统的通用处理器（如CPU）和图形处理器（GPU）相比，IPU具有以下特点：

1. 并行计算能力：IPU具备高度并行的计算能力，可以同时执行大量的矩阵计算和向量操作，这对于深度学习和神经网络模型的训练和推理非常重要。

1. 低功耗高效能：IPU的设计旨在提供高能效的计算性能。它通过优化架构、采用低功耗设计和专门的硬件加速器，以实现高性能计算的同时降低功耗。

1. 特定于AI工作负载：IPU的设计和优化主要针对人工智能相关的计算任务。它提供了针对神经网络计算的特殊指令集和硬件加速器，以优化AI工作负载的执行效率。

相比之下，GPU（Graphics Processing Unit）是一种专门用于图形渲染和图形计算的处理器。虽然GPU在执行并行计算方面非常出色，并且已被广泛应用于加速AI工作负载，但其设计和优化仍主要面向图形渲染和游戏等领域。与IPU相比，GPU在AI计算方面可能存在一些性能和能效上的差距。

总结起来，IPU是专门为AI计算任务设计的处理器，具有高度并行、低功耗和特定于AI工作负载的特点。而GPU则是一种广泛应用于图形渲染和图形计算领域的处理器，虽然也可以用于加速AI计算，但在某些情况下可能不如IPU在性能和能效上具有优势。

读取数据与计算的计算换算。

首先内存会把它的数据传到CPU上面，每秒假设大概能够传输200G的字节（200G bytes per second）。而网上看一看CPU，大概每秒钟能够进行2万亿次的双精度，也就是Flops64的运算，这个数呢，看上去非常非常的夸张。不过这个计算量对于CPU来说，它只是一个非常经典的速率，我们知道每一个FP64是8个字节，那内存每秒传输200G的字节，也就是说每秒能够传输25Giga-FP64的数值，那这个数值对于内存来说就是每秒可以提供250亿个FP64的数据这是非常非常多的。但是CPU每秒确实能够处理2万亿个FP64的数据，这两个值一对比就是下图左边的那条公式，设备的计算强度。计算强度维持整体的计算平衡，也就是说对每一个数据都要进行80次操作。否则我们PE、CPU、PU就会处于空闲的状态，处于等待的状态。如果cpu不能做到对每个数据进行80次操作，那这个时候，还不如买一个更便宜的cpu，它的flop数，它的价格没有那么高，每秒没有必要去计算那么多flops。不过话说回来，每一个都进行80次操作，这真的有必要吗？我们真的会有算法对每一个数据都执行这么多遍操作吗？个人觉得在cpu里面是非常非常少见的。实际上只有一种非常特殊的情况。我们的AI，图形图像，也就是迎来了gpu。AI芯片对矩阵乘，可能会有这种情况。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112840.jpg)

当Flop的计算的速度的增加比我们内存带宽速度增加更快的时候，我们的计算强度就会去上升，就好像左边英特尔这一款，可以看到明显它的计算强度高了很多，而这个时候，就需要程序在算法上面不断地去做一些创新，来保持尽可能的去塞满，尽可能的去提升算力的利用率。

其实并不是说非常的在乎FLOPs，因为现在已经知道有足够多的FLOPs了，我们的GPU、CPU、npu已经有足够多的计算的能力了，我们要做的是提升算力利用率。如果不能够让CPU忙起来，那情况就会变得非常的糟糕， 因此我们更应该去关注的是内存、带宽还有时延，尽可能的提升算力利用率。

### CPU计算时延

更应该关注  内存、带宽 >> 时延

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112515.jpg)

上图有两个运算，一个是乘法，还有一个是加法。有两个内存的读取，第一个就是x[i]，另外一个就是y[i]。

有一个独特的操作，就是可以用一个指令去代替，例如FMA（fused multiply-add），就是把乘法和加法融合成一个指令，当然了，在之前叶建国可以叫做MAC，通过这么一个简单的例子，去看看整体时延是怎么产生的。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172112452.jpg)

上图叫做指令流水，叫做instruction pipeline 

首先需要从内存里面去读取一个元素x[0]，同时去读取另外一个元素y[0]，因为x[0]和y[0]之间没有依赖关系，所以我们可以同时从DRAM里面去读取。读取的过程中就会产生一个memory的latency，就是内存的时延。读完之后就可以read from cache，写到我们的cache里面，接着控制器就会从cache里面或者寄存器里面去读取x[0]，同时也可以去读y[0]，先去读取x[0]，然后执行a*x的操作，同样这个a也是一个元素，内存里面cache里面的一个元素，我们需要读取，然后乘的操作，其实FLOP计算的非常的快，只占了中间的这么一小撮，如下图红色框部分

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112691.jpg)

接着就写回结果，我的y[0]得到了，我的a*x的结果得到了。接着做一个+y的操作，然后加的操作同样也需要点时间，所占时间如下图红色框部分

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112645.jpg)

最后计算完Ax+B之后，就写回结果，写到cache里面，至于后面要不要写回内存，就是另外一个事情了。

可以看出整个指令流水，最耗时的是memory latency，内存的时延。

现在看几个物理上的概念。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112563.jpg)

光的传播速度3亿米/S，

计算机的频率（时钟周期），30亿赫兹每秒，所以在一个时钟周期内，光的传播速度为100mm（10厘米）

光速在一个时钟周期内传播的距离 = 300,000,000米/秒 ÷ 3,000,000,000Hz

光速在一个时钟周期内传播的距离 = 0.1米（10厘米）

电流的传播速度，这个电流主要指在硅芯片里面的传播速度，只是光的5分之一，6万公里每秒。这个时候就得出一个比价有意思的规律，就是一个时钟周期内，电流的传播距离是20mm（2厘米，也就是上面10厘米的5分之一）

假设有一个系统，有CPU、DRAM，电流只是简单的从CPU传到dram里面，它的一个举例大概是5-10毫米，也就是使用5-6个时钟周期了。5-6个时钟周期就是产生了我们的时延，而在CPU里面一个时钟周期，ALU可以自行非常多个FLOP的计算，也就每秒执行几亿万次的时钟周期计算，这就衍生了时延的问题。

上面还是站在一个整体系统的角度去看待问题的，如果我们放在处理器内部，放在晶体管内部，实际上数据之间的搬运，就是把一个晶体管里面的数据，传输到另外一组晶体管，于是刚才就用了晶体管里面的数据的传输速率去反推计算系统里面跟数据之间的一个关系，里面就引出了一个时延。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112661.jpg)

还是以Ax+Y的demo

假设内存带宽有131GB每秒

内存时延89纳秒

这就意味着在89ns（纳秒）以内，能够传输11659个字节的内容

这里换算按1000来的

131GB=131*10^9字节  ，然后秒换算成纳秒

131*10^9字节/1*10^9=131字节/纳秒

131*89=11659字节/89纳秒

而刚才的demo是Ax+Y传输了16个字节（x[0]和y[0]是双精度，每个占8字节，一共16字节），也就是x[0]和y[0]在89纳秒以内，这个时候16字节对比11659个字节整个内存的利用率只有0.14%，是非常非常低的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172112805.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113953.jpg)

从上图可以看出0.14已经是整个AMD，英特尔英伟达里面，数据传输效率内存利用率最高的了，为什么会有这种情况发生呢，为什么英伟达更低呢？这个时候就很有意思了，就涉及到计算，计算刚才只是搬运了少量的数据，而英伟达A100或者GPU大部分都是对大量的数据进行大量的计算，同一组数据进行非常之夸张惊人的计算，GPU就非常不擅长做一些小数据的计算，而是对大数据进行大计算。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113291.jpg)

回到上图指令架构图，大部分事件，我们的内存都是在等待的时间，都是在传输的时间，而内存的时延就严重的阻碍了计算时延，现在内存利用率都这么低，更不用说计算的利用率了，可以看到整个计算里面这整个时间周期里面，整个程序的周期，计算只有中间这两个点，就是乘和加，只占了里面可能非常少的一个时间（如下图红色框）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113573.jpg)

最后总结下，CPU的架构真的只擅长逻辑控制，而计算的利用率其实并不高，如果要提高CPU的FLOP数，提到非常非常高，其实这是不合理的，提高它的内存带宽的利用率很高也是不合理的，更多我们想要提高计算的利用率，可能不仅仅是依托于CPU，而是依托于超异构架构（CPU+GPU+NPU组合起来）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113543.jpg)

### GPU架构看AI

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113541.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113445.jpg)

出现了顶点级可编程性，同时像素级也具有有限的编程性

顶点级可编程性（Vertex-level programmability）是指在图形渲染管线中对顶点数据进行编程的能力。传统的图形渲染管线包括顶点输入阶段、顶点处理阶段、图元装配阶段和光栅化阶段等，其中顶点处理阶段对输入的顶点数据进行变换、变形、光照计算等操作。

顶点级可编程性引入了顶点着色器（Vertex Shader）这一概念，它是一种由开发者编写的程序，用于在顶点处理阶段对输入的顶点数据进行自定义的计算和变换。通过顶点着色器，开发者可以控制顶点的位置、颜色、法线、纹理坐标等属性，以实现各种特定的图形效果和变换操作。

顶点级可编程性的引入极大地提高了图形渲染的灵活性和表现力。它使得开发者能够通过编写自定义的顶点着色器程序，实现各种复杂的几何形状、动画效果、变形效果等。通过对顶点数据的编程控制，可以实现高度个性化的图形效果，并且能够适应不同的应用需求和场景。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113130.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113466.jpg)

2006年退出CUDA

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113730.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113868.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113148.jpg)

计算能力的提升只是CPU里面很小的一部分

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113278.jpg)

 首先GPU得cache是非常少的，第二个，它有很少的控制单元，不擅长处理，if else for这种控制流语句，第三就是有非常多的ALU（上图绿色部分）ALU已经成矩阵排列，这里面很有意思的就是cache非常的少，DRAM并不大，但是里面却有非常多的计算单元，这意味着有非常之长的latency，但是会有一个很高的throughput（时延可能稍微相对长一点，但是吞吐是非常非常的大）。第四点就是具有非常多的线程。每个线程去控制一个ALU，或者控制多个ALU，通过大量线程的并发，可以处理并行的内容。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113559.jpg)

CPU里面cache很多，是因为它能做很多大量的缓存，缓存起来，提供后面的数据进行处理，但是GPU得cache它的缓存并不是保存之后供后续进行访问的，而是为线程提供服务的。

GPU是一个SIMT的架构 ,T就是线程，很多时候，大量的线程，需要访问同一段数据，也就意味着可能一段数据要执行非常多的计算，这个时候缓存就会合并很多这种线程的访问，然后再去通过多级流水，去访问DRAM。获取数据之后，cache会统一分发到对应的线程上面，所以它的作用不是为了保存后面要访问的数据，而是方便线程进行合并读写。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113980.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113450.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113887.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113237.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172113454.jpg)

### AI芯片NPU基础

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114716.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114022.jpg)

专用芯片的好处，就是可以专门针对某一类的应用作为加速的，最开始的就有之前提过的图形图像处理器（GPU），现在一般叫做GPGPU了，就不仅仅能够处理一些图形图像的加速，还能够处理很多并行的内容。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114233.jpg)

上图右半部分是AI芯片dataflow架构。，往右边的这个图是AI芯片里面的dataflow架构，可以看到右非常多的feature map，还有input kernel，这些内容都写在硬件上面，都是以神经网络的计算模式，作为芯片的硬件的基础和设计原则，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114646.jpg)

上图可以看出CPU，大部分的工作都是再做一个控制，里面就占了芯片电路板面积的大部分，而里面的计算单元，其实并不多，我们经常说的4核8核到现在的32核64核，它的核数还是非常的少的，

而了解到GPU，可以看出里面SM数（计算单元）就有上千个，而这里面GPU的控制单元反倒是很少。

**NPU更多的是以AI Core,Tensor Core这种方式进行加速的，这个AI Core就是专门用来加速神经网络里面的卷积、transformer、MatMul这种计算**

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114759.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114906.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114980.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114116.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114274.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114615.jpg)

  

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114776.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114013.jpg)

 

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114265.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114490.jpg)

### 超异构并行计算

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114877.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114778.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114753.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114082.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172114151.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115518.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115766.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115792.jpg)

GPU其实是不能单独工作的，需要跟CPU进行一个配合，既然两种不同架构在一起配合工作，于是就迎来了第一种异构的芯片，异构的工作流程。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115761.jpg)

上图，首先CPU要处理一些工作的时候，它会把一些数据存在里面的DRAM，所以在第一步就会把DRAM的数据搬运到HBM（也就是GPU的显存里面），接着第二步的时候，CPU就会发射一些相对应的指令，给到GPU的cuda core或者GPU的线程真正的去执行计算，计算完之后，GPU就会把相关的结果返回并存储到HBM里面，最后一步，会把一些GPU计算完的结果，返回给CPU，这个时候GPU计算的是一些特殊的加速或者特殊的需要进行并行的一些计算的功能，这个时候CPU跟GPU之间的异构，就非常明显了。

CPU主要是用来处理一些通用的应用逻辑程序，而GPU在一开始出现的时候，主要是来处理一些图形图像，渲染的一些工作。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115683.jpg)

![](D:/download/youdaonote-pull-master/data/Technology/人工智能/ZOMI/18【AI芯片】芯片基础/images/WEBRESOURCE4e0d9cad26fe4bef25fb478be08f1441image.png)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115242.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115327.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115274.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115325.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115506.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115750.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115918.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115035.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115106.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115227.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115190.jpg)

CUDA开放了自己的整个变成的体系，虽然CUDA这个语言是没有开源的，里面的编译层也没有开源，但是它整套编译体系是开放出来了，让更多的人去介入，也形成了现在GPU的一个非常之良好的生态。 

![](https://gitee.com/hxc8/images1/raw/master/img/202407172115058.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172116151.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172116378.jpg)

虽然我们计算系统或者超异构系统很难定义，但很多时候是依靠于编译体系去承载的，具体的计算由编译去决定的，很多时候各类各样的应用，大部分怎么泡在不同的芯片上面，靠的就是编译体系，而计算体系基本三都是定型的，根据计算的原则来去定义，有了这些计算原则之后，就需要定义编译体系，就需要反向的去对编译体系，提出各种各样的需求。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172116524.jpg)

 

![](https://gitee.com/hxc8/images1/raw/master/img/202407172116580.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172116164.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172116317.jpg)
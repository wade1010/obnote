### 引子

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122281.jpg)

上图中上部分就是一个神经网络层，每一层有大量的连接线，这一些连接线就构成了权重W1、W2、W3.....各种各样的权重，网络模型越多，权重参数就越大。

第二步，有了神经网络之后，需要定义一个优化的目标。就是告诉程序，预测的到底是个什么东西，接着定义了损失函数和优化器之后。

第三步，就是计算梯度并更新权重了，就是计算每一个权重，w1到w2到wn的一个权重参数，然后从而不断的形成一个循环，去更新权重参数，使得损失值越小越好。

实现一个简单神经网络里面的一个算子，如下图

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122522.jpg)

以第一个卷积为例。可以看到实现一个卷积的算子，卷积简单的就是一个滤波的操作，从原始的图像o_n到o_c里面，不断的去迭代每一个batch的数据（o_h），然后再找个窗口（o_w）里面不断的去滑动卷积核，然后去计算卷积核跟原始模板的一个数，然后得到这个数进行累积，最终就得到整个卷积核的值。

可以看到有非常多的for循环，实现一个一个神经网络非常复杂。要是CPU里面执行这么复杂的for循环操作是很耗时的，所以就想出了多线程算子加速。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122512.jpg)

以maxpool算子为例，实现起来也很复杂，嵌套了很多个for循环.

找个时候就有个问题，如果把这些并行操作跑在GPU、NPU一些通用或者AI加速芯片上？

假如上面算子都实现了（卷积  relu maxpool 全连接 softmax），如何把这些算子串起来？串起来后，反向的算子怎么实现呢？

### 自动微分

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122892.jpg)

程序员写完正向网络，反向的怎么办呢？无论正向还是反向，构建的都是一个有向无环图，通过构建找个有向无环图，表示正向和反向。

实现微分的方式有：1、符号微分（优点：精确数值结果，缺点：表达式膨胀，部分算子无法求导（Relu，Switch等））2:、数值微分，使用有限差分来近似（优：容易实现，缺：结果不精确、计算复杂度高）

3、自动微分，就推出了表达式追踪。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122857.jpg)

圈圈就是中间变量

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122796.jpg)

上图灰色的其实也是中间变量，把这些中间变量保存起来，通过虚线逆向去求倒

数值微分（设定一个很小的数，然后用差分的方式去求解）、符号微分（人工展开，计算数值精确，但表达式膨胀）、自动微分（把v和dv一起给计算机求解）

自动微分不是符号微分，符号微分也实现不了自动微分。

一般就会写一个正向表达式，反向的由AI框架自动微分的功能去帮我们实现的。

自动微分的三种方法：

1、表达式或图

2、操作符重载

3、源码转换

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122979.jpg)

无论正向还是反向，构建的都是一个有向无环图

反向产生新的节点，方向还会用到正向的这些节点的变量，所以需要把中间变量都存起来。这个就是非常吃内存的原因、神经网络的图保存了大量的中间变量。

### 计算图

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122919.jpg)

计算图主要是有张量还有算子组成，tensor作为最基本的数据结构，主要是对高维的数字进行表示，它是对标量、向量矩阵的一种推广

【3,2,5】从后往前看，5维的一个数组，2就是数组有两排，3就是3组2*5的数组，这样就组成了一个3维shape维325的一个张量。

在一般情况下，张量（Tensor）中的元素应该具有相同的基本数据类型。张量是一个多维数组，它的元素在内存中是连续存储的，并且其元素类型在创建张量时被确定。

然而，某些特殊情况下，存在一些特殊的张量类型，例如异构张量（Heterogeneous Tensor）或联合数据类型（Union Data Type）。这些张量类型允许在一个张量中存储具有不同基本数据类型的元素。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122027.jpg)

算子不仅是加减乘除，卷积、batchNorm、sigmoid等等，包括现在的transformer也已经成为一个大的算子。

小的算子可以是加减乘除，根号、sin、cos，复杂神经网络的算子，可以类似于CuDNN,提供了LSTM RNN transformer这一类大算子。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122061.jpg)

- torch.Tensor是整个package中的核心类, 如果将属性.requires_grad设置为True, 它将追踪在这个类上定义的所有操作. 当代码要进行反向传播的时候, 直接调用.backward()就可以自动计算所有的梯度. 在这个Tensor上的所有梯度将被累加进属性.grad中.

- torch.ones(n, n, requires_grad=True)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122625.jpg)

构建这个反向计算图，不同框架有不同方法，感兴趣的可以后续再聊，大体就是三种，一种就是前向计算保留中间结果，然后根据反向模式的原理依次计算出中间的倒数，最后通过表达式追踪，把这些过程存起来。问题就是保存大量的计算结果和中间变量结果，优点就是方便跟踪计算过程，pytorch为代表。

第二种，将倒数的计算也表示成计算图进行统一表示。缺点：不利于调试跟踪，优点：方便全局优化，节省内存。TensorFlow为代表。所以在部署阶段还是有很多人用TensorFlow。部署的时候用TensorFlow，开发的时候用pytorch

第三种，就是将前两种结合起来，比如条件语句for while这些控制流，第二种表示起来比较难。

这里面还有非常多的知识，有兴趣的可以深入了解下。  

![](https://gitee.com/hxc8/images1/raw/master/img/202407172122637.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123878.jpg)

看下训练和推理计算图的一些对比

下面简单说下图优化

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123065.jpg)

这些概念大家可以看下，接下来就简单介绍下其中的算子融合

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123062.jpg)

(画图)可以把C算子和D算子进行融合，这个时候已经减少了一次kernel的开销，也减少了一次中间数据访存。

第二种就是算子a计算完之后，分别给b算子和c算子，b算子和c算子是并行执行的，这里面就会有两次的访存。可以利用算子融合，ab、ac是同时并行的，这个时候只执行两次kernel的开销，并且只需要一次访存。

还有很多优化内容，什么布局转换、内存分配等，有兴趣后续可以展开讨论。这里就不继续说了。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123938.jpg)

第一个卷积放在GPU1........第四个放在GPU4上面.这就是最简单的放在不同设备执行，一旦放到不同设备就涉及到多种调度模式，右边就是三种调度模式。

第一种串行，现在一般没人这么做了。

把各个卡的结果concat 到一起。最终输出。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123947.jpg)

上面是一个具体的例子，简单的例子，这么切分后，把1-2两个卷积放到server0执行，另外放到server1执行，中间会有两个通讯原语或者通讯算子，一个是发送一个是接收。最终把结果concat到一台服务器上做聚合。

这里可以通过send/recv这些算子去代替数据传输，也可能直接通过NCCL这种通讯原语或者通讯算子进行一个数据传输。

计算图还有更深入的一些内容，比如表达控制流、动静统一等，这些这里就不展开了。

下面，结合前面提到的计算图讲讲现在一些常见的框架

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123976.jpg)

PS：这里可以把上图和下图的右边画出来，让大家看看有什么区别，然后引出编译器话题。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123261.jpg)

现在还有第三种就是结合前两种的融合架构

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123298.jpg)

这里可以看出pytorch也在做这一个，JIT就是just in time，还有一个就是AOT，ahead of time

通过pytorch和TensorFlow的图对比发现pytorch没有编译器。

### 编译器

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123290.jpg)

可以大致看下对比。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123221.jpg)

以前的定义，前端 中间优化 后端，通常来说3段是非常分明的，但是实际实现过程中分解不明显

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123225.jpg)

GCC里面C的前端是对应多个后端的。中间是 没有通过所谓的IR去提取的。没有做分界就会引发一个问题，工程师要懂前端语言、还懂中间优化、还要懂不同架构硬件的指令。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123308.jpg)

LLVM提出IR概念，中间表达，其实上面说的计算图，也算是一种IR。各种语言对接到IR里面，编译器前端工程师想要增加一种语言，只需要了解IR就行了，然后中间优化、底层优化都不需要了解。

假设新增件一个HB-OS的硬件，只需要对接到IR，后端的工程师，只需要把寄存器、硬件调度、指令调度对接到IR，就可以在LLVM编辑器里面增加一个硬件支持了

这里为什么想说说编译器呢，因为CUDA底层就是用了LLVM作为编译器后端。所以一般来说兼容CUDA应该是比较好做的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123299.jpg)

而大名鼎鼎的GCC编译器在设计的时候没有做好层次划分，导致很多数据在前端和后端耦合在了一起，所以GCC支持一种新的编程语言或新的目标架构特别困难。好像GCC代码是有150多万行，不过目前应该也在做IR，但是具体什么时候能做完还不清楚。

### 分布式并行

下面开始讲讲分布式并行，从单卡-》单机-》AI集群，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123384.jpg)

训练数据规模可能是越大越好，另外一个是单步的计算量，单步计算量也是跟网络模型相关的，现在的模型量越大，它单步计算的时间肯定是越长的。另外一个就是计算速率，计算速率越快，总体耗时肯定越小。前两个模型相关，相对固定，第三个计算速率是个可变因素。所以就想法提高计算速率。单设备计算速率：就是每一张卡的计算速率，制程越高肯定速率越快

设备数：越多，训练越快

多设备并行效率（加速比）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123388.jpg)

这里主要是通过增加设备数，设备数多了，就变成一个集群了，有了集群就要去解决服务器架构问题、通信拓扑问题、软硬件通信问题、框架能在集群跑的问题等等。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123453.jpg)

（可以画下这个图）这个时候就引入了分布式并行架构，这个架构其实在框架里面都已经做好了。我们只要去调用就行了。

下面内容主要围绕通信与协调。

设备数越多不代表计算速率越高，设备数多到一定程度，计算速率就上不去了。所以要通过优化网络，或者服务器的架构来提升计算速率。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123470.jpg)

三种

1、CPU作为参数服务器，首先CPU把一些计算下发下来，就是把网络模型给到每一个卡去执行运算，然后（蓝色）做梯度聚合，最后CPU做完梯度聚合，把所有参数广播给每一个机器。 

2、GPU0作为参数服务器，首先CPU把一些计算下发下来，就是把网络模型给到每一个卡去执行运算（计算梯度和损失函数）然后做梯度聚合到GPU0上面，最后把参数广播给其它GPU。

3、 所有GPU作为参数服务器，首先CPU把一些计算下发下来，就是把网络模型给到每一个卡去执行前向和反向运算，接着通过蓝色模块对它进行梯度聚合，最后更新参数并广播。**这种就是分布式并行还有大模型所采取最常用的一种方式**

定义完分布式模式架构后，看看多个设备具体怎么进行同步的，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123516.jpg)

白色代表空载的时间，优点：保证模型的参数都是同步的， 缺点也很明显：造成计算硬件资源浪费

![](https://gitee.com/hxc8/images1/raw/master/img/202407172123491.jpg)

device1做完前向计算之后，再做反向计算，然后反向计算之后，就把所有参数同步给另外一个设备。但是这时候device2还没有计算完第一个反向，还在执行第一个正向计算，这个时候就会引起很大的问题，模型不收敛。相同数据训练完之后，又被后面来的重新覆盖了，所以异步并行没真正落地使用。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124393.jpg)

于是就出现了一种半同步的方式，就是动态去调整等待的时间窗口。去掉非常慢的，以大多数可以接受的时间窗口去更新。

上面讲的这些内容，在网络拓扑里面是怎么实现的呢？

其实有个环同步，GPU跟GPU之间是通过Nvlink进行链接的，如果没有nvlink就走pcie

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124673.jpg)

（画图）一个接口负责接收数据，一个接口负责发送数据，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124123.jpg)

（结合PPT动态展示）第一步是scatter reduce 第二步是all gather。有5个GPU，每个GPU都有自己的数据，

首先会把a0的数据给a1,不断地循环迭代。遍历完一个环之后，可以看到每一个设备都有一个全部数据的备份。接着把这个所有数据广播给其它GPU，第二步也就是all gather。这个比较简单，执行完一个环之后，所有设备拥有所有数据。

然后再聊聊统信的具体实现方式，

一般可以分为服务器内部和服务器之间。

服务器内：共享内存、PCIe、nvlink（直连模式），还有一些操作系统里面学的，进程通信有哪些？（消息队列，管道，信号量、共享内存等）

服务器间：TCP/IP、RDMA（直连模式），这个RDMA可能没怎么听过，这个我之前做高性能存储那会接触过，

cpu offload、kernel bypass、zero copy。这些都是做高性能软件最终可能需要考虑的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124136.jpg)

（如果需要可以画一下网络拓扑图）

上面说的都是硬件，统信协调的软件方面就不展开了，记住一点英伟达的叫NCCL，这个可以拓扑检测、通用路径搜索、针对NVIDIA优化等。NCCL API从CPU启动，GPU执行，在GPU内存之间移动或者交换数据，最后利用nvlink聚合多个高速NIC的带宽。

下面讲下通信原语，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124243.jpg)

大模型可能一张卡都放不下或者一台服务器都已经放不下，于是会对网络模型进行切分，切分完之后，可能放在不同的机器里面。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124287.jpg)

大的模型切分成很多小的模型，每个小的模型之间是有相互依赖的，所以需要跨节点对数据进行同步，同步的过程就需要进行通信 ，跨节点、跨卡的通信，

比如pytorch里面用分布式训练里面可能会有一个方法叫average_gradients(model)，底层就是用了一个all_reduce方法

这个all reduce，在这里可以称为通信原语。

集合式通信方式：

一对多：Scatter/Broadcast

多对一：gather/reduce

多对多：all-reduce/all-gather

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124339.jpg)

这个需要展开讲吗？如果需要就往下讲

#### 集合通信详细（需要将就展开）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124184.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124194.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124167.jpg)

所有数据都汇聚在一张卡上，这个gather不进行任何计算，分布式并行算法里面经常用来做排序和搜索，然后再广播给其它卡。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124107.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124218.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124293.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124447.jpg)

现在开始往硬件接触的更多了

#### 分布式并行策略

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124744.jpg)

第一个就是数据并行，单机多卡，每个卡模型都是一样的，每一个卡上面算一部分数据，每个卡上的模型算出自己的梯度，然后把这个梯度重新更新一下，然后再把新更新的参数分发到各个模型中。三个模型参数始终保持一致。数据并行，有一个条件，你的模型不是很大，只是单纯的数据多。单张卡能加载的下。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124064.jpg)

如果你的模型非常大，比如说是130B的模型，正常的GPU都没有这么大的显存，把模型分成几个模块，比如分成4个部分，每一块对应一个GPU，GPU0算完以后，把数据给到GPU1，GPU1再算，依次执行到GPU3，反向传播的时候，GPU3先更新一下参数，依次到GPU0，完成一个个参数的更新。

但是有一个问题，白色区域，GPU处于一个闲置状态。 四块GPU利用率不是很高，所以下图，有一个优化策略

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124582.jpg)

把训练的数据切成很多份，GPU1先算第一份F1，然后传给GPU2,传过去以后不用等，直接把第二份数据F2进行计算，让GPU空闲的点变得少一些。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124011.jpg)

比如8块GPU，上面4块一组，下面4块一组，上面4块可以用一个流水线并行，把模型拆分成4份，下面四块也一样，然后这两大部分之间可以再同步梯度，

就是说，流水线并行和数据并行是可以同时运行的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172124242.jpg)

张量并行，涉及到矩阵运算的一个内部结构，比如上图，两个矩阵相乘，机会发现结果中，四个数字都是可以独立计算出来的，可以进行并行运算，可以搞两种并行，

第一种列并行，把A矩阵单独拆成两列，X不变，上半部分在GPU1，下半部分在GPU2中，同时计算，然后把两个结果拼起来就是最终结果。代价是占用内存，X复制了两份

第二种行并行，把X竖向对半分，A也一样对半分，然后进行局部乘，最终拼接成结果。找个和原始的存储数据量是一样的，也就是没有所谓的内存浪费

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125419.jpg)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125413.jpg)

列并行，算完矩阵相乘以后，其实还有一个激活函数，列并行好处，就是不听的套激活函数的时候，可以把激活函数分解并行激活，也就是列并列，最终的Y1、Y2是可以独立激活的，而行并行里面是不能先激活Y1再激活Y2再相加，这是不行的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125447.jpg)

在做大规模运算的时候，一般都是流水线并行，再张量并行，再数据并行，一起来。

流水线并行，就是模型过大，张量并行，矩阵很大，数据并行，训练数据大。

推理的时候，流水线并行用的比较多。

#### 并行处理硬件架构

下面就开始转向硬件，先说说并行处理硬件架构

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125507.jpg)

单指令流单数流（SISD）系统。

单指令流多数据流（SIMD）系统。

多指令流单数据流（MISD）系统。

多指令流多数据流（MIMD）系统。

I就是指令instruction D就是数据

SIMD：

每个指令部件每次仅译码一条指令，而且在执行时仅为操作部件提供一份数据

串行计算，硬件不支持并行计算；在时钟周期内，CPU只能处理一个数据流。

SIMD：

一个控制器控制多个处理器，同时对一组数据中每一个分别执行相同操作

SIMD主要执行向量、矩阵等数组运算，处理单元数目固定，适用于科学计算

特点是处理单元数量很多， 但处理单元速度受计算机通讯带宽传递速率的限制

适合向量矩阵的计算，适合科学计算的场景。

MISD:

多指令流单数据流机器，采用多个指令流来处理单个数据流

作为理论模型出现，没有投入到实际应用之中

MIMD:

在多个数据集上执行多个指令的多处理器机器

共享内存 MIMD 和分布式内存（这里可以展开讲下，画图）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125496.jpg)

基于SIMD的新架构SIMT，T就是thread线程。使用这个的代表就是GPU。

单指令多线程，有效地管理和执行多个单线程，允许一条指令的多数据分开寻址,而且用户可以控制每一个线程，每一个线程都有自己的逻辑，非常方便开发者进行编程。

硬件这里面其实还有指令集，比如我们比较关注的sw_64、arm、x86等等。CISC和RISC啦。这里就不展开了。

### GPU

#### 位宽

这里开始切入到GPU了，在说GPU之前，大家得了解下数据位宽。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125441.jpg)

TF32怎么分布，大家猜猜。还有TF32+,增加了精度的位数

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125575.jpg)

（下图仅做范围示范）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125563.jpg)

用好GPU首先要知道数据位宽，在大模型或者神经网络里，其实都是浮点数，float也分很多种，如上图。有些GPU只支持某几种，

拿FP32介绍下，1位正负，8位指数部分，表示取值范围，还有23位表示精度。

用FP32，精度肯定会越高，副作用就是内存的占用大小。算力消耗变大。会对你的GPU带来非常大的压力，有可能你的机器根本跑不起来。

对于训练使用 FP16、BF16、TF32，反而FP32用的不多。

推理 CV 任务以 int8 为主，因为CV大部分都是分类问题，分类相对不需要高精度，概率差那么一点点也不会影响最终结果。

NLP相对来说稍微高一点，因为NLP做分类的时候，比如预测一个词，都是几十万挑1甚至几百万挑1，那么精度可能会对最终结果造成影响。

大模型 int8/FP16 混合，最后一层用FP16，前面用int8，这样可以让模型需要的内存资源、算力资源比较小。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125643.jpg)

看一下这个混合精度。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125915.jpg)

（PPT无上面的图，可以写写）如果我们只适用FP16，权重是2^-3，梯度是2^-14，然后要做梯度下降，就是权重+梯度，但是FP16，他表达不了这种精确的相加，它进度没那么高，最终它会把2^-14舍去，最终等于2^-3，所以梯度下降法因为数值计算的原因导致了四舍五入的失去精度。10进制打个比方10.1、10.2、10.3统统变成10.

所以这个地方需要一个混合精度，FP32.

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125974.jpg)

了解下这几个指标，后续用的到。FLOPs也就是说一个神经网络的一层模型，计算的时候，它需要消耗多少算力

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125917.jpg)

（无上图，要画图）CPU到GPU的异构工作流程。

#### GPU计算本质

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125828.jpg)

深度学习或者大模型里面，无论什么运算，最后都转成矩阵运算， 比如CNN做卷积，图像，卷积核，图像其实在GPU里面会展开，卷积核也会进行展开，展开成2个特别大的矩阵，然后做乘法，结果再把它折叠回原始图像。本质上都是一个矩阵运算。GPU就是对矩阵运算做一个加速。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125958.jpg)

就是一个相乘再加的乘加运算。分解为两个算子，一个乘法，一个加法。

下面从一个例子说明CPU有什么问题，GPU是怎么样解决问题的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125480.jpg)

CPU每秒钟可以上传200GBytes数据，所以每秒可以上传25G-FP64，然而CPU每秒可以算2000GFLPs FP64,,这样的话，如果要让计算单元饱和运行，就需要一个数据做80次运算。要是运行不到80次，那么计算指标都是虚的，因为给它的数据远远不够，但是正常情况下，没有一个数据需要算80次，没有这种场景，所以只能增加带宽，让每秒传输更多的数据。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125781.jpg)

50-100mm传输的时间要89纳秒，这是延迟。每秒能传131GB/s,所以89纳秒的时间理论上可以传11659bytes。但是我们做ax+b的运算，实际上只传了16字节。16/11659=0.14%。找个内存利用率也是非常低的，因为我们数据传的不够多，大部分处于闲置状态。为了让机器满负荷运载，要不然，指标都是虚的。

V100A100工艺是把内存直接跟计算处理单元焊在一起，这样延迟比较小。而游戏卡大部分还是通过外接电路来做的。因此延迟比较高。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125185.jpg)

内存读取x、y到寄存器，发现真正用于计算的时间是非常少的。大部分都是传输延迟过程中。

解决办法就是并行运算，依次传很多数据，x[0],y[0]、x[1],y[1]、x[N],y[N]，，通过并行计算，一次多传依稀诶需要的数据，然后进行大规模运算。这样计算的就会饱和。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125204.jpg)

可以看出内存利用率都挺低的（有一些提高手段，这里不考虑）比如至强8280，就用729个线程，同时算。729*0.13%约等于1

GPU线程数更多。

GPU用超多线程解决内存利用率比较低的问题，但是它的延迟还是很高的。所以延迟它不太关心，更多的是通过大量的线程来解决这个问题。

对于CPU来说更加关注延迟。但是不可能单纯通过降低延迟，也肯定要增加线程，只不过增加的线程没有GPU多。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125315.jpg)

从内存传数据到计算单元，其实是有两个东西，一个是延迟，多久能传到，一个是带宽，一次能传多少数据。

从L1到L2到HBM，延迟是越来越大。带宽是越来越小。

我们数据放在计算机的内存条里面，那么CPU和gpu的通信，通过PCIe的话，带宽非常低，延迟非常高。这也就是GPU为什么要单独配一个显存的原因。

L1 每秒可以接收19400GB的数据，带宽是非常高的，每秒能扔给寄存器的数据是非常多的，如果它要满负荷运行，只需要算8次就行了。 到后面（最下面的PCIe）要让机器满负荷运行，你每条数据要算6240次，这种场景在现实中根本没有场景，

下面有一张非常经典的图

![](https://gitee.com/hxc8/images1/raw/master/img/202407172125850.jpg)

这是一张非常经典的图，paper链接在这里，横坐标MAC，每条数据需要算多少次，纵坐标是计算强度，实际计算消耗。如果每条数据算0次，也就是不需要计算，其实没有计算消耗，随着你需要计算数据越多，计算消耗越大，但是GPU得算力是有上限的，超过上限后，MAC就算再怎么增加也是不会增长了。

如果每条数据算的非常少的时候，GPU是能够应付的，但是你没有打满，有很多时间是空载的，到第一次交叉点是刚好打满的。再多工作量的时候，GPU也计算不过来了，再增加就会增加延迟，

假设途中这条斜线是带宽为a的情况，然后另外一个块GPU，带宽是b，且b>a,也就意味着每秒能往计算单元里面传的数据会更多。那么同样的MAC，我传的数据多了，消耗的算力就更大，所以斜率会更加抖。

假如a带宽MAC需要50，b贷款MAC需要10，就能够让GPU打满，因此高带宽会比较好，因为实际场景中，a带宽（每个算50次）的场景遇不到，带宽太低，导致GPU利用率比较低，

实际GPU运行情况是由，MAC（跟模型有关，决定横坐标的点）、带宽（决定斜率 ）、， 算 力（决定上限）。国产的GPU，它很多时候都说算力很强，不提带宽，所以实际表现没有那么强。所以不能专门追求算力，要追求三者的一个平衡。

#### GPU线程原理

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126810.jpg)

下面开始说说GPU的线程原理，

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126662.jpg)

可以看到这个是A100的抽象架构图，有108个SM，SM上面有L1，下面是L2，最下面是HBM高带宽内存，，这就是分级缓存。右边是一个SM里面具体的情况，可以看到有很多warp，这个后续会讲到，一个SM里面有64个warp，每个warp可以同时去并行执行多个线程。下面还有线程调度器、寄存器。

#### 线程分层执行

下面以一张图作为例子，给大家介绍下GPU线程分层执行的原理

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126901.jpg)

先用网格对图片进行覆盖，分割成9个块，拿出其中一个块进行处理，块是独立执行的，块里面会有大量的线程，可以对它负责的像素点或者图片块进行处理。

可以理解为网格就是非常非常多的线程，然后块，是其中一部分，块里面也有非常多的线程，块里面的线程独立执行，可以通过local memory进行数据共享和交换数据。

这样每一个块里面就可以执行相同操作了，这里面线程是超配的，通过大量的线程掩盖了延时的问题。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126786.jpg)

再回到之前的一张图，讲一下AI计算模式与线程关系。 

大家都了解AI计算里面一般都是转为矩阵乘加，最终在硬件上执行，

红色矩阵每一行跟蓝色矩阵每一列进行一个乘加，就得到橙色的一块。

GPU主要是通过FMA(Fused multiply- add)指令在一个运算周期内完成一次先乘再加的浮点运算。

而且tensor core MACs运算是支持混合精度运算的，这里需要强调的是MAC操作是在一个cycle里面完成的。

这里我们要算一行，然后蓝色取第一列、第二列。。。第五列然后得出橙色第一行的第一个元素、第二个原色。。。第五个元素。这里红色每加载了一次，蓝色加载了5次。因此一共进行了25次计算。随着相乘举着大小的提高，算力的需求就提高了，这个时候就有提到了前面说的算术强度。需要进行相乘的矩阵越大，算术的强度越大，需要搬运的数据量就会增大。

![](D:/download/youdaonote-pull-master/data/Technology/人工智能/ZOMI/images/WEBRESOURCEb29c8b930cb280608224dc2b517bd5e4截图.png)

看一个矩阵大小和计算强度的关系图，随着矩阵大小增长，矩阵乘法计算强度就跟着增加，

比如矩阵大小为8的时候，计算强度大概对应10次，那GPU算力是要超过的，这个时候资源是被浪费的，随着数据的增加，比如达到50的时候，GPU计算能力刚好被打满，数据再增加，GPU可能算不过来，这样就会导致延迟，设计大模型的时候尽可能让算力处于交叉点的位置。（为了 让计算和搬运数据之间保持一个平衡）

为什么精度精度越高的数据反而对应的计算强度越大呢？因为精度越高的数据，它对应的内存越小。也就是比如你同样1M的内存，FP64能传的比较少，，因此他计算强度反而能够算更多次。

（当矩阵大小不断增加之后，比如超过56之后，GPU里面的内存就会空闲下来，所谓空闲是指内存的搬运越来越慢了，内存数据的刷新越来越慢了，因为在GPU里面的计算单元里面需要花费更多的时间去对矩阵进行计算。）所以后来出现了专门针对矩阵乘，提高了计算强度，使得内存搬运更得上运算的速度。比如tensor core  、TPU、NPU等。大概计算强度能超过300。

这里面其实还能看出一个事情，数据存在哪里也很重要，比如在寄存器里面，容量小，带宽足够，所以计算强度很小。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126319.jpg)

下面开始聊聊GPU的架构。这里主要是指英伟达的GPU。因为GPU也迭代了好多代了，所以这里大家熟悉下，知道有这么一个东西，可能在某一代里面，这个就改了，由于时间关系，我这里就没详细展开每一代的变化。但是会把几个比较重要的版本拿出来简单讲下。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126576.jpg)

可以先看下上图左边的概念，其中SM的核心组件包括CUDA核心、共享内存、寄存器等。SM包含许多为线程执行数学运算的Core，是 NVIDA 的核心。SM里面可以并行执行数百个线程，每个线程执行对应的指令。

看完我们在展开一个SM来看看。（下面图有详细介绍）

![](D:/download/youdaonote-pull-master/data/Technology/人工智能/ZOMI/images/WEBRESOURCEc1bbdbcb8fde02b9afbd81533fb7d1a2image.png)

看完这些，我下面介绍下这里面的CUDA core(切图)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126010.jpg)

大家观察下，上面两个图。有没有发现之前给的整体架构图里面没有CUDA core，这里有（上图左边）。这里没有（上图右边）

左边这个图，CUDA Core 在 Fermi 架构里提出，是最小的运算执行单元。而且有一点说一下Fermi架构是首个完整GPU计算架构。2010年提出的。每个CUDA core里面，包含一个ALU和一个FPU（一个 SM 中包含了有 2 组各 16 个 CUDA Core，每个 CUDA Core 包含了一个整数运算单元 ALU (Integer Arithmetic Logic Unit) 和一个浮点运算单元 FPU (Floating Point Unit) ）

倒是到了右边这个图的架构，其实就是volta架构，变成了单独的 FP32  FPU 和 INT32 ALU。这样做的好处是每个 SM 现在支持 FP32 和 INT32 的并发执行，更好地提升运算吞吐量和提升系统利用率。

再介绍下上图的左右两个图的最上面都有一个warp scheduler组件。

从逻辑上来说，所有的线程是可以同时执行的，但是从硬件的角度，不是所有线程都可以同时执行的，所以英伟达就引入了warp这个概念，通过warp去控制线程，通过warp对线程进行锁同步，然后拆解成具体的指令，给计算单元去执行。

Warp 是 SM 基本执行单元，一个 Warp 包含32个并行 Thread，这32个 Thread 执行于 SIMT模式。也就是说所有 Thread 以锁同步的方式执行同一条指令，但每个 Thread 会使用各自的 Data 执行指令分支。如果在 Warp 中没有32个 Thread 需要工作，那么 Warp 虽然还是作为一个整体运行，但这部分 Thread 是处于非激活状态的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126159.jpg)

本来是需要讲讲CUDA的，碍于这个篇幅，当时看了下已经70页PPT了，就不讲了，简单看下架构图，跟大部分底层软件的架构都差不多，然后我们用的最多的可能就是这个CUDA libraries，比如cuML、cuDNN就属于这里，这里讲下CUDA软件层面和GPU硬件层面的对应关系，其实之前也都提到，这里总结下

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126100.jpg)

（可以画下上图右边大体对应图）

[https://zhuanlan.zhihu.com/p/622972092?utm_id=0](https://zhuanlan.zhihu.com/p/622972092?utm_id=0)

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126352.jpg)

（可以画下上图）给大家画一下图，让大家增强一下印象。

#### 重要的GPU版本

开始讲几款比较重要的GPU版本。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126564.jpg)

（跟着念念）这里面的主角就是NVLINK

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126675.jpg)

多机之间，采用InfiniBand和100Gb Ethernet通信，单机内单GPU到单机8 GPU，PCIe 带宽成为瓶颈。所以英伟达就提出了第一代nvlink，带宽就带到了160GB/s ， 实现单台服务器内GPU可以高效数据互联。

下面再说一个重要的版本，伏特架构

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126941.jpg)

大家一看就知道这里面的核心是哪个，那就是tensor core咯。专门针对矩阵乘进行计算加速。这里面其实还有NVlink第二代

这里面PC有了解的吗？线程是进程内的执行单元，每个线程都有自己的程序计数器（Program Counter，PC）和栈（Stack）。PC是一个特殊的寄存器。用于存储当前正在执行的指令的地址。它指示了下一条要执行的指令在内存中的位置。每个线程都有自己的程序计数器，这意味着每个线程都可以独立地跟踪它自己的指令执行状态。从而实现并发执行。这种独立性和并发性是多线程编程的基础，允许程序在多个任务之间进行并行处理，提高系统的性能和响应能力。

这里面最重要的就是tensor core，后面有介绍，这里先不展开。有一点注意的是从这个架构开始取消了CUDA core，应该理解为硬件层面取消了CUDA core的概念，软件层面其实还是保留的。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126982.jpg)

上面GPU的架构发展图，就是总结下我印象比较深的几个东西，2006年提出CUDA,2010年费米架构，首个完整GPU计算架构，2016年提出nvlink，2017年出现第一代tensor core。

其实这里面的ampere架构和hopper架构也是非常可以说道说道的，篇幅太多了。我这简单说几句，

比如ampere：

超过540亿晶体管，

tensor core3.0,新增TF32，针对AI的扩展，可以使得FP32精度的AI性能提高20倍。

NVlink2.0，互联速度加倍

稀疏矩阵的加速，这个也挺有意思的，大概就是把一个稀疏矩阵压缩成一个稠密矩阵，压缩的部分就是0或者跟0相乘的，有一个索引记住这个位置，计算的时候能够依靠这个索引进行还原。

还有多实例GPU，将单个A100划分多大7个独立的GPU，为不同人物提供不同算力。为云厂商提供不同算力切分的方案把。

比如hopper架构：

这个就是CPU和GPU的异构架构了，CPU和GPU之间都是nvlink链接了，带宽可以达900GB/s，

还有一个就是单独针对transformer做一个优化，叫transformer engine。当然还有好多指标的提升，就不说了。

总的来说也就是从2016年开始，英伟达才越来越牛逼的。大模型训练用的最大的是ampere架构的A100，一般都是整机卖，比如DGX，8个A100，我们公司的都是通过PCIe插到主板上的，这个是直接焊在主板上。

### tensor core

下面介绍下CUDA core。

先读上图第一段

那现在一个Tensor core，一个指令就可以执行4X4X4的一个GEMM，也就是64个FMA，极大减少了系统内存的开销、硬件搬运的开销。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172126145.jpg)

读一读

![](https://gitee.com/hxc8/images1/raw/master/img/202407172127376.jpg)

读一读

可以看出这个tensor core是支持混合精度计算的。这个混合精度的使用是训练加速的必要条件。

下面介绍下tensor core跟cuda之间的关系

![](https://gitee.com/hxc8/images1/raw/master/img/202407172127692.jpg)

在CUDA里面其实不是控制每个线程，而是控制warp，一个warp包含很多线程，同一时间去执行。warp就是软件层面做一个大的线程概念。

在CUDA程序执行过程中，多个 Tensor Core 可以同时通过线程 wrap 来执行，在一个 Wrap 内的线程可以通过 Tensor Core 来提供16x16x16 的矩阵运算。上一张图不是4X4X4嘛，这个怎么16X16X16了？（切到下图）

![](https://gitee.com/hxc8/images1/raw/master/img/202407172127645.jpg)

Tensor Core其实是一个4x4x4的核，但是一个SM里面有很多个tensor core，就跟之前warp概念一个样，不可能去控制最细粒度的线程一样，这里也不可能最细粒度的去控制每个tensor core，这样效率会很低。所以这里warp又做了一些优化，就把好几个tensor core包装起来，对外提供一个16x16x16的一个warp level的WMMA指令，最后通过mma_sync这个方法进行计算。

（有了tensorcore硬件之后，在真正CUDA变成的时候，会通过warp把多个tensorcore里面的线程聚合起来做计算，最终对外提供一个16x16x16的wmma的api给到CUDA，因此用户最终看到的是16x16x16的warp wmma的api）

前面讲了这么多，现在有一个问题，看看大家有什么想法。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172127512.jpg)

（停留在这，听听看有没有什么想法）

其实前面讲了这么多，最想讲的可能就是这里了。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172127293.jpg)

针对用户场景，GEMM会非常大，会通过多级的缓存，利用数据的局部性拆分成block、warp还有thread，最终通过thread提供实际的tensorcore的运算。

这里以C=A X B为例。上面的大GEMM，就好比这里蓝色和黄色的矩阵，两个相乘得到绿色的矩阵，这个大矩阵肯定不能直接放到tensorcore上面进行计算，实际计算的时候会取片段的数据，也就是fragment，取完后就到了第二张图，这个时候成为thread block tile，对应具体硬件部分就是线程块，在真正线程块执行的时候，就会再把其中一部分数据，再提取出来变成warp level的计算，也就是第三张图warp tile，这个warp level的计算其实还是很大，继续变成满足CUDA矩阵输入的计算，也就是threat tile，这了是概览。下面详细看看每一步！

   

CUDA 没说

175B参数模型，权重参数320GB（FP16+FP32），参数放内存占用700GB内存（HBM），训练过程会增加7~8倍内存开销~5600GB内存（权重+激活+优化器状态等），NPU 64G显存需要87张卡才能放下一个大模型。

175B参数模型，预训练需要 ~300B Tokens 的语料数据训练一轮 Epoch，因此 87 张卡HMB不能全用于放模型，需要留有一大部分给数据（Batch Size）。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172127204.jpg)

Warp： 线程束。逻辑上，所有Thread是并行；但是，从硬件的角度来说，并不是所有的 Thread能够在同一时刻执行，这里就需要Warp的引入。

Warp 是 SM 基本执行单元，一个 Warp 包含32个并行 Thread，这32个 Thread 执行于 SIMT模式。也就是说所有 Thread 以锁步的方式执行同一条指令，但每个 Thread 会使用各自的 Data 执行指令分支。如果在 Warp 中没有32个 Thread 需要工作，那么 Warp 虽然还是作为一个整体运行，但这部分 Thread 是处于非激活状态的。

这句话的意思是，一个 Warp 包含32个并行的线程（Thread），它们以锁步（SIMT）模式执行指令。也就是说，所有的线程会同时执行相同的指令，但每个线程会使用自己的数据来执行指令的不同分支。

如果在一个 Warp 中并不需要32个线程执行任务（例如只有24个线程需要工作），那么这个 Warp 仍然会作为一个整体运行。但是，那些没有工作的线程处于非激活状态，它们不会执行任何指令。这样可以确保整个 Warp 的执行保持同步，而不会因为部分线程缺失而导致错误。

简而言之，Warp 中的线程在执行指令时以锁步的方式工作，但如果有线程没有工作，它们会处于非激活状态，不参与指令的执行。这样可以保持整个 Warp 的同步性和一致性。

以锁步（SIMT）的方式是指在一个Warp中的所有线程同时执行相同的指令，但每个线程可能会使用不同的数据来执行指令的不同分支。简单来说，锁步执行意味着所有线程在同一个时钟周期内执行相同的指令。

在锁步执行中，Warp中的线程被分为小组，每个小组中的线程按顺序依次执行指令。线程之间会进行同步，以确保在执行下一条指令之前，所有线程都完成了当前指令的执行。这样可以保证在同一个时钟周期内，所有线程都执行了相同的指令。

锁步执行有助于提高并行计算的效率和吞吐量，因为它允许多个线程同时执行相同的指令，从而避免了线程之间的等待和冲突。同时，通过使用不同的数据来执行指令的不同分支，锁步执行也提供了灵活性和并行性，以满足不同任务的需求。

指令缓存（Instruction Cache）和指令缓冲区（Instruction Buffer）是计算机体系结构中的两个不同的概念：

1. 指令缓存（Instruction Cache）：指令缓存是用于存储已经从内存中读取的指令的高速缓存。它位于处理器内部，用于加快指令的访问速度。指令缓存通常是由硬件实现的，它会将最常用的指令存储在高速缓存中，以便处理器能够更快地获取指令并执行它们。指令缓存的大小和组织方式会对指令访问的效率产生影响。

1. 指令缓冲区（Instruction Buffer）：指令缓冲区是用于存储已经被解码的指令的一部分。在指令执行的过程中，处理器需要对指令进行解码，将其转换为可执行的微操作。指令缓冲区是用来存储已经解码的指令，并提供给后续的执行单元使用。它可以用来提前解码指令，以便在执行阶段更高效地执行指令。

因此，指令缓存和指令缓冲区有以下区别：

- 指令缓存是用于存储从内存中读取的指令的高速缓存，而指令缓冲区是用于存储已经解码的指令的一部分。

- 指令缓存位于处理器内部，用于加快指令的访问速度，而指令缓冲区通常位于解码阶段，用于提供已解码的指令给执行阶段使用。

- 指令缓存的作用是减少指令访问的延迟，而指令缓冲区的作用是提前解码指令，以提高执行效率。

综上所述，指令缓存和指令缓冲区是两个不同的概念，用于不同的目的，并在处理器的不同阶段发挥作用。

时钟周期和时钟频率是计算机科学中重要的概念。

时钟周期，也被称为振荡周期，是计算机中最基本、最小的时间单位。它表示的是在计算机内，一个事件发生所需的时间，通常以时间单位为秒。在一个时钟周期内，计算机的CPU只完成一个最基本的动作。

时钟频率则定义为每秒钟发生的时钟周期数，其单位通常为兆赫兹（MHz）或吉赫兹（GHz）。例如，如果计算机的时钟频率为1 GHz，那么它每秒钟会产生10^9个时钟周期。

时钟周期和时钟频率的关系是：时钟周期 = 1 / 时钟频率。例如，如果时钟频率是1 GHz，那么每个时钟周期就是0.001秒。
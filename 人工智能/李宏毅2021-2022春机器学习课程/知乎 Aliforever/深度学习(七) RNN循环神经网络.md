## 前言

*本篇文章将会介绍另外一种神经网络，是RNN循环神经网络*

## 一、Slot Filling(槽填充)

### 1.Chatbot简介

chatbot是一个对话系统，基本分为两类:闲聊型对话和任务驱动类。前者产生有意义且丰富的回复，后者则是生活中常见的智能机器人。目前市面上的技术都是基于规则型的，这种机器人的对话生硬，但是准确率高，而基于生成式的chatbot，则可以生成更加丰富、有意义、特别的对话响应，但是准确率不高。

### 2.Chatbot结构

1. **开放域多轮对话**

![](https://gitee.com/hxc8/images0/raw/master/img/202407172047609.jpg)

由于前面没有任何对话，当用户发出第一个对话的时候，Chatbot并不能很清楚用户的意图，所以此时就开始进入了开放域多轮对话，随便抽出一个回答进行回答。 但是，机器从用户的信息中，获取到了奶奶这一信息，奶奶是一个**槽(slot)**,在这里实际上就开始进行了**槽填充(slot filling)**。

1. **准入条件**

![](https://gitee.com/hxc8/images0/raw/master/img/202407172047250.jpg)

从一个开放域转入到封闭域，或者从一个封闭域转入到另一个封闭域，中间的跳转是需要逻辑判断的，而这个逻辑判断就是准入条件。正正是因为有了这个准入条件，才能进入封闭域。而上面触发这一个准入条件的恰恰就是*打车*这一个关键字。当机器读取到这一个关键字后，就关闭了开放域，转而询问了

![](https://gitee.com/hxc8/images0/raw/master/img/202407172047210.jpg)

在这里插入图片描述

1. **封闭域多轮对话**

![](https://gitee.com/hxc8/images0/raw/master/img/202407172047071.jpg)

从上面我们知道，现在已经进入了封闭域，封闭域中会存在多个槽位，这些槽位都可以枚举的，当这些槽位没有完全填充完时，机器将会一直询问用户，得到相应的数据填充槽位。这个过程我们称为澄清话术。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172048745.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172048485.jpg)

在这里插入图片描述

### 3.槽的性质

1. **可默认填写/不可默认填写** 

1. **澄清话术** 

1. **澄清顺序** 

1. **平级槽或依赖槽** 

1. **槽的能力：多轮记忆状态** 

## 二、RNN(循环神经网络)

### 1.为什么要使用RNN?

前面我们了解到，卷积神经网络CNN能够很好地处理图片，但是在处理语句问题上就会出现问题，这是因为CNN没有记忆性，输入和输出的一一对应，也就是一个输入得到一个输出。不同的输入之间是没有联系的。换就话说，在处理语句问题中它把前后几句话，几个词语孤立起来，这样子整句话的意思可能就有了翻天覆地的变化。 $RNN(循环神经网络)$是一类用于处理序列数据的神经网络。序列数据的特点:后面的数据与前面的数据有关系。

### 2.RNN的基本结构

传统神经网络的结构比较简单：输入层 – 隐藏层 – 输出层。RNN 跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。如下图所示：

![](https://pic1.zhimg.com/80/v2-6c7db633cdb9cbe05c0c20860df91cd4_720w.webp)

在这里插入图片描述

在上图我们可以看到，有两个一模一样的**Taipei**字样，如果是传统的神经网络，这两个一模一样的字符输出来的结果肯定是一样的，因为每一个输出都是独立的输出。但是通过RNN网络时，由于前面的隐藏层存储了上一层的输出，所以这两个**Taipei**字样虽然输入是一样的，但是通过隐藏层的存储不同，最后计算出来的结果也是不同。

![](https://pic3.zhimg.com/80/v2-5a87f17db960bb3643d6842fb4b671ae_720w.webp)

在这里插入图片描述

我们把它展开，其中$x_t$是输入，$s_t$是一个向量，它表示隐藏层的值。$U$是输入层到隐藏层的权重矩阵，$V$隐藏层到输出层的权重矩阵，$W$是隐藏层上一次的值作为这一次的输入的权重。 ��=�(���)��=�(���−1+���) $f()$是神经网络中的激活函数，做一个非线性的映射，将实现信息过滤（如将不重要的信息过滤掉，表现为概率低的信息）

### 3.Bidirectional RNN(双向神经网络)

![](https://pic3.zhimg.com/80/v2-5c4e9541986f2e0ea841a12c0d756306_720w.webp)

在这里插入图片描述

这样使得预测结果不仅输入了过去的信息，还有现在的信息，同时涉及了前向和反向的传播信息以及未来的信息,就可以联系上下文信息来进行预测，一定程度上利用了整个序列的信息进行预测

### 4.RNN的缺点

1. **RNN只能处理短时记忆的问题，无法处理很长的输入序列**

1. **RNN 是一种死板的逻辑，越晚的输入影响越大，越早的输入影响越小，且无法改变这个逻辑。**

1. **RNN会产生梯度消失和梯度爆炸**

## 三、LSTM(长短期记忆网络)

### 1.LSTM的优缺点

- 优点

- LSTM是RNN的一个优秀的变种模型，可以有限度地抑制RNN出现的梯度消失问题

- LSTM可以长期的保存输入。一种称作记忆细胞的特殊单元类似累加器和门控神经元：它在下一个时间步长将拥有一个权值并联接到自身，拷贝自身状态的真实值和累积的外部信号。

- 缺点

- 并行处理上存在劣势。与一些最新的网络相对效果一般

- 计算费时。每一个LSTM的cell里面都意味着有4个全连接层(MLP),如果LSTM的时间跨度很大，并且网络又很深，这个计算量会很大，很耗时。

### 2.LSTM的结构

![](https://pic4.zhimg.com/80/v2-c1a12499b2cf17e2d183fef22f3736c3_720w.webp)

在这里插入图片描述

我们可以看到LSTM里面存在三个特殊的神经网络作为门信号控制(**遗忘、输入、输出**)，还有一个作为输入。 这个图反映了LSTM里面详细的内部结构，下面将会慢慢剖析这个内部结构。

![](https://pic2.zhimg.com/80/v2-2a57dc0e93591f4faaf9be6feb43f409_720w.webp)

在这里插入图片描述

其中$x_t$是输入，$h_t$是输出cell经过sigmoid函数归一化后的值，而黄色方框中的$\sigma$跟$\tanh$都是一个神经元，分别代表不同的函数，其中$\tanh$的值域是(-1,1)

### 1.遗忘门

![](https://pic4.zhimg.com/80/v2-642ec0b2f77daecfb5d40eee42e7f4f7_720w.webp)

在这里插入图片描述

遗忘门中，将输入$x_t$与上一层的归一化输入$h_{t-1}$进行两个向量连接起来，然后经过激活函数后输出到第一个cell节点上，如果$f_t$越接近1，说明$C_{t-1}$里面的数据保留程度就越高，反之越接近0，则遗忘程度就越高。

### 2.输入门

![](https://pic3.zhimg.com/80/v2-3ef2abd4dee86769aee8cd7a25ef63e6_720w.webp)

在这里插入图片描述

![](https://pic3.zhimg.com/80/v2-18b6328f7208d18c466a5775a051f756_720w.webp)

在这里插入图片描述

输入门中加入了一个$tanh$函数处理进来的数据，另一部分与遗忘门一样。然后两者加权相乘，跟遗忘门一样，如果$C_t$越接近1，说明$C_t$里面更新的数据就越多，反之越接近0，则更新数据越少。

### 3.输出门

![](https://pic4.zhimg.com/80/v2-c6aca97aefb1bd983eb30d2f9748a68f_720w.webp)

在这里插入图片描述

输出门则通过与细胞$C_{t-1}$中存放完的值作两个向量连接起来，得到的结果则为输出，当乘积越接近0时，则越阻止输出，只有接近1时，才允许输出

## 总结

本文介绍了RNN，希望大家能从中获取到想要的东西，下面附上一张思维导图帮助记忆。

![](https://pic2.zhimg.com/80/v2-a7a1597620264e44d1b61501e3ea7cd5_720w.webp)
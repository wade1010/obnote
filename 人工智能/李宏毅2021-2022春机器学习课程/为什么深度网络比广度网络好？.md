虽然一个hidden layer的network可以表示任何function，但是当你要表示某个function的时候，往往用一个deep架构会比较有效的。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172053401.jpg)

假设有一个function，就如上图绿色的function，你可以用一个矮胖的neuron来产生这个function，也可以用高瘦的neuron来产生这个function，但是往往使用高瘦的neuron需要的参数量比较少。矮胖的neuron反而需要比较多的参数。

所以你会发现你要产生某一个function，用高瘦的neuron，反而比较有效的，所以用deep learning并不是比较容易overfitting。deep learning真正的优势所在，当你要产生某一个方向的时候，你需要较少的参数，比较少的参数意味着比较不会overfitting，或者是你需要比较少的训练参数。

跟你想的或者坊间传的是正好相反的，因为一般印象是，deep learning就是大资料、容易overfitting，但事实上DeepLearning它真正的强项反而是不容易overfitting。

那为什么把network叠多层会得到这样的效果呢？

先举几个例子

![](https://gitee.com/hxc8/images0/raw/master/img/202407172053563.jpg)

上面不是一个有效率的方法，

 

如果你要用逻辑电路来解parity check的问题，根部不需要用到2^d次方的gate，如果依照上面的例子，输入是4个bit，其实只需要3个XNOR gate就可以得到我们想要的结果了，XNOR就是上图右上角

   

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054526.jpg)

所以你发现说当你把这些XNOR gate不是排成两层，而是排成一个deep的结构的时候，就是第一个XNOR gate的输出是下一个XNOR gate的输入，下一个XNOR gate的输出是下下一个XNOR gate的输入，当你把它串成一排的时候，会用比较少的gate，就做到比较复杂的事情。这是用逻辑电路来做举例。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054639.jpg)

下面用程序来举例

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054769.jpg)

不会把所有程序都写在main里面，会在适当的时机去调用其它的module，每一个module或者function里面都还有他们去调用的sfunction。有这种结构的好处，它会避免你的程序太过冗长。这种结构的好处是什么呢？有这种结构的好处是避免你的程序太过冗长，增加它的可读性，如果同一个功能在一个程序里面反复使用的话，可以去调用同样的function，让你的程序更加简洁，所以在写程序的时候，你也会用到顶的结构。

下面再举个例子

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054778.jpg)

剪窗花，把纸折起来，在折起来的纸上剪几刀展开就有了一个复杂的图案，其实这个复杂的图案你也可以直接去剪它，但要剪出这个复杂的图案， 是很麻烦的，你要剪很多刀

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054162.jpg)

但是如果把纸折起来，我们只需要剪几刀就可以剪出复杂的图案，比较有效。所以这个折纸的过程，把纸对折再对折这个过程，其实就是DeepLearning里面的layer在做的事情。

举例完毕。

下面是真正推到为什么DeepLearning需要更少的参数

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054395.jpg)

 以此类推，假设从0-1之间，你想让neuron network的output y呢，它有2^k次方的线段，你需要多复杂的neuron network呢？

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054766.jpg)

你只需要k层，每层两个neuron，总共2k个neuron。就可以达成想上图右边的input和output关系。

但是同样的关系，你想要用一个shallow network，你要怎么做到呢？假设输入是x，输出是y，但是你的network只有一层，你要怎么产生上图右边锯齿状的输入和输出关系呢？

也不是做不到，因为我们说就算是只有一个hidden layer，其实也可以制造出任何可能得function，但是你需要大量的neuron，因为当做一个hidden layer的时候，假设你用的activation是relu，那每一个neuron只能制造一个线段出来，2^k次方的线段，就需要2^k次方个neuron。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054860.jpg)

所以比较deep和shallow，要产生同样的function，DeepLearning，2k个neuron，shallow需要2^k次方个neuron，差距是非常显著的。

DeepLearning的参数量比较小，它有比较简单的模型，而shallow的network它的参数量比较大，你需要一个比较复杂的模型，而复杂的模型比较容易overfitting，或者是比较复杂的模型，因为比较容易overfitting，所以你必须要大量的资料，所以DeepLearning跟你直觉想象是反过来的。很多人认为DeepLearning需要大量的资料，但事实上你要做到同样的事情，你不用DeepLearning，选择shallow network，需要更大的资料。
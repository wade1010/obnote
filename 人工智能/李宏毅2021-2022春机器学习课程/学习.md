![](https://gitee.com/hxc8/images0/raw/master/img/202407172054997.jpg)

为什么小的batch size在train set上得到比较好的结果。为什么noisy的update,noisy的gradient会在training的时候给我们更好的结果呢？

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054101.jpg)

一个可能的解释，假设是full batch，那么在你update参数的时候，是沿着一个Loss Function来update参数，update的时候走到一个local minima或者一个saddle point，显然就停下来了，gradient是0(如果不去看识别是不是local minima、saddle pont，就是李老师之前将的课)，你就没办法再更新你的参数了。

但是加入是small batch的话，因为每一次是挑一个batch出来，算它的loss，等于每一次update你的参数的时候，你用的loss function都是略有差异的

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054299.jpg)

你选到第一个batch的时候，你是用L1来算你的gradient的，你选到第2个batch的时候，你是用L2来算你的gradient的，假设你用L1计算gradient的时候，发现gradient是0，卡住了，但是L2它的function跟L1又不一样。L2不一定卡住。所以L1卡住了，没关系，换下一个batch来。L2再算gradient。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054482.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054560.jpg)

Momentum

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054783.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172054205.jpg)
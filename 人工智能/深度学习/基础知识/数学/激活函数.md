![](https://gitee.com/hxc8/images2/raw/master/img/202407172203752.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203459.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203427.jpg)

如上图，输入x，经过w1,w2,w3几次线性变换等到out

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203628.jpg)

然后一步一步反过来求这个参数，

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203224.jpg)

当求某一个位置的时候，由于加入了sigmoid非线性激活函数之后，它会使得，求偏导过程中，一旦为0（接近为0），

由于是累乘，一个为0，结果都为0，所以说会杀死梯度。所以现在sogmoid函数没那么流行了。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203477.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203729.jpg)

阶梯式更新，使得效果比较差，收敛速度一般

其实就是两个缺点：

1 杀死梯度

2 非原点中心对称会产生阶梯式的更新，收敛饱和得比较慢

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203364.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203127.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172203742.jpg)

leaky relu ，小于0部分稍微往下偏一点。
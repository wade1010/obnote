在 PyTorch 中，contiguous() 方法用于确保张量在内存中是连续的。虽然这听起来可能有些抽象，但它在某些情况下是必要的，特别是在进行某些操作（如 view）之前。

当你对一个张量进行转置（transpose）或挤压（squeeze）等操作时，PyTorch 并不会立即改变张量在内存中的布局。相反，它会创建一个新的视图（view），这个视图在逻辑上表示相同的数组，但在内存中可能不是连续的。这种情况下，如果你尝试对这个视图进行某些需要连续内存布局的操作（如 view），PyTorch 会抛出一个错误。

contiguous() 方法的作用就是将这个视图转换为一个在内存中连续的张量。这样，你就可以安全地对其进行需要连续内存布局的操作。

view 方法用于改变张量的形状，而不改变其数据。它类似于 NumPy 中的 reshape 方法。
## 1.1 如何解决检索出错：embedding算法是关键之⼀

### 1.1.1 针对「Bert的预训练过程是什么？」检索出的结果与问题不相关

使⽤原始的langchain-chatchat V0.2.6版本，会出现对某些问题检索不到的情况

⽐如问⼀个⾯试题：Bert的预训练过程是什么？

其在⽂档中的结果如下：![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407291544325.png)
但基于m3e的系统实际检索得到的内容如下：

```
出处 [1] 2021Q2⼤⼚⾯试题共121题（含答案及解析）.pdf

成. 15.6 bert 的改进版有哪些 参考答案： RoBERTa：更强⼤的 BERT 加⼤训练数据 16GB -> 160GB，更⼤的batch size，训练时间加⻓ 不需要 NSP Loss：

natural inference 使⽤更⻓的训练 SequenceStatic vs. Dynamic Masking 模型训练成本在 6 万美⾦以上（估算） ALBERT：参数更少的 BERT⼀个轻量级的

BERT 模型 共享层与层之间的参数 （减少模型参数）

出处 [2] 2022Q1⼤⼚⾯试题共65题（含答案及解析）.pdf

可以从预训练⽅法⻆度解答。

… 20

5、RoBERTa 相⽐ BERT 有哪些改进？

…

20 6、BERT 的输⼊有哪⼏种 Embedding？

出处 [3] 2022Q2⼤⼚⾯试题共92题（含答案及解析）.pdf

保证模型的训练，pre-norm 显然更好⼀些。 5、GPT 与 Bert 的区别 1） GPT

是单向模型，⽆法利⽤上下⽂信息，只能利⽤上⽂；⽽ BERT 是双向模型。 2） GPT 是基于⾃回归模型，可以应⽤在 NLU 和 NLG两⼤任务，⽽原⽣的 BERT

采⽤的基于⾃编码模 型，只能完成 NLU 任务，⽆法直接应⽤在⽂本⽣成上⾯。 6、如何加速 Bert模型的训练 BERT 基线模型的训练使⽤ Adam with weight

decay（Adam 优化器的变体）作为优化器，LAMB 是⼀款通⽤优化器，它适⽤于⼩批量和⼤批量，且除了学习率以外其他超参数均⽆需调整。LAMB 优化器⽀持⾃

```


可以看出，是没有检索到相关内容的

在没检索对的情况下，接下来，⼤模型便只能根据⾃⼰的知识去回答(下图左侧是chatglm2-6b的回答，下图右侧是chatglm3-6b的回答)

![image.png](https://gitee.com/hxc8/images10/raw/master/img/image.png)


结果就是造成了⼤模型所谓的编造或幻觉问题，没有答到点⼦上：MLM和NSP

### 1.1.2 可能的原因分析与优化⽅法

使⽤默认配置时，虽然上传⽂档可以实现基础的问答，但效果并不是最好的，通常需要考虑以下⼏点原因

1. ⽂件解析及预处理：对于PDF⽂件，可能出现解析不准确的情况，导致检索召回率低；

2. ⽂件切分：不同的chunk_size切分出来的粒度不⼀样。如果设置的粒度太⼩，会出现信息丢失的情况；如果设置的粒度太⼤，⼜可能会造成噪声太多，导致模型输出

的结果明显错误。且单纯根据chunk_size切分⽐较简单粗暴，需要根据数据进⾏针对性优化；

3. embedding 模型效果：embedding效果不好也会影响检索结果

**优化⽅法：**

- ⽂件解析及预处理

⼀⽅⾯可以尝试不同的PDF解析⼯具，解析更加准确

另⼀⽅⾯可以考虑将解析后的内容加上标题，并保存成Markdown格式，这样可以提⾼召回率

- ⽂件切分

基于策略：对于特定的⽂档，⽐如有标题的，可以优先根据标题和对应内容进⾏划分(就是按照题⽬和对应答案切分成⼀个块)，再考虑chunk_size

基于语义分割模型：还可以考虑使⽤语义分割模型

- 模型效果

尝试使⽤更多embedding模型，获得更精确的检索结果。如：piccolo-large-zh 或 bge-large-zh-v1.5等等，下⽂很快阐述

- 向量库

如果知识库⽐较庞⼤（⽂档数量多或⽂件较⼤），推荐使⽤pg向量数据库

如果⽂件中存在较多相似的内容，可以考虑分⻔别类存放数据，减少⽂件中冲突的内容

- 多路召回

结合传统⽅法进⾏多路召回

- 精排

对多路召回得到的结果进⾏精排
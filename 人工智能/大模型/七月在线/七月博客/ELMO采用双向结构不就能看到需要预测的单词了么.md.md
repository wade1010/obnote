ELMO采用双向结构不就能看到需要预测的单词了么，你都能看到参考答案了，那岂不影响最终模型训练的效率与准确性，因为哪有做题时看着参考答案做题的。不过，巧就巧在ELMO虽然采用的双向结构，但两个方向是彼此独立训练的，从而避免了这个问题！

好家伙，一般的文章可能面对这个问题就是一笔带过，但我July不行啊，咱得解释清楚啥叫“ELMO虽然采用的双向结构，但两个方向是彼此独立训练的”，啥叫既然是双向的，又何来什么独立，然后避免see itself的问题呢？
好问题！虽然ELMO用双向LSTM来做encoding，但是这两个方向的LSTM其实是分开训练的（看到上图中那两个虚线框框没，分开看左边的双层LSTM和右边的双层LSTM，一个从左向右预测，一个从右向左预测，但在左边和右边的内部结构里，其本质不还是单向么，所以其实就是个伪双向，^_^），只是在最后在loss层做了个简单相加

换言之，两个关键点

对于每个方向上的单词来说，因为两个方向彼此独立训练，故在一个方向被encoding的时候始终是看不到它另一侧的单词的，从而避免了see itself的问题
而再考虑到句子中有的单词的语义会同时依赖于它左右两侧的某些词，仅仅从单方向做encoding是不能描述清楚的，所以再来一个反向encoding，故称双向
看似完美！“伪双向”既解决了see itself的问题，又充分用上了上下文的语义

然，BERT的作者指出这种两个方向彼此独立训练即伪双向的情况下，即便双层的双向编码也可能没有发挥最好的效果，而且我们可能不仅需要真正的双向编码，还应该要加深网络的层数，但暂且不管加不加深，真正的双向编码网络还是会不得不面对这个老问题：导致模型最终可以间接地“窥探”到需要预测的词，即还是那个see itself的问题。

                        
原文链接：https://blog.csdn.net/v_JULY_v/article/details/127411638
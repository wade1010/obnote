### 背景介绍
	企业知识库特点
	RAG技术概述
	RAG的领域应用
### RAG整体优化方案介绍
	   构建索引优化
	   检索优化
	   向量与排序模型微调
### RAG趋势展望



##### 拿企业客服知识库举例：

客服人员流动大、知识培训周期长，导致企业内存在信息孤岛且沟通阻塞，面对繁多、复杂的客户问题，客服人员往往难以及时解决

信息孤岛，重复劳动，客户问题反复出现，缺少更新和处理记录，无法实现知识复用

知识匮乏，解决难度大：客服人员缺乏知识或不熟悉特定产品或服务，难以精准解决客户问题

客服效率低客户体验差：客户问题繁多，客服人员沟通成本高，无法即时解决客户问题

知识难共享，沟通困难：公司内部知识缺少系统性分享，导致信息孤岛和闭环沟通失败


##### 企业知识库构建意义：

帮助企业提高客服效率和满意度，减轻客服压力，降低客服成本，提升客户体验和口碑，还可以持续改进和优化企业服务流程和产品

##### RAG在企业知识库构建中的作用：

通过检索增强的技术，支持海量知识库文档的检索、整合与生成。


### RAG技术概述
RAG，全称为检索增强生成（Retrieval-Augmented Generation），是一种结合了检索机制和大语言模型的技术。它通过从一个长文档或文档集合中检索相关信息，然后将这些信息融合到大模型的生成过程中，以提高模型的回答质量和准确性。RAG技术在自然语言处理领域，尤其是在客服问答、对话系统和内容生成等任务中展现了优异的性能。

检索器：检索器的作用是在一个大型的知识库或文档集合中，根据输入的问题或提示，快速检索出相关的文档或信息片段。常见检索方式为向量相似度、相关性计算、BM25、关键词检索等


生成器：生成器接收来自检索器的信息和原始输入，然后生成回答或内容。生成器通常是一个大型语言模型，如ChatGPT、ChatGLM、LLAMA等，它能够综合考虑检索到的信息和原始问题，生成连贯、相关且实时的文本。

![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311353808.png)

#### RAG技术概述：特点与优势
##### 可扩展性
RAG模型可以通过扩展知识库的方式来不断增强其性能，而无需重新训练模型。这一特点使得RAG模型可以轻松适应新的数据和信息，保持其在各种任务中的竞争力。
##### 实时性
RAG模型在生成回答前进行实时检索，确保了输出内容的实时性和相关性。这对于需要最新信息的应用场景尤为重要。
##### 提升特定领域任务的性能
对于特定领域的任务，如医疗问答或法律咨询，RAG技术可以通过检索特定领域的知识库来显著提升模型的性能。且可以有效避免幻觉，限制输出范围。

### RAG技术概述：RAG与大模型微调的区别

大模型微调，指在一个预训练好的大型语言模型（如ChatGLM、LAMMA等）的基础上，通过在特定任务上的进一步训练（微调），使模型更好地适应该任务的过程。这种方法依赖于大量的标注数据，通过调整模型参数，使其在特定任务上表现更佳。

##### RAG与大模型微调的区别
- 数据来源： RAG依赖于外部知识库来增强其生成的内容，而大模型微调依赖于模型在预训练阶段学习到的知识和微调阶段的特定数据集。

- 适用场景: RAG特别适用于需要外部知识或信息的任务，如知识库问答。大模型微调则适用于广泛的NLP任务，如翻译、代码生成、风格学习等。

- 灵活性与更新方式: RAG可以通过更新外部知识库来实时获取最新的信息，更新知识方式为增加外部检索知识库；而大模型微调的知识更新则依赖模型训练，耗时较久，如今有技术可以提高模型的上下文长度，但是面对大容量知识库仍然面对无法把文本一次性都输入模型的情况。

- 计算资源: RAG在运行时需要进行实时检索与较多的提示词，可能需要更多的计算资源。大模型微调则主要在训练阶段需要大量计算资源，但一旦训练完成，运行成本较低。

### RAG的领域应用
![智慧芽专利检索问答应用](https://gitee.com/hxc8/images10/raw/master/img/202407311551407.png)

### RAG技术优化方案

![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311554217.png)
### RAG技术优化方案—构建索引优化

RAPTOR （Recursive Abstractive Processing for Tree-Organized Retrieval）迭代摘要聚类，核心思想是将doc构建为一棵树，然后逐层递归的查询。大多数现有方法仅从检索语料库中检索短的连续块，限制了对整个文档上下文的整体理解。

![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311557005.png)
构建树的过程，RAPTOR 根据其语义embedding递归地对文本块chunk进行聚类，并生成这些聚类的文本摘要。

根据向量递归地对文本块进行聚类，并生成这些聚类的文本摘要，从而自下而上构建一棵树。 聚集在一起的节点是兄弟节点； 父节点包含该集群的文本摘要。这种结构使 RAPTOR 能够将代表不同级别文本的上下文块加载到 LLM 的上下文中，以便它能够有效且高效地回答不同层面的问题。

树的聚类算法基于高斯混合模型 (GMM)，聚类后，每个聚类中的节点被发送到LLM进行概括。在实验中，作者使用 gpt-3.5-turbo 来生成摘要。摘要步骤将可能大量的检索信息压缩（summarization）到一个可控的大小。

### RAG技术优化方案汇总—构建索引优化
PAPTOR 迭代摘要聚类，处理文本：https://browse.arxiv.org/abs/2401.1805
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311558146.png)
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311558909.png)
查询有两种方法，基于树遍历（tree traversal）和折叠树（collapsed tree）

树遍历：从 RAPTOR 树的根层开始，然后逐层查询
折叠树：全部平铺，用ANN库查询。


### RAG技术优化方案—检索优化1
FILCO：Filter Context for Retrieval-Augmented Generation:https://arxiv.org/pdf/2311.08377.pdf
过滤器Filter:筛选出有利于LLM生成的内容，过掉不相关的检索信息

![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311600227.png)
过滤器是基于flan -t5微调得到。过滤器的输入是检索到的文章，输出是更细致的句子，句子的划分通过spacy完成。实际是完成一个粒度由粗到细的过程

FILCO过滤器的基座都是统一的，区别是训练集的不同，分别有三种策略去构造训练集

##### （1）StrInc：String Inclusion
当o（query 对应的answer）完整在一个片段t中，就把这个片段召回，只要第一个，这样就构成了<（q，P），t>一条训练数据。 StrInc比较简单直接，缺点是可能找不到满足条件t或者找到t仍然是噪声内容；

##### （2）CXMI，Conditional Cross-Mutual Information，
其定义一个度量指标，即：
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311601402.png)
其含义是对比在有没有检索内容t的支撑下， 
 Mgen生成o的概率；可这样理解：如果为1，表示t可有可无，如果大于1，表示t利于模型生成，若小于1，表示t带来了噪声，不利于模型生成。所以，利用该度量指标，可以选择fcxmi分数最大对应的片段作为最终的t，这样也可以构建出训练数据集。其优势为可以更有效那个片段是否有效；![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311602933.png)
##### （3）Lexical：Lexical Overlap
其意思将e=q+o放在一起进行分词，然后在将所有召回的片段也分词，最后看那个片段与e的词重叠最多，就将其作为训练数据的t


### RAG技术优化方案—检索优化2
self-DC（ Self Divide-and-Conquer ）自我分治解决复合问题:https://arxiv.org/pdf/2402.13514

![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311604251.png)
给定一个问题，首先得到大模型对该问题的置信度分数，然后可以相应地选择处理当前问题的方式。具体地，其将置信度评分分为[0，α-β]，(α-β，α+β)，[α+β，1]三个范围，其中α和β是超参数。因此，当置信度分数落在左侧([0，α−β])或右侧([α+β，1])时，可以分别调用检索-读取或生成-读取来回答。

此外，当遇到不确定或令人困惑的问题时，将问题分解成几个子问题，旨在减少不确定性。为了保证迭代不溢出，设置了几个终止条件:1)子问题数为1，即可能是单个问题；2)迭代深度的次数小于一个预定义的t，在这种情况下，简单地将当前子问题视为未知问题，然后调用retrieve-then-read。然后，将所有子问题的答案结合起来，以提示大模型获得原始问题的最终答案。


关于回复置信度的估计
使用两种类型的方法来提示LLM本身获得置信度分数来回答问题。
1)基于语言化的方法verbalize-based，直接指示llm在问题的答案之后输出从0到100的置信水平，置信水平表示确定性的程度，然后将置信度分数重新映射到区间[0,1]。
2)基于概率probability-based的方法，还利用概率信息来计算置信分数。首先使用几个单词提示llm生成答案，然后得到生成内容中第i个token的概率pi，根据如下公式获取序列中概率的平均值作为置信度分数。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311605739.png)
针对问题的不同操作

根据不同的置信度，设计了几个功能来完成组合性问答任务，包括几个模块

1)先生成后阅读generate-then-read:首先提示大模型从维基百科生成背景文档来回答给定的问题，然后要求大模型通过引用生成的段落来回答问题。

2)先检索后阅读retrieve-then-read:在第一步使用检索器检索外部知识，然后要求LLM根据检索到的文章来回答问题

3)问题分解decompose:提示大模型将总体问题分解为几个较小的子问题，这些子问题的答案共同有助于得出原始总体问题的答案，类似于Press等人(2023)和Xu等人(2023)，这种用法很常见，其实就是query-expansion的处理。


### RAG技术优化方案—向量与重排序模型微调

向量表示（embedding)、交互式模型（cross encoder）、 迟交互式模型（colbert） 。微调代码解析
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311611065.png)
### RAG2.0

RAG最初的发明者现任Contextual AI 的CEO Douwe Kiela等人对外发布了RAG 2.0。该方法将所有组件作为单个集成系统进行预训练、微调和对齐，通过语言模型和检索器进行反向传播以最大化性能
[Introducing RAG 2.0 - Contextual AI](https://contextual.ai/introducing-rag2/)

![image.png](https://gitee.com/hxc8/images10/raw/master/img/202407311613682.png)

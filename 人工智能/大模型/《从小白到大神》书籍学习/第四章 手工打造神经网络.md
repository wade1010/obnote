![](https://gitee.com/hxc8/images2/raw/master/img/202407172155511.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155681.jpg)

**4.2 人工神经网络的结构**

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155563.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155364.jpg)

将输出层的值与类别标签进行一个差值分析，以此误差作为一个目标函数，来反向传播，来更新输入层到隐藏层，隐藏层到输出层，他们之间的一个权值和偏置，以此来优化我们的模型，这就是神经网络的反向传播。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155180.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155283.jpg)

在反向传播的过程中，首先计算输出值与类别标签的误差，并且将这个误差作为目标函数，它优化的目标使得误差达到一个最小值，我们将总的误差分摊到各个神经元中，通过链式法则来计算每一个神经元中对应的权重，来调整权重和偏置的大小，以此来寻找它的一个最优解，使 模型达到一个最优状态。

第一步就是计算总误差，以此，作为一个目标函数；

第二步更新输入层到隐藏层、隐藏层到隐藏层，隐藏层到输出层，它们的一个权值更新。

最后通过不断地循环和迭代，以此来达到误差最小。

**4.3 梯度消失与梯度爆炸**

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155048.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155024.jpg)

多层前馈神经网络，但是在训练过程中仍然存在一定的问题，而所面临的问题就是梯度消失，或者梯度爆炸，也就是神经元在梯度上变得非常小或者非常大，从而加大训练的难度。

神经网络在传播的过程中，以输出值和真实值之间的误差来作为目标函数，但是误差从输出层反向传播时，在每一层都要乘以每一层的激活函数的倒数。所以先介绍下每一种激活函数以及它们的导数。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155070.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155896.jpg)

也叫梯度弥散，主要出现在反向传播过程中，误差从输出层反向传播时，在每一层都会乘以该层的激活函数的导数，如果这个导数的值小于1，比如Sigmoid在两端时，它的一个饱和区域，值趋向于0，那么神经元就会在梯度上不断地衰减，甚至于消失。

梯度消失主要出现在两种情况下，一种是深层的网络，一种是采用不合适的损失函数。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155728.jpg)

梯度爆炸是在模型初始化时，将它的一个权重w初始化较大，并且在每一层激活函数的导数都大于1的情况下，随着层数的增加，求出的梯度就会以指数的形式进行增加，从而发生梯度爆炸。

梯度爆炸一般出现在深层网络和权值w初始化比较大的情况下。

**4.4 损失函数**

在正向传播的过程中，人工神经网络将每一个输入与其对应的权重进行相乘，加上偏置，最后经过一个激活函数，得到转换，得到我们最终想要输出的一个结果。

而在反向传播的过程中，它首先是计算输出值与类别标签的一个误差值，并将这个误差作为我们的目标函数，优化的目标使其达到一个最小值，将总的误差分摊到各个神经元当中，然后通过链式法则改变每一个神经元中对应的一个权重和偏置，通过调整权重和偏置的大小，以此达到最优解，使得我们模型的一个误差达到最小

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155407.jpg)

![](images/WEBRESOURCEdd1854a824b19d16b88ad7c384529447截图.png)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172155160.jpg)
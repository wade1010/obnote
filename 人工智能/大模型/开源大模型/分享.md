# 一、申请试用阿里云服务器

页面网址[https://free.aliyun.com/?product=9602825&crowd=personal](https://free.aliyun.com/?product=9602825&crowd=personal)

只要没有申请过PAI-DSW资源的新老用户皆可申请5000CU的免费额度，3个月内使用。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159068.jpg)

申请试用成功后

创建实例

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159273.jpg)

进入创建页面资源组就选择默认的吧，大概能用13天

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159562.jpg)

进项选择这里不要用默认的，默认的是python3.6比较旧了，现在好多开源项目都是python3.9+，建议选择最后一个

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159392.jpg)

安装好（大概需要15分钟左右）进入实例

[https://pai.console.aliyun.com/?regionId=cn-hangzhou&workspaceId=75930#/notebook](https://pai.console.aliyun.com/?regionId=cn-hangzhou&workspaceId=75930#/notebook)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172159586.jpg)

进入终端，然后就可以进入实操步骤

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200227.jpg)

# 二、ChatGLM6B操作记录

## 1、下载模型

由于模型较大（13G左右），我们最好先把模型拉到本地，再运行（也可以让代码运行时拉取模型，容易出问题）

模型文件仓库[https://huggingface.co/THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200510.jpg)

下载模型仓库，需要安装Git LFS（Large File Storage），它用来拉去Git仓库中的大型文件

### 1.1安装LFS   

```
sudo apt-get install git-lfs
```

### 1.2拉取模型

git clone [https://huggingface.co/THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)

#### 1.2.1 小技巧

有可能网络波动，会导致拉取卡住，可以手动停止掉，然后进入文件夹内：

```
git lfs pull
```

## 2、源码

### 2.1下载源码

git clone [https://github.com/THUDM/ChatGLM-6B.git](https://github.com/THUDM/ChatGLM-6B.git)

### 2.2创建Python虚拟环境

cd ChatGLM-6B

virtualenv -p python3 venv    (如果提示没有安装virtualenv，pip3 install virtualenv 安装下)

### 2.3激活Python虚拟环境

source venv/bin/activate

### 2.4安装依赖

这里我们需要用到gradio进行公网访问，在requirements.txt加上这个（也可以单独安装）

在 requirements.txt末尾加上gradio

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200806.jpg)

 

```

Could not create share link. Missing file: /home/cheng/miniconda3/envs/VisualGLM/lib/python3.10/site-packages/gradio/frpc_linux_amd64_v0.2.

Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps:

1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64
2. Rename the downloaded file to: frpc_linux_amd64_v0.2
3. Move the file to this location: /home/cheng/miniconda3/envs/VisualGLM/lib/python3.10/site-packages/gradio
```

pip install -r requirements.txt

如果下载较慢可以尝试

pip install -r requirements.txt -i [https://pypi.doubanio.com/simple](https://pypi.doubanio.com/simple)

## 3、部署模型

由于我们本地提前下载好了模型，使用本地模型需要修改下源码

使用模型的方法主要有两种,cli和web页面，主要是下面两个文件

### 3.1 修改源码

cli_demo.py

web_domo.py

修改其中的模型加载路径

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200604.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200264.jpg)

启用gradio公网访问，所有网络会经过 Gradio 服务器转发，导致打字机体验大幅下降，不分享给别人就不要打开

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200571.jpg)

### 3.2启动web界面

```
python web_demo.py
```

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200030.jpg)

点击上图链接就可以自己访问进行测试了（没启用gradio的时候只有127.0.0.1的链接）

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200183.jpg)

### 3.3命令行启动

```
python cli_demo.py
```

程序会在命令行中进行交互式的对话，在命令行中输入指示并回车即可生成回复，

输入 clear 可以清空对话历史，

输入 stop 终止程序。

### 3.4API部署

首先需要安装额外的依赖 pip install fastapi uvicorn，然后运行仓库中的 api.py：

```
python api.py
```

默认部署在本地的 8000 端口，通过 POST 方法进行调用

```
curl -X POST "http://127.0.0.1:8000" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "你好", "history": []}'
```

得到的返回值为

```
{
  "response":"你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。",
  "history":[["你好","你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。"]],
  "status":200,
  "time":"2023-03-23 21:38:40"
}
```

## 4、基于 P-tuning v2 的高效参数微调

### 4.1** P-tuning v2简介**

P-Tuning是一种较新的模型微调方法，它采用了参数剪枝的技术，可以将微调的参数量减少到原来的0.1%。具体来说，P-Tuning v2是基于P-Tuning v1的升级版，主要的改进在于采用了更加高效的剪枝方法，可以进一步减少模型微调的参数量。

P-Tuning v2的原理是通过对已训练好的大型语言模型进行参数剪枝，得到一个更加小巧、效率更高的轻量级模型。具体地，P-Tuning v2首先使用一种自适应的剪枝策略，对大型语言模型中的参数进行裁剪，去除其中不必要的冗余参数。然后，对于被剪枝的参数，P-Tuning v2使用了一种特殊的压缩方法，能够更加有效地压缩参数大小，并显著减少模型微调的总参数量。

总的来说，P-Tuning v2的核心思想是让模型变得更加轻便、更加高效，同时尽可能地保持模型的性能不受影响。这不仅可以加快模型的训练和推理速度，还可以减少模型在使用过程中的内存和计算资源消耗，让模型更适用于各种实际应用场景中。

### 4.2安装软件依赖

```
pip install rouge_chinese nltk jieba datasets
```

对于需要pip安装失败的依赖，可以采用源码安装的方式，具体步骤如下

```
git clone https://github.com/huggingface/peft.git
cd peft
pip install -e .
```

### 4.3下载数据集

项目根目录执行

```
AdvertiseGen.tar.gz
wget https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1
tar zxf AdvertiseGen.tar.gz
```

官方微调样例是以 ADGEN (广告生成) 数据集为例来介绍微调的具体使用。

ADGEN 数据集为根据输入（content）生成一段广告词（summary），具体格式如下所示：

```
{
    "content": "类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳",
    "summary": "这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。"
}
```

查看数据集大小：

```
> wc -l AdvertiseGen/*
> 1070 AdvertiseGen/dev.json
> 114599 AdvertiseGen/train.json
> 115669 total
```

### 4.4训练

修改train.sh

![](images/WEBRESOURCEd297161a9d64ca651416a41af64e15b5截图.png)

将上图model_name_or_path改为你下载到本地的模型路径，跟之前部署模型修改一样

执行bash train.sh脚本

免费试用的这个服务器大概需要4个小时训练完

> 除了P-Tuning v2还有全参数的Finetune训练方法，这个目前的机器暂时不能做


## 5、模型评估

### 5.1修改源码里面的路径

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200967.jpg)

将上图model_name_or_path改为你下载到本地的模型路径，跟之前部署模型修改一样

### 5.2运行代码

```
bash evaluate.sh
```

大概需要3个多小时

页面输出大概如下

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200918.jpg)

## 6、微调后运行模型

### 6.1修改源码

![](https://gitee.com/hxc8/images2/raw/master/img/202407172200236.jpg)

6.2运行代码

```
bash web_demo.sh
```

经过P-tuning之后大模型参数变成原来的0.1%,所以微调后，应该基于微调的数据集进行提问，其它问题也只会基于数据集的内容回复

2023-6-29日结束
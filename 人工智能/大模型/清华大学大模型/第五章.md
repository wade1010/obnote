## 数据并行

![image.png](https://gitee.com/hxc8/images9/raw/master/img/202408031220852.png)
上图，把数据切为3份，每张显卡处理一部分数据，每张显卡利用得到的数据进行前向传播和反向传播，得到各自的梯度，为了让模型学到这份数据的所有知识，就需要把这些梯度的信息进行一个聚合，也就是取平均的操作。那么我们得到聚合好的参数去更新模型，就能学到切分3部分数据合起来的完整的数据的知识。
具体来说：在数据并行的过程中，我们有一个参数服务器，里面保存了模型的参数，还有完整的一批数据，前向传播的过程中，参数服务器上的参数，会被复制到所有的显卡上，每张显卡上都得到了很参数服务器上一样的模型参数，然后把数据切分为3份，每张显卡上各拿到一部分数据，然后每张显卡用完整的模型参数和一部分的数据去进行前向传播和反向传播，我们就能够得到每张显卡上各自的梯度，最后将这个梯度进行聚合，将聚合后的梯度传回我们的参数服务器。那么参数服务器上面有了原始的模型参数和这个聚合好的模型的完整的梯度，我们就可以用优化器去对模型的参数进行更新，那么更新后的参数又会进入下一轮的模型的训练迭代

### 集合通信

1. broadcast
   把数据从一张显卡传到其它所有的显卡上
2. reduce
   规约可以是求和、平均、MAX等，把各张显卡上的数据进行一个规约，然后把规约得到的结果，放到我们其中一张指定的显卡里面。 
3. all reduce
   和reduce几乎相同，不同点就是把结果发送到所有显卡上面（reduce是发送到指定的一张显卡上）
4. reduce scatter
   跟all reduce相比，相同之处是它们都把规约得到的结果发给所有显卡，不同之处在于，reduce scatter，最后每张显卡上只得到一部分的规约结果。![image.png](https://gitee.com/hxc8/images9/raw/master/img/202408031236427.png)
如上图，0号显卡，会得到in0的前1/4的参数，加上in1的前1/4的参数，加上in2的前1/4的参数，加上in3的前1/4的参数，得到out0。其他同理。
5. all gather
   可以跟all reduce类比下，把各张显卡上的数据进行收集，然后进行一个拼接，然后广播到所有显卡上，所有显卡得到了一个搜集后的结果。![image.png](https://gitee.com/hxc8/images9/raw/master/img/202408031244132.png)
## 分布式数据并行
对数据并行进行了优化，没有参数服务器。
每张显卡各自完成参数更新，
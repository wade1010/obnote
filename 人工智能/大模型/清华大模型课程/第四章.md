
## prompt learning

![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408021433716.png)
上图左边是一个掩码遮蔽的预训练任务，对于一个数据，我们会把其中一些词MASK掉，然后使用这个MLM HEAD去预测MASK掉了什么词。
然后对于fine-tune来说（上图右边），对于一个输入的句子，把它输入到预训练好的模型（上图Encoder）中，然后通过一个额外的随机初始化的一个分类器（Task Head），然后去训练它，让它输出是positive还是negative。
可以看出其实pre-training和fine-tuning之间好像有一点点gap，它们做的其实并不是一件事。因为我们在预训练时其实并不知道还要做后面的分类，其实我们做的只是去预测这个mask的位置的token而已。
在fine-tuning中，我们其实就是没有去预测任何mask的token，就是去建一个新的分类层，然后让它去预测。所以这种gap，我们就可以很显然地去用prompt区给它弥补它。

下面就是做法。![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408021455114.png)
上图右边。对于一个输入的instance，我们给它新加一句话，it was [MASK],就是给它加一个prompt，我们给它包装成一个也是和预训练任务一样的形式，比如说预训练我们用mask language model，就是完形填空，这里我们也用一个完形填空，我们也让这个模型去预测，这个mask位置的token。这时候它会预测一个什么东西呢？它会预测一个和预训练中一样的东西，就是说在整个词表上的分布，然后根据它在整个词表上的分布，我们只需要抽取其中我们想要的那些词。比如这是一个情感分类，那么可能就是一个正类和负类，比如对于positive这个类，得到预测结果可能个就是good，wonderful，对于negative类，可以是bad，terrible等。我们得到整个词表上的概率分布后，只需要去比较，positive类的概率和negative类的概率，它们两个谁大，其它的词我们都不需要，我们只需要比较这两个谁大。我们就可以去断定这个东西被分到了哪一类。
这里额外加的上下文，我们叫做template
然后把这个标签给映射到标签词的映射器叫做verbalizer，可以结合下图理解
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408021513851.png)

## template
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408021535606.png)
 比如实体关系的任务，比如上面，现在我们知道伦敦它是世界上最大的城市之一，然后这时候，我们要怎么做呢？我们假如给它加一个prompt template，我们可以把这个London给复制到这个template里面去，然后就直接问它，它到底是什么类别，这样的话，我们对于每一个输入，我们开头的这个template都是变化的，因为它是不同的实体，但是我们要问的东西嗯，就是它是一个什么样的实体类别，比如是city还是location，还是person，还是organization。这样的话我们就可以趣味完成这个实体分类，从而达到我们去抽取这个所谓的世界知识的效果。通过这种做法，其实可以在那个少样本，甚至零样本上表现的特别好。实际上这样的话，我们就可以完成一种类似于下游任务和预训练任务之间语义之间的一个语义的对齐，就是让它去想起来，伦敦好像是一个城市。 就让它想起来哪些在预训练过程中，它所碰到的pattern，应该是什么样的。
# 信息检索(IR)
## 传统方法
### BM25 (Best Matching 25)
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051408703.png)
### TF(Term Frequency)
词频，就是query中每个词，在这个文档中出现的频率，就是一个简单的统计。
如果一篇文档中 ，它这个词汇匹配率，与这个查询的词汇匹配率越高的话，就可以认为这篇文档与这个查询的相关程度越高。

### IDF（Inverse Document Frequency）
你文档频率，用于评估查询中一个词汇在所有文档中常见或者稀有程度，比如一根词在所有文档中都很常见，它的IDF打分反而会很低。如果IDF分数高的话，反向说明这个查询词，可以它包含的信息比较大，也更重要。
## 传统IR存在的问题
### 1、词汇失配
我们会用不同的词汇表达相同的意思。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051421713.png)

### 2、语义失配
即文档跟我们查询之间即使存在很高的词汇匹配率，但描述的含义却完全不一样。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051421330.png)
## Neural IR
使用神经网络，将用户的查询和文档库中的文档投射到同一个向量空间，然后再去预测两则相关性的分数，从而避免了传统IR中词汇失配合语义失配的问题。

### 基于大模型的IR架构
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051431109.png)
### Cross-Encoder
通常会在re-ranking的阶段采用cross-encoder的大模型架构（上图左下角），它会在一开的时候，将query和document进行词汇级别的拼接，然后一起未入这个大模型，然后让它惊醒一个精细地交互式地建模，接着生成一个q d(紫色部分)的共同表示。最后再产生他们的相关性分数。
好处：比较精细，达到的检索性能也比较好。
缺点：计算代价比较高。所以通常是召回之后的第二阶段使用。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051440870.png)
上图步骤，给定一个query和document，这个cross-encoder会先把他们进行拼接，然后再一起喂入大模型，以bert为例，拼接完之后的sequence会经过一个多层transformer的建模之后，会把我们最后一层的CLS token，最为q d的共同表示。在经过NLP的投射，变成标量分数。
在训练这个大模型的时候，训练数据的形式就是每个query要配备一个相关文档与至少一篇的不相关文档，然后采用一些常见的ranking loss，比如上图的pairwise hinge loss,为相关文档和query分配一个更高的分数。 
### Dual-Encoder
retrieval阶段一般会采用右边dual-encoder的双塔架构。就是使用大模型对query和document分别进行编码，形成两个独立的向量，然后再去计算向量间的相似性。这样的话，可以极大的减小计算的开销。
以DPR为例，DPR是使用了2个独立的encoder，去分别对query和document进行编码。
好处：分开编码可以对整个文档库提前全部都编码好，把它向量存起来，这样用户新的query进来之后，只需要在线地编码用户的query，然后再使用一些近邻搜索的工具（KNN等）去找出query的vector与他最相近的k个文档即可。
Faiss(Facebook AI Similarity Search) 能做到1毫秒就能搜索ten million篇documents。

## 前沿研究热点
- fine-tune阶段挖掘更好的负例
    - in-batch negative
    - random negative
    - BM25 negative
    - self-retrieved hard negative
- IR-oriented Pretraining
    - SEED-Encoder
    - ICT(Inverse Cloze Task)
    - Few-Shot IR
    - Zero-Shot IR
## 其它话题
- conversational IR
- how to use big model to retrieve long documents?


# 机器问答
1、阅读理解相关任务
2、开放域的问答 
3、基于知识的问答
4、对话式的QA系统

# 文本生成
## 解码
### 1、Greedy decoding
选择计算后概率最大的token。好处：生成的都是模型认为最相关的token，缺点：生成重复的内容。

### 2、Beam Search
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051558797.png)

beam size设置为1就退化为greedy decoding
比较大的beam size，需要计算更多种可能，耗费更多的计算资源，但是增大beam size并不一定让模型表现得更好，同样也会出现一些其它的问题，比如机器翻译中，会发现，这个k增大得太大的话，会降低BLUE score；也可能会出现生成跟我们input text无关的语句（因为生成的可能性变多了  ）![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051604700.png)

### 3、Sampling-based method
以上两种做法，都会在每一步中选择一个概率最大的token，其实是不必要去选择这个最大概率的token。下面介绍下sampling-based decoding方法。
- Pure sampling
  在每一步的时间t，我们不会直接地选择概率最大的token，而是按照模型计算出的词概率分布，对于概率比较大的词，我们会议较大的概率去选它，而对于概率比较小的词，我们会议较小的概率去选它。我们会随机的从词表中选择一个token，这样的话，模型生成的多样性就会大大增加。但是也可能会出现一些概率比较小的词，这些词其实是与模型输入无关的。为了避免这样大量无意义词的出现，又采取了top-n和top-p的两种方法（下面会讲解）去限制模型生成的范围。
- Top-n sampling
  采样的过程中不会从整个词表上进行采样，而是仅仅局限于在n个最优概率的词上面进行采样.n=1就是greedy search，n=v(v表示词表大小)就是pure sampling。
- Nucleus sampling
  也是对采样范围进行限制的方法，与top-n不同的是，它会限制采样是在若干个token上进行的，这些token需要满足：首先是概率最大的token，这些概率加起来大于这个概率阈值p。如果p=1就是pure sampling。
  - sample with temperature 
    在最后运用softmax之前，会把送入softmax之前的概率去除以一个temperature，如下图，中间的temperature是1，也就是我们不做这个操作的花，概率分布的原始模样。
    ![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051618478.png)
如果我们去除以2.0的话（上图右边），这些概率分布会变得平均。
如果我们去除以一个temperature等于0.5的话，这个概率会变得稀疏起来（上图左边）。
也就是说在进行采样的时候，我们去选择temperature等于2的话，它就会以更加平均的概率去摘这些词中进行采样，因此会生成更加多样性，但是也可能跟文本更不相关的内容。那如果我们选择0.5，它就会生成一个单一性比较高，但是跟文章相关度比较高的一些内容。
### 总结
- Greedy decoding
    - 简单的方法
    - 生成质量比较低
- Beam Search
    - 提供比Greedy decoding更好的质量
    - 如果beam size过高，会返回跟输入无关的输出(例如，通用、短)
- sampling methods
    - 获得更多的多样性和随机性
    - 对开放/有创造力的一代人有好处(诗歌、故事)
    - Top-n/p/温度采样允许控制多样性

## 受控文本生成
1、通过prompt
- 在文本前面加入控制信号，如下图红色开头的词
  ![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051630800.png)

- 在模型前面加一个prefix（prefix-tuning）
2、修改概率分布
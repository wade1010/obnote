# 信息检索(IR)
## 传统方法
### BM25 (Best Matching 25)
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051408703.png)
### TF(Term Frequency)
词频，就是query中每个词，在这个文档中出现的频率，就是一个简单的统计。
如果一篇文档中 ，它这个词汇匹配率，与这个查询的词汇匹配率越高的话，就可以认为这篇文档与这个查询的相关程度越高。

### IDF（Inverse Document Frequency）
你文档频率，用于评估查询中一个词汇在所有文档中常见或者稀有程度，比如一根词在所有文档中都很常见，它的IDF打分反而会很低。如果IDF分数高的话，反向说明这个查询词，可以它包含的信息比较大，也更重要。
## 传统IR存在的问题
### 1、词汇失配
我们会用不同的词汇表达相同的意思。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051421713.png)

### 2、语义失配
即文档跟我们查询之间即使存在很高的词汇匹配率，但描述的含义却完全不一样。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051421330.png)
## Neural IR
使用神经网络，将用户的查询和文档库中的文档投射到同一个向量空间，然后再去预测两则相关性的分数，从而避免了传统IR中词汇失配合语义失配的问题。

### 基于大模型的IR架构
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051431109.png)
### Cross-Encoder
通常会在re-ranking的阶段采用cross-encoder的大模型架构（上图左下角），它会在一开的时候，将query和document进行词汇级别的拼接，然后一起未入这个大模型，然后让它惊醒一个精细地交互式地建模，接着生成一个q d(紫色部分)的共同表示。最后再产生他们的相关性分数。
好处：比较精细，达到的检索性能也比较好。
缺点：计算代价比较高。所以通常是召回之后的第二阶段使用。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051440870.png)
上图步骤，给定一个query和document，这个cross-encoder会先把他们进行拼接，然后再一起喂入大模型，以bert为例，拼接完之后的sequence会经过一个多层transformer的建模之后，会把我们最后
### Dual-Encoder
retrieval阶段一般会采用右边dual-encoder的双塔架构。就是使用大模型对query和document分别进行编码，形成两个独立的向量，然后再去计算向量间的相似性。这样的话，可以极大的减小计算的开销。
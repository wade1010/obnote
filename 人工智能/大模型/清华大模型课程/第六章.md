# 信息检索(IR)
## 传统方法
### BM25 (Best Matching 25)
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051408703.png)
### TF(Term Frequency)
词频，就是query中每个词，在这个文档中出现的频率，就是一个简单的统计。
如果一篇文档中 ，它这个词汇匹配率，与这个查询的词汇匹配率越高的话，就可以认为这篇文档与这个查询的相关程度越高。

### IDF（Inverse Document Frequency）
你文档频率，用于评估查询中一个词汇在所有文档中常见或者稀有程度，比如一根词在所有文档中都很常见，它的IDF打分反而会很低。如果IDF分数高的话，反向说明这个查询词，可以它包含的信息比较大，也更重要。
## 传统IR存在的问题
### 1、词汇失配
我们会用不同的词汇表达相同的意思。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051421713.png)

### 2、语义失配
即文档跟我们查询之间即使存在很高的词汇匹配率，但描述的含义却完全不一样。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051421330.png)
## Neural IR
使用神经网络，将用户的查询和文档库中的文档投射到同一个向量空间，然后再去预测两则相关性的分数，从而避免了传统IR中词汇失配合语义失配的问题。

### 基于大模型的IR架构
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051431109.png)
### Cross-Encoder
通常会在re-ranking的阶段采用cross-encoder的大模型架构（上图左下角），它会在一开的时候，将query和document进行词汇级别的拼接，然后一起未入这个大模型，然后让它惊醒一个精细地交互式地建模，接着生成一个q d(紫色部分)的共同表示。最后再产生他们的相关性分数。
好处：比较精细，达到的检索性能也比较好。
缺点：计算代价比较高。所以通常是召回之后的第二阶段使用。
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051440870.png)
上图步骤，给定一个query和document，这个cross-encoder会先把他们进行拼接，然后再一起喂入大模型，以bert为例，拼接完之后的sequence会经过一个多层transformer的建模之后，会把我们最后一层的CLS token，最为q d的共同表示。在经过NLP的投射，变成标量分数。
在训练这个大模型的时候，训练数据的形式就是每个query要配备一个相关文档与至少一篇的不相关文档，然后采用一些常见的ranking loss，比如上图的pairwise hinge loss,为相关文档和query分配一个更高的分数。 
### Dual-Encoder
retrieval阶段一般会采用右边dual-encoder的双塔架构。就是使用大模型对query和document分别进行编码，形成两个独立的向量，然后再去计算向量间的相似性。这样的话，可以极大的减小计算的开销。
以DPR为例，DPR是使用了2个独立的encoder，去分别对query和document进行编码。
好处：分开编码可以对整个文档库提前全部都编码好，把它向量存起来，这样用户新的query进来之后，只需要在线地编码用户的query，然后再使用一些近邻搜索的工具（KNN等）去找出query的vector与他最相近的k个文档即可。
Faiss(Facebook AI Similarity Search) 能做到1毫秒就能搜索ten million篇documents。

## 前沿研究热点
- fine-tune阶段挖掘更好的负例
    - in-batch negative
    - random negative
    - BM25 negative
    - self-retrieved hard negative
- IR-oriented Pretraining
    - SEED-Encoder
    - ICT(Inverse Cloze Task)
    - Few-Shot IR
    - Zero-Shot IR
## 其它话题
- conversational IR
- how to use big model to retrieve long documents?


# 机器问答
1、阅读理解相关任务
2、开放域的问答 
3、基于知识的问答
4、对话式的QA系统

# 文本生成
## 解码
### 1、Greedy decoding
选择计算后概率最大的token。好处：生成的都是模型认为最相关的token，缺点：生成重复的内容。

### 2、Beam Search
![image.png](https://gitee.com/hxc8/images10/raw/master/img/202408051558797.png)

beam size设置为1就退化为greedy decoding
比较大的beam size，需要计算更多种可能，耗费更多的计算资源，但是增大beam size并不一定让模型表现得更好，同样也会出现一些其它的问题，比如机器翻译中，会发现，这个k增大得太大的话，会降低BLUE score；也可能会出现生成跟我们input text无关的语句（因为生成的可能性变多了）
### 3、Sampling method
- Pure sampling
- Top-n sampling
- Nucleus sampling
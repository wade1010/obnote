把模型的转换、图的优化、模型的压缩包括端云协同的一些学习，都建立在一个统一的IR里面。

中间通过一个统一的IR进行一个串通

![](https://gitee.com/hxc8/images1/raw/master/img/202407172117094.jpg)

在Runtime部分，大部分去用的时候都是runtime去用的，但是我们会将很多不同的算子，那这些算子都是在kernel层里面去承载的，而kernel层你可以看到里面有非常多的内容。

kernel层主要是高性能算子层，接着kernel层会做很多新的事情，第一个需要对这些算子kernel进行优化，然后执行这些算子kernel，还有对这些算子kernel进行调度，它主要作用是在这个上面

kernel又分两部分，人工高性能算子和高性能算子库，像在x86或者arm的cpu里面，大部分像NEON指令集呢基本上都会用NEON来去实现，而一些在X86里面，可能会使用一些AVX的指令去实现算子，在GPU里面会使用CUDA、openCL、Vulkan、metal去实现一些人工定义的高性能算子。至于在一些MPU里面呢，可能在华为昇腾会用TIK，还有在一些边缘推理芯片里面也会用到TVM去生成一些算子。这就是高性能的人工算子库，大部分都是我们先写好一个人工定义的算子，然后去进行一个极致的优化，优化完之后呢，其实有很多类型的算子，可以把它封装起来，变成例如cuDNN、MKLDNN高性能算子库，给runtime去调度的，当然 runtime也可以直接调人工实现的算子，具体怎么调用就要看runtime的策略了。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172117422.jpg)

它不仅只有engine这个引擎（上图右边），它还包括离线模块。而离线模块是把训练框架的网络模型转成自己的一个推理的模块， 那这个推理模块会经过模型压缩，也可以不经过模型压缩，然后给到离线模块，经过一些编译的优化或者图优化，优化完之后呢，就真正的在线执行，而在线执行的这个执行推理引擎这里面就叫runtime，是把我们一些算子调度起来，而后面执行的就是算子，也就是我们的kernel层。 

对一个简单的卷积算子，在CPU里面可能会使用NEON指令集去实现，也可能会使用X86的AVX指令集去实现。

在一个推理引擎里面的实现方式就有非常多种，因为我们推理引擎要支持CPU，也要支持GPU

而CPU就有两种不同的实现，可能在GPU上面就有更多种不同的实现，可能会用CUDA实现，用openCL，用Vulkan，可能还会用openGL,还有metal去实现，最终实现完，我们会把它封装成一个高性能算子库，也可能直接提供kernel层.

![](https://gitee.com/hxc8/images1/raw/master/img/202407172117477.jpg)

上图外面三层for循环，OH就是对NHWC里面的h这个通道进行遍历，然后对W这个通道进行遍历，然后对C这个通道进行遍历，最后才拿到我们第一个数据C[oh][ow][oc],然后把它赋一个值，然后有三个内层的for，就是卷积核了，这个卷积核就是kernel的h  kernel的w,还有input chanel，通过这种方式对他进行一个组织，然后就是求和公式里面进行一个计算。具体就是这里面卷积核跟这个原始图片进行加权求和，然后赋值

可以看出非常多的嵌套循环。

![](https://gitee.com/hxc8/images1/raw/master/img/202407172117668.jpg)

为了让整个嵌套循环没那么深，我们会做很多循环优化，

循环的展开、分块、重拍、融合、拆分（之前AI编译器说过）

img2col
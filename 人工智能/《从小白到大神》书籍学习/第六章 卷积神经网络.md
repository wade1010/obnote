### 6.1 卷积神经网络概述

![](https://gitee.com/hxc8/images2/raw/master/img/202407172153881.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172153155.jpg)

输入图像进入特征提取器，通过卷积、池化等操作，逐渐获得高维特征图，最后将高维特征图扁平化输入全连接层，最终得到分类结果。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172153719.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154549.jpg)

什么叫局部感受野，就是每个神经元只需对上一层图像的局部区域进行感知，而不需要对全局图像进行感知，即每一个神经元只需要负责图像的固定的一块地方。

权值共享是指，所有与图像进行局部连接的神经元都使用同一组参数。而再也不像传统的全连接神经网络一样，每哥像素点都要与神经元相连，大大减少了参数量。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154142.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154418.jpg)

上图是动态图，截图没法弄。

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154007.jpg)

激活函数的主要作用是提供网络的非线性建模能力，没有激活函数的话，网络仅能表达线性映射，此时即便有再多的层，整个网络跟单层网络也是等价的。只有加入了激活函数之后，深度神经网络才具备了分层、非线性映射学习的能力。

sigmoid和tanh是曾经训练神经网络模型的常用方法，但是通过Alexnet实践表明，使用relu非线性函数，我们训练深度神经网络时比使用tanh和sigmoid饱和激活函数要快得多，因为relu函数大于0的部分，斜率为1，相对于tanh或sigmoid在末端越来越趋近于0的斜率，收敛速度会显得更快。

此外还有诸多改进型，如maxout、leaky relu、elu

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154661.jpg)

本质作用是压缩特征图，过程就是选取最具有代表性的像素替代一个区域的像素值，这样做的好处是，能够有效的减少特征簇的参数量，减少计算量，在保证特征信息的情况下，缩小特征图尺寸。

上图方法左斜最大值

### 6.2 卷积神经网络结构

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154264.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154307.jpg)

利用归一化来取消量高，使梯度始终朝着最小的方向前进，少走弯路，加速收敛。

如果样本x的一个特征A，在10000到100000万这一区间内，而特征B在0.1到1这一区间，在训练神经网络的过程中会导致特征B的作用非常小，A的作用被过分放大，这不是我们想看到的。

求偏导时，A的参数变化太大而B的参数变化过小，这样会导致Loss在最小化的过程中不停地走弯路，浪费了训练时间

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154454.jpg)

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154882.jpg)

更新其实就是就el求w的偏导乘上学习率，然后用原来的权值w减去前面得到的值，最终得到新的权值

![](https://gitee.com/hxc8/images2/raw/master/img/202407172154052.jpg)

优点：通过矩阵计算，每次在一个batch上优化神经网络参数，并不会比单个数据慢太多，而每使用一个batch，可以大大减少收敛所需要的迭代次数，可以使收敛得到的结果更接近梯度下降的效果，同时可以实现一个并行化的效果。

缺点在于batch size选择不当的话，带来一些训练上的麻烦。
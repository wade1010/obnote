![](https://gitee.com/hxc8/images0/raw/master/img/202407172039015.jpg)

我们其实每一个词最终都可以变成一个向量，但是变成这个向量，不可能让它无限多，如果每一个词都是一个向量的花，这个也很夸张。因为它其实理论上可以有无限多个token出现的。这个token就是指变成这个向量之后给它标的标签。

那怎么办呢？比较好的方式还是回去做一个词表，假设我们要把中文的所有书籍全部都编导一个向量空间里，然后需要做什么步骤呢？

1、分词，把这个词分出来，就像英语，也需要做分词，只不过英语可能一开始空格就分好了；

2、分完词，很多词的意思其实是一样的，可以把它们变成同样的token，这个是有可能的，如果你做的粗糙点，那就不去做这个处理，它们就是不一样的token。

无论如何最终可以编成一个词表。在我这个向量空间里，想通的向量有一个统一的ID，然后这个ID其实就表示下面的这样一串。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039490.jpg)

上图101和102是特殊字符。

通过这个步骤，我们可以把原始的文本变成一个统一的向量空间，这个向量空间的维度可以有我们来定义，它可以是一个一开始人工设定的，也可以是一个超参数，我们来学出来的。然后再这个空间里面，每一个向量都有它特定语义了。而这样的一个向量，才是模型可以接受的输入。

tokenizer，第一步分词，第二步映射，把分出来的词映射到embedding的向量。这个向量在模型里面通过我们之前学过的不同的模型，有不同的网络结构，然后这个网络结构回去做一个前向的处理（前向传播），这个传播就会变成从x进过一堆的Y=WX，变成最终的Y，这个Y就是输出概率。

选出这个概率最终能变回一个词，怎么变回那个词，就是在向量空间里，哪个向量跟我现在输出这个概率的向量最近，可能就会去选它了。那这里就会有各种各样的一些技术手段。包括，基于相似度的，基于各种各样的手段，我们也可以看到像bert里面使用的这个top k，也是一种方式，但无论如何，不同的模型，有不同的后处理，但不同的模型，他们的前处理是有一个大体的框架可以被抽象出来的，那这个框架就是tokenizer，先分词再映射，变成一个embedding向量后处理，根据你的下游任务不同，会有一些不同的后处理手段。 

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039286.jpg)

下载的目录为 ~/.cache/huggingface/hub/models--bert-base-chinese

使用BERT Tokenizer编码文本

编码（Encoding）过程包含两个步骤：

- 分词：使用分词器按某种策略将文本切分为tokens；

- 映射：将tokens转化为对应的token IDs.

当你想要做这个预训练/微调的时候，下载的模型里面就已经实现好了一个tokenizer。通常是不用去改这个分词策略的，通常是把新的数据，用它的这个分词策略分词之后检验这个新的数据里面有没有一些token，不在它的词表里。

词表就是一个token id对应一个人类看得懂的自然语言，decode就是去读词表，然后把它映射回来。所以词表跟embedding的关系就是一个decode反映射回来的关系。

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039321.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039293.jpg)

除了刚刚的encode和decode去做这个模型层面上的操作以外，有一个更抽象的方法，就是tokenizer(xxx)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039731.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039580.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039914.jpg)

![](https://gitee.com/hxc8/images0/raw/master/img/202407172039986.jpg)
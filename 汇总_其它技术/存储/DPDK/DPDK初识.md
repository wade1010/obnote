利用内存大页HUGEPAGE降低TLB miss

（一）DPDK出现之前的问题：

以Linux 为例，传统网络设备驱动包处理的动作可以概括如下：

1. 数据包到达网卡设备。

1. 网卡设备依据配置进行 DMA 操作。

1. 网卡发送中断，唤醒处理器。

1. 驱动软件填充读写缓冲区数据结构。

1. 数据报文达到内核协议栈，进行高层处理。

1. 如果最终应用在用户态，数据从内核搬移到用户态。

1. 如果最终应用在内核态，在内核继续进。

因此，可能产生以下问题：

1：数据包产生通过中断上报处理器，再到处理，延时过长

2：数据报文先送到内核协议栈进行高层处理，再复制到用户空间的耗时

3：用户态线程由操作系统调度，除去本身任务切换的开销，由切换导致的cache替换，或是cache missing等会对性能造成负面影响

由于数据包处理任务存在内核态与用户态的切换，以及多次的内存拷贝，系统消耗变大，以CPU为核心的系统存在很大的处理瓶颈。为了提升在通用服务器（COTS）的数据包处理效能，Intel推出了服务于IA（Intel Architecture）系统的DPDK技术。

DPDK是Data Plane Development Kit的缩写。简单说，DPDK应用程序运行在操作系统的User Space，利用自身提供的数据面库进行收发包处理，绕过了Linux内核态协议栈，以提升报文处理效率。与传统的数据包处理相比,

DPDK的特点（优势）：

轮询：在系统被中断唤醒后，使用轮询的方式一次处理多个数据包，直到网络再次空闲重新转入中断等待；避免了中断上下文切换的开销；

用户态驱动：避免了内存拷贝和系统调用；并且对于数据buff的重新定义不受限于内核的数据结构；也方便快速的迭代优化；

亲和性和独占：指定特定任务在某个核上工作，所谓绑核。避免了线程在不同核间频繁的切换，核间的切换也容易导致cache missing和cache write back造成的大量损失。如果更进一步的限定了核不参与系统调度，可以进一步实现独占，避免了核内的任务切换开销；

降低访存开销：数据包处理是I/O密集型的。通过内存大页降低TLB missing、优化cache等方式减少内存I/O的开销。

软件调优：从软件-代码结构上来优化。比如结构的cache line对齐等方式；

硬件加速：包括利用一些IA的最新指令集或是挖掘网卡的本身性能来实现加速；

DPDK作为基于IA多核处理器的高速包处理平台，以软件库的形式，为上层应用的开发提供了一个高性能的基础I/O开发包；

![](https://gitee.com/hxc8/images6/raw/master/img/202407190001130.jpg)

## 二、DPDK的基石UIO

为了让驱动运行在用户态，Linux提供UIO机制。使用UIO可以通过read感知中断，通过mmap实现和网卡的通讯。

![](https://gitee.com/hxc8/images6/raw/master/img/202407190001511.jpg)

[https://blog.csdn.net/Edidaughter/article/details/109391293](https://blog.csdn.net/Edidaughter/article/details/109391293)
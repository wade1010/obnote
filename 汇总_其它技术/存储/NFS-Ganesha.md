NFS-Ganesha就是实现FUSE的一种方式。

C语言 [https://github.com/nfs-ganesha/nfs-ganesha](https://github.com/nfs-ganesha/nfs-ganesha)

## **NFS-Ganesha 的四大优势**

2007 年左右，CEA 的大型计算机中心每天都会产生 10TB 左右的新数据，CEA 将这些数据放在由 HSM 组成的 HPSS 中，而这些 HSM 本身提供了 NFS 接口。但是开发者在生产环境中发现 HSM 和 NFS 的桥接仍旧有不少问题，因此开发者决心写一个新的 NFS Daemon 来让 NFS 接口更好的配合 HPSS。

这个项目需要解决以上的问题之外，开发团队还指定了其他目标：

– 可以管理百万级别的数据缓存，从而来避免底层文件系统阻塞

– 除了可以对接 HPSS 以外，还可以对接其他文件系统

– 支持 NFSv4，实现易适配( adaptability )，易扩展，安全等特性

– 从根本上解决软件所带来的性能瓶颈

– 开源

– 支持 Unix 系统

由此 NFS-Ganesha 应运而生，它并不是用来替代内核版本的 NFSv4，相反，NFS Ganesha 是一个全新的程序，可能对比 kernel 版本的 NFSv4，Ganesha 的性能有所欠缺，但是基于 user-space 的方法会带来更多有意思的功能。

**1.灵活的内存分配**

首先，user-space 的程序可以分配大量的内存让程序使用，这些内存可以用来建立软件内部缓存，经过测试，我们只需要 4GB 就可以实现百万级别的数据缓存。在一些 x86_64 平台的机器上，我们甚至可以分配更大的内存(16 32GB)，来实现千万级别的数据缓存。

**2.更强的可移植性**

如果 NFS Ganesha 是 kernel-space 的话，那样 NFS Ganesha 的内部结构只能适应一款特定的 OS，而很难移植到别的 OS 上。另外考虑的是代码本身：在不同的平台上编译和运行的产品比在一个单一平台上开发的产品更安全。 我们开发人员的经验表明，只在单一平台上开发会让开发后期困难重重；它通常会显示在 Linux 上不会轻易检测到的错误，因为资源不一样。

当然可移植性不单单指让 NFS Ganesha 可以运行在不同的 OS 上，能够适配不同的文件系统也是考量之一。在 NFSv2 和 NFSv3 中，由于语义设计上偏向 Unix 类的文件系统，因此基本不可能适配非 Unix 类的文件系统。这一情况在 NFSv4 中大有改观，NFSv4 的语义设计出发点是让 NFS 能尽可能多地适配不同的文件系统，因此加强了文件/目录属性参数的抽象。

Ganesha 设计初衷是成为一个 NFSv4 通用服务器，可以实现 NFSv4 的所有功能，因此也需要适配各种文件系统。在内核中实现这一功能是不容易的(内核编程会有很多限制)，然而在 user-space 中实现这一点会便捷一些。

**3.更便捷的访问机制**

内核中的 NFSv4 访问用户空间中的服务不是那么方便，因此其引入了 rpc_pipefs 机制， 用于解决用户空间服务的桥梁，并且使用 kerberos5 管理安全性或 idmapd 守护程序来进行用户名转换。然而 Ganesha 不需要这些，它使用常规 API 来对外提供服务。

**4.对接FUSE**

由于 NFS Ganesha 是一个运行在用户空间的程序，因此它还提供了对一些用户空间文件系统( FUSE )的支持，可以让我们直接把 FUSE 挂载在 NFS 上而不需要内核的帮助。

## **NFS-Ganesha 框架浅析**

![](https://gitee.com/hxc8/images6/raw/master/img/202407190007657.jpg)

Figure 1 – NFS Ganesha 分层架构图

由上图我们可以看到，Ganesha 是一个基于模块的程序，每个模块都负责各自的任务和目标。开发团队在写代码之前就对每个模块进行了精心的设计，保证了后期扩展的便捷性。比如缓存管理模块只负责管理缓存，任何在缓存管理模块上做出的更改不能影响其他模块。这么做大大减少了每个模块间的耦合，虽然开发初期显得困难重重，但在中后期就方便了很多，每个模块可以独立交给不同开发人员来进行开发、验证和测试。

**Ganesha 的核心模块**

– Memory Manager： 负责 Ganesha 的内存管理。

– RPCSEC_GSS：负责使用 RPCSEC_GSS 的数据传输，通常使用 krb5, SPKM3 或 LIPKEY 来管理安全。

– NFS 协议模块：负责 NFS 消息结构的管理

– Metadata（Inode） Cache: 负责元数据缓存管理

– File Content Cache：负责数据缓存管理

– File System Abstraction Layer( FSAL ): 非常重要的模块，通过一个接口来完成对命名空间的访问。所访问的对象随后会放置在 inode cache 和 file content cache 中。

– Hash Tables：提供了基于红黑树的哈希表，这个模块在 Ganesha 里用到很多。

**内存管理**

内存管理是开发 Ganesha 时比较大的问题，因为大多数 Ganesha 架构中的所有模块都必须执行动态内存分配。 例如，管理 NFS 请求的线程可能需要分配用于存储所请求结果的缓冲器。 如果使用常规的 LibC malloc / free 调用，则存在内存碎片的风险，因为某些模块将分配大的缓冲区，而其他模块将使用较小的缓冲区。 这可能导致程序使用的部分内存被交换到磁盘，性能会迅速下降的情况。

因此 Ganesha 有一个自己的内存管理器，来给各个线程分配需要的内存。内存管理器使用了Buddy Malloc algorithm，和内核使用的内存分配是一样的。内存分配器中调用了 madvise 来管束 Linux 内存管理器不要移动相关页。其会向 Linux 申请一大块内存来保持高性能表现。

**线程管理**

管理 CPU 相比较内存会简单一些。Ganesha 使用了大量的线程，可能在同一时间会有几十个线程在并行工作。开发团队在这里用到了很多 POSIX 调用来管理线程，让 Linux 调度进程单独处理每一个线程，使得负载可以覆盖到所有的 CPU。

开发团队也考虑了死锁情况，虽然引入互斥锁可以用来防止资源访问冲突，但是如果大量线程因此陷入死锁状态，会大大降低性能。因此开发团队采用了读写锁，但是由于读写锁可能因系统而异，因此又开发了一个库来完成读写锁的转换。

当一个线程池中同时存在太多线程时，这个线程池会成为性能瓶颈。为了解决这个问题， Ganesha 给每一个线程分配了单独的资源，这样也要求每个线程自己处理垃圾回收，并且定期重新组合它的资源。同时 ”dispatcher thread” 提供了一些机制来防止太多线程在同一时间执行垃圾回收；在缓存层中垃圾回收被分成好几个步骤，每个步骤由单独代理处理。经过生产环境实测，这种设计时得当的。

**哈希表**

关联寻找功能在 Ganesha 被大量使用，比如我们想通过对象的父节点和名称来寻找对象元数据等类似行为是很经常的，因此为了保证 Ganesha 整体的高性能，关联寻找功能必须非常高效。

为了达到这个目的，开发团队采用了红黑树，它会在 add/update 操作后自动冲平衡。由于单棵红黑树会引发进程调用冲突（多个进程同时 add/update，引发同时重平衡），如果加读写锁在红黑树上，又会引发性能瓶颈。因此开发团队设计了红黑树数组来解决了这个问题，降低了两个线程同时访问一个红黑树的概率，从而避免了访问冲突。